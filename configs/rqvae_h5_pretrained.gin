# Configuration for RQ-VAE training with H5 pretrained embeddings
# This config demonstrates how to use pre-computed embeddings from H5 files

# Import the training function
import train_rqvae

# H5 Dataset Configuration
train.use_h5_dataset = True
train.h5_item_data_path = "data/preprocessed/item_data.h5"
train.h5_sequence_data_path = "data/preprocessed/sequence.h5" 
train.h5_max_seq_len = 200
train.h5_test_ratio = 0.2

# Training Configuration
train.iterations = 50000
train.batch_size = 64
train.learning_rate = 0.0001
train.weight_decay = 0.01
train.gradient_accumulate_every = 1
train.amp = False
train.mixed_precision_type = "fp16"

# Model Configuration 
# Note: vae_input_dim will be automatically set from H5 file embedding dimension
train.vae_embed_dim = 64
train.vae_hidden_dims = [256, 128]
train.vae_codebook_size = 1024
train.vae_codebook_normalize = True
train.vae_codebook_mode = %QuantizeForwardMode.GUMBEL_SOFTMAX
train.vae_sim_vq = False
train.vae_n_layers = 3
train.commitment_weight = 0.25

# Evaluation and Saving
train.do_eval = True
train.eval_every = 5000
train.save_model_every = 10000
train.save_dir_root = "out/h5_pretrained/"

# Logging
train.swanlab_logging = False

# Other settings
train.use_kmeans_init = True
train.split_batches = True

# Original dataset parameters (not used when use_h5_dataset=True)
train.dataset_folder = "dataset/amazon" 
train.dataset = %RecDataset.AMAZON
train.dataset_split = "beauty"
train.force_dataset_process = False