import h5py
import numpy as np
import paddle
import random
from typing import Dict, List, Optional, Tuple
from data.schemas import SeqBatch
from data.utils import batch_to
import os


class H5PretrainedDataset:
    """
    Dataset for loading pre-trained item embeddings from H5 files for RQ-VAE training.
    
    This dataset is optimized for RQ-VAE training and provides item-level train/eval splitting:
    - 95% of items for training
    - 5% of items for evaluation
    
    Expected file structure:
    - item_data.h5: Contains item embeddings and features
    """
    
    def __init__(
        self,
        item_data_path: str,
        train_test_split: str = "train",
        test_ratio: float = 0.05
    ):
        """
        Initialize H5 pretrained dataset for RQ-VAE training.
        
        Args:
            item_data_path: Path to item_data.h5 file
            train_test_split: "train" or "eval" for item-level splitting
            test_ratio: Ratio of items to use for evaluation (default: 0.05 = 5%)
        """
        self.item_data_path = item_data_path
        self.train_test_split = train_test_split
        self.test_ratio = test_ratio
        
        # Verify file exists
        if not os.path.exists(item_data_path):
            raise FileNotFoundError(f"Item data file not found: {item_data_path}")
            
        # Load item data
        self._load_item_data()
        
        # Create item-level train/test split
        self._create_train_test_split()
        
    def _load_item_data(self):
        """Load item embeddings and create item mapping."""
        print(f"Loading item data from {self.item_data_path}")
        
        with h5py.File(self.item_data_path, 'r') as f:
            self.item_ids = f['item_ids'][:]
            self.item_embeddings = f['embeddings'][:]
            
            # Load metadata
            self.n_items = f.attrs['n_items']
            self.embedding_dim = f.attrs['embedding_dim']
            
        # Create item_id to index mapping
        self.item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}
        
        print(f"Loaded {self.n_items} items with {self.embedding_dim}D embeddings")
        
        
    def _create_train_test_split(self):
        """Create item-level train/test split for RQ-VAE training."""
        # Create item-level train/test split: 95% train, 5% eval
        paddle.seed(42)  # For reproducibility
        self.item_is_train = paddle.rand([self.n_items]) > self.test_ratio
        
        # Determine active items based on train_test_split mode
        if self.train_test_split == "train":
            self.active_item_mask = self.item_is_train
            print(f"Using {self.item_is_train.sum().item()}/{self.n_items} items for training")
        elif self.train_test_split == "eval":
            self.active_item_mask = ~self.item_is_train
            print(f"Using {(~self.item_is_train).sum().item()}/{self.n_items} items for evaluation")
        else:
            raise ValueError(f"Unsupported train_test_split: {self.train_test_split}. Use 'train' or 'eval'.")
        
    def __len__(self):
        """Return number of active items."""
        return self.active_item_mask.sum().item()
        
    def __getitem__(self, idx) -> SeqBatch:
        """Get individual item sample for RQ-VAE training."""
        # Handle batch indices (tensor input)
        if isinstance(idx, paddle.Tensor):
            if idx.ndim == 0:  # Single tensor index
                idx = idx.item()
            else:  # Multiple indices - return concatenated batch
                samples = [self[i.item()] for i in idx]
                return self._batch_samples(samples)
        
        # Get active item indices
        active_item_indices = paddle.where(self.active_item_mask)[0]
        
        if idx >= len(active_item_indices):
            raise IndexError(f"Index {idx} out of range for item-level access")
        
        # Get the actual item index
        item_idx = active_item_indices[idx].item()
        
        # Create tensors for single item (optimized for RQ-VAE)
        ids = paddle.to_tensor([item_idx], dtype=paddle.int64)
        user_ids = paddle.to_tensor([-1], dtype=paddle.int64)  # Placeholder
        ids_fut = paddle.to_tensor([-1], dtype=paddle.int64)  # Placeholder
        
        # Get item embedding - this is what RQ-VAE actually uses
        x = paddle.to_tensor(self.item_embeddings[item_idx], dtype=paddle.float32).unsqueeze(0)
        x_fut = paddle.zeros_like(x)  # Placeholder
        
        # Single item mask
        seq_mask = paddle.to_tensor([True], dtype=paddle.bool)
        
        return SeqBatch(
            user_ids=user_ids,
            ids=ids,
            ids_fut=ids_fut,
            x=x,
            x_fut=x_fut,
            seq_mask=seq_mask
        )
    
    def _batch_samples(self, samples):
        """Batch multiple SeqBatch samples into a single batch."""
        user_ids = paddle.concat([sample.user_ids for sample in samples], axis=0)
        ids = paddle.concat([sample.ids for sample in samples], axis=0)
        ids_fut = paddle.concat([sample.ids_fut for sample in samples], axis=0)
        x = paddle.concat([sample.x for sample in samples], axis=0)
        x_fut = paddle.concat([sample.x_fut for sample in samples], axis=0)
        seq_mask = paddle.concat([sample.seq_mask for sample in samples], axis=0)
        
        return SeqBatch(
            user_ids=user_ids,
            ids=ids,
            ids_fut=ids_fut,
            x=x,
            x_fut=x_fut,
            seq_mask=seq_mask
        )


def create_h5_dataloader(
    item_data_path: str,
    batch_size: int = 64,
    train_test_split: str = "train",
    test_ratio: float = 0.05,
    shuffle: bool = True,
    num_workers: int = 0,
    prefetch_factor: int = 2
) -> paddle.io.DataLoader:
    """
    Create DataLoader for H5 pretrained dataset (RQ-VAE training).
    
    Args:
        item_data_path: Path to item_data.h5
        batch_size: Batch size
        train_test_split: "train" or "eval" for item-level splitting
        test_ratio: Ratio of items for evaluation (default: 0.05 = 5%)
        shuffle: Whether to shuffle data
        
    Returns:
        paddle.io.DataLoader
    """
    dataset = H5PretrainedDataset(
        item_data_path=item_data_path,
        train_test_split=train_test_split,
        test_ratio=test_ratio
    )
    
    def collate_fn(batch):
        """
        Collate function for item-level batching (RQ-VAE training).
        
        Each sample in batch is a single item with shape [1, embedding_dim].
        Output batch will have shape [batch_size, embedding_dim].
        """
        if len(batch) == 1:
            return batch[0]
            
        # Concatenate item-level tensors
        user_ids = paddle.concat([item.user_ids for item in batch], axis=0)  # [batch_size]
        ids = paddle.concat([item.ids for item in batch], axis=0)  # [batch_size]
        ids_fut = paddle.concat([item.ids_fut for item in batch], axis=0)  # [batch_size]
        x = paddle.concat([item.x for item in batch], axis=0)  # [batch_size, embedding_dim]
        x_fut = paddle.concat([item.x_fut for item in batch], axis=0)  # [batch_size, embedding_dim]
        seq_mask = paddle.concat([item.seq_mask for item in batch], axis=0)  # [batch_size]
        
        return SeqBatch(
            user_ids=user_ids,
            ids=ids,
            ids_fut=ids_fut,
            x=x,
            x_fut=x_fut,
            seq_mask=seq_mask
        )
    
    sampler = paddle.io.RandomSampler(dataset) if shuffle else None
    
    return paddle.io.DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        collate_fn=collate_fn,
        num_workers=num_workers,
        use_shared_memory=False,
        prefetch_factor=prefetch_factor
    )


class H5SequenceDataset:
    """
    ç”¨äºDecoderè®­ç»ƒçš„H5åºåˆ—æ•°æ®é›†ï¼Œå®Œå…¨ç‹¬ç«‹äºç‰©å“çº§æ•°æ®é›†ã€‚
    
    è¯¥æ•°æ®é›†å¤„ç†ç”¨æˆ·è¡Œä¸ºåºåˆ—ï¼Œæ”¯æŒè®­ç»ƒRQ-VAE Decoderæ¨¡å‹ã€‚
    
    é¢„æœŸçš„H5æ–‡ä»¶ç»“æ„ï¼š
    - sequence_data.h5: åŒ…å«ç”¨æˆ·è¡Œä¸ºåºåˆ—å’Œç‰©å“embeddings
      - user_ids: ç”¨æˆ·IDæ•°ç»„ [n_sequences]  
      - item_sequences: æ¯ä¸ªç”¨æˆ·çš„ç‰©å“IDåºåˆ— (å˜é•¿)
      - sequence_lengths: æ¯ä¸ªåºåˆ—çš„é•¿åº¦ [n_sequences]
      - item_embeddings: ç‰©å“embeddingçŸ©é˜µ [n_items, embedding_dim]
      - item_id_mapping: ç‰©å“IDåˆ°embeddingç´¢å¼•çš„æ˜ å°„
    """
    
    def __init__(
        self,
        sequence_data_path: str,
        item_data_path: str,
        is_train: bool = True,
        max_seq_len: int = 200,
        test_ratio: float = 0.2,  # ä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†å®é™…ä¸Šä¸ä½¿ç”¨
        subsample: bool = False
    ):
        """
        åˆå§‹åŒ–H5åºåˆ—æ•°æ®é›†ã€‚
        
        Args:
            sequence_data_path: åºåˆ—æ•°æ®H5æ–‡ä»¶è·¯å¾„
            item_data_path: ç‰©å“æ•°æ®H5æ–‡ä»¶è·¯å¾„ (åŒ…å«item embeddings)
            is_train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ (ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨)
            subsample: æ˜¯å¦å¯¹è®­ç»ƒåºåˆ—è¿›è¡Œå­é‡‡æ ·
        
        æ³¨æ„: éªŒè¯é›†é€šè¿‡æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®è‡ªåŠ¨æ„å»ºï¼Œä¸éœ€è¦test_ratioåˆ†å‰²
        """
        self.sequence_data_path = sequence_data_path
        self.item_data_path = item_data_path
        self.is_train = is_train
        self.max_seq_len = max_seq_len
        self.test_ratio = test_ratio
        self.subsample = subsample
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        print(f"ğŸ” Checking sequence file exists: {sequence_data_path}")
        if not os.path.exists(sequence_data_path):
            raise FileNotFoundError(f"Sequence data file not found: {sequence_data_path}")
        print("âœ… Sequence file exists")
        
        print(f"ğŸ” Checking item data file exists: {item_data_path}")
        if not os.path.exists(item_data_path):
            raise FileNotFoundError(f"Item data file not found: {item_data_path}")
        print("âœ… Item data file exists")
            
        # åŠ è½½åºåˆ—æ•°æ®
        print("ğŸ”„ Starting _load_sequence_data()...")
        self._load_sequence_data()
        print("âœ… _load_sequence_data() completed")
        
        # åˆ›å»ºè®­ç»ƒ/æµ‹è¯•åˆ†å‰²
        print("ğŸ”„ Starting _create_train_test_split()...")
        self._create_train_test_split()
        print("âœ… _create_train_test_split() completed")
        
    def _load_sequence_data(self):
        """åŠ è½½åºåˆ—æ•°æ®å’Œç‰©å“embeddingsã€‚"""
        print(f"Loading sequence data from {self.sequence_data_path}")
        
        with h5py.File(self.sequence_data_path, 'r') as f:
            # åŠ è½½ç”¨æˆ·åºåˆ—æ•°æ®
            print("ğŸ”„ Loading user_ids...")
            self.user_ids = f['user_ids'][:]
            print(f"âœ… Loaded {len(self.user_ids)} user_ids")
            
            print("ğŸ”„ Loading sequence_lengths...")
            self.sequence_lengths = f['sequence_lengths'][:]
            print(f"âœ… Loaded sequence_lengths")
            
            # åŠ è½½å˜é•¿åºåˆ— (å­˜å‚¨ä¸ºvlenæ•°æ®ç±»å‹)
            print("ğŸ”„ Loading sequences data (this may take time)...")
            sequences_data = f['sequences'][:]  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
            print(f"âœ… Loaded sequences data, shape: {len(sequences_data)}")
            
            print("ğŸ”„ Converting sequences to list format...")
            self.item_sequences = [seq.tolist() for seq in sequences_data]
            print(f"âœ… Converted to list format")
            
        # ä»item_data.h5åŠ è½½ç‰©å“embeddings
        print(f"ğŸ”„ Loading item embeddings from {self.item_data_path}...")
        with h5py.File(self.item_data_path, 'r') as item_f:
            self.item_ids = item_f['item_ids'][:]
            self.item_embeddings = item_f['embeddings'][:]
            self.embedding_dim = item_f.attrs['embedding_dim']
            
            print(f"âœ… Loaded item_embeddings: {self.item_embeddings.shape}")
            print(f"âœ… Embedding dimension: {self.embedding_dim}")
            
            # åˆ›å»ºitem_idåˆ°embedding indexçš„æ˜ å°„
            self.item_id_to_embedding_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}
            print(f"âœ… Created item_id_to_embedding_idx mapping: {len(self.item_id_to_embedding_idx)} items")
            
        # åŠ è½½å…ƒæ•°æ®
        print("ğŸ”„ Loading metadata...")
        with h5py.File(self.sequence_data_path, 'r') as f:
            if hasattr(f, 'attrs'):
                self.n_items = f.attrs.get('total_items', len(self.item_id_to_embedding_idx))
                self.total_items = f.attrs.get('total_items', len(self.item_id_to_embedding_idx))
                print(f"âœ… Loaded metadata from sequence file attrs: n_items={self.n_items}, total_items={self.total_items}")
            else:
                self.n_items = len(self.item_id_to_embedding_idx)
                self.total_items = self.n_items
                print(f"âœ… Using default metadata: n_items={self.n_items}, total_items={self.total_items}")
            
        self.n_sequences = len(self.user_ids)
        
        print(f"ğŸ‰ Successfully loaded {self.n_sequences} sequences")
        print(f"ğŸ‰ Found {self.total_items} unique items with {self.embedding_dim}D embeddings")
        print("ğŸ‰ _load_sequence_data completed successfully")
        
    def _create_train_test_split(self):
        """åˆ›å»ºè®­ç»ƒ/éªŒè¯æ•°æ®åˆ†å‰²ã€‚
        
        æ³¨æ„: ç”±äºéªŒè¯æ˜¯é€šè¿‡åºåˆ—å†…éƒ¨çš„æœ€åä¸€ä¸ªä½ç½®å®ç°çš„ï¼Œ
        è®­ç»ƒå’ŒéªŒè¯éƒ½ä½¿ç”¨æ‰€æœ‰åºåˆ—ï¼ŒåŒºåˆ«ä»…åœ¨äºåºåˆ—çš„å¤„ç†æ–¹å¼ã€‚
        """
        print(f"ğŸ”„ Creating active indices for {self.n_sequences} sequences...")
        # ä½¿ç”¨æ‰€æœ‰åºåˆ—ï¼Œå› ä¸ºéªŒè¯é€šè¿‡åºåˆ—å†…éƒ¨çš„æœ€åä½ç½®å®ç°
        self.active_indices = paddle.arange(self.n_sequences)
        print(f"âœ… Created active_indices: {len(self.active_indices)} items")
        
        if self.is_train:
            print(f"ğŸ“ Using all {self.n_sequences} sequences for training (input: first n-1 items, target: last item)")
        else:
            print(f"ğŸ“Š Using all {self.n_sequences} sequences for evaluation (input: first n-1 items, target: last item)")
    
    def __len__(self):
        """è¿”å›æ´»è·ƒåºåˆ—çš„æ•°é‡ã€‚"""
        return len(self.active_indices)
    
    def _get_item_embedding(self, item_id: int) -> np.ndarray:
        """æ ¹æ®ç‰©å“IDè·å–embeddingã€‚"""
        if item_id == -1:  # padding token
            return np.zeros(self.embedding_dim, dtype=np.float32)
        
        # ä½¿ç”¨æ˜ å°„è·å–embeddingç´¢å¼•
        if item_id in self.item_id_to_embedding_idx:
            emb_idx = self.item_id_to_embedding_idx[item_id]
            return self.item_embeddings[emb_idx]
        else:
            # æœªçŸ¥ç‰©å“ï¼Œè¿”å›é›¶å‘é‡
            return np.zeros(self.embedding_dim, dtype=np.float32)
    
    def __getitem__(self, idx) -> SeqBatch:
        """è·å–å•ä¸ªåºåˆ—æ ·æœ¬ã€‚"""
        # è·å–å®é™…åºåˆ—ç´¢å¼•
        seq_idx = self.active_indices[idx].item()
        
        user_id = self.user_ids[seq_idx]
        sequence = self.item_sequences[seq_idx]
        
        if self.subsample and self.is_train:
            # å¯¹è®­ç»ƒåºåˆ—è¿›è¡Œå­é‡‡æ ·
            if len(sequence) >= 3:
                start_idx = random.randint(0, max(0, len(sequence) - 3))
                end_idx = random.randint(start_idx + 3, 
                                       min(start_idx + self.max_seq_len + 1, len(sequence) + 1))
                sequence = sequence[start_idx:end_idx]
        
        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        if len(sequence) > self.max_seq_len:
            sequence = sequence[-self.max_seq_len:]
        
        # åˆ†ç¦»å½“å‰åºåˆ—å’Œæœªæ¥ç‰©å“
        if len(sequence) > 1:
            item_ids = sequence[:-1]
            item_ids_fut = [sequence[-1]]
        else:
            item_ids = sequence
            item_ids_fut = [-1]  # æ²¡æœ‰æœªæ¥ç‰©å“
        
        # Paddingåˆ°å›ºå®šé•¿åº¦
        while len(item_ids) < self.max_seq_len:
            item_ids.append(-1)
        
        # è½¬æ¢ä¸ºtensor
        item_ids = paddle.to_tensor(item_ids[:self.max_seq_len], dtype=paddle.int64)
        item_ids_fut = paddle.to_tensor(item_ids_fut, dtype=paddle.int64)
        user_ids = paddle.to_tensor([user_id], dtype=paddle.int64)
        
        # è·å–ç‰©å“embeddings
        x = []
        for item_id in item_ids.numpy():
            x.append(self._get_item_embedding(int(item_id)))
        x = paddle.to_tensor(np.stack(x), dtype=paddle.float32)
        
        x_fut = []
        for item_id in item_ids_fut.numpy():
            x_fut.append(self._get_item_embedding(int(item_id)))
        x_fut = paddle.to_tensor(np.stack(x_fut), dtype=paddle.float32)
        
        # åˆ›å»ºåºåˆ—mask (æœ‰æ•ˆä½ç½®ä¸ºTrue)
        seq_mask = (item_ids >= 0)
        
        return SeqBatch(
            user_ids=user_ids,
            ids=item_ids,
            ids_fut=item_ids_fut,
            x=x,
            x_fut=x_fut,
            seq_mask=seq_mask
        )


def create_h5_sequence_dataloader(
    sequence_data_path: str,
    item_data_path: str,
    batch_size: int = 64,
    is_train: bool = True,
    max_seq_len: int = 200,
    test_ratio: float = 0.2,  # ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨
    subsample: bool = False,
    shuffle: bool = True,
    num_workers: int = 0,
    prefetch_factor: int = 2
) -> paddle.io.DataLoader:
    """
    åˆ›å»ºH5åºåˆ—æ•°æ®é›†çš„DataLoader (ç”¨äºDecoderè®­ç»ƒ)ã€‚
    
    Args:
        sequence_data_path: åºåˆ—æ•°æ®H5æ–‡ä»¶è·¯å¾„
        item_data_path: ç‰©å“æ•°æ®H5æ–‡ä»¶è·¯å¾„ (åŒ…å«item embeddings)
        batch_size: æ‰¹æ¬¡å¤§å°
        is_train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ (ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨)
        subsample: æ˜¯å¦å¯¹è®­ç»ƒåºåˆ—è¿›è¡Œå­é‡‡æ ·
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        
    Returns:
        paddle.io.DataLoader
        
    æ³¨æ„: éªŒè¯é›†é€šè¿‡æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®è‡ªåŠ¨æ„å»ºï¼Œä¸éœ€è¦test_ratioåˆ†å‰²
    """
    print(f"ğŸ”„ Creating H5SequenceDataset...")
    dataset = H5SequenceDataset(
        sequence_data_path=sequence_data_path,
        item_data_path=item_data_path,
        is_train=is_train,
        max_seq_len=max_seq_len,
        test_ratio=test_ratio,  # ä¼ é€’ä½†ä¸ä½¿ç”¨
        subsample=subsample
    )
    print(f"âœ… H5SequenceDataset created successfully, dataset size: {len(dataset)}")
    
    def collate_fn(batch):
        """
        åºåˆ—çº§æ‰¹å¤„ç†å‡½æ•°ã€‚
        
        è¾“å…¥: åˆ—è¡¨of SeqBatch, æ¯ä¸ªåŒ…å«ä¸€ä¸ªåºåˆ—
        è¾“å‡º: æ‰¹æ¬¡åŒ–çš„SeqBatch, å½¢çŠ¶ä¸º [batch_size, seq_len, ...]
        """
        if len(batch) == 1:
            return batch[0]
            
        # æ‰¹æ¬¡åŒ–åºåˆ—çº§tensors
        user_ids = paddle.concat([item.user_ids for item in batch], axis=0)  # [batch_size]
        ids = paddle.stack([item.ids for item in batch], axis=0)  # [batch_size, seq_len]
        ids_fut = paddle.concat([item.ids_fut for item in batch], axis=0)  # [batch_size]
        x = paddle.stack([item.x for item in batch], axis=0)  # [batch_size, seq_len, embed_dim]
        x_fut = paddle.stack([item.x_fut for item in batch], axis=0)  # [batch_size, embed_dim]
        seq_mask = paddle.stack([item.seq_mask for item in batch], axis=0)  # [batch_size, seq_len]
        
        return SeqBatch(
            user_ids=user_ids,
            ids=ids,
            ids_fut=ids_fut,
            x=x,
            x_fut=x_fut,
            seq_mask=seq_mask
        )
    
    sampler = paddle.io.RandomSampler(dataset) if shuffle else None
    
    return paddle.io.DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        collate_fn=collate_fn,
        num_workers=num_workers,
        use_shared_memory=False,
        prefetch_factor=prefetch_factor
    )