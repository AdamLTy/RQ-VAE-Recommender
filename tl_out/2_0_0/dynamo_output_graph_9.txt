class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "i64[256, 1][1, 1]cuda:0", L_self_modules_emb_parameters_weight_: "f32[2000, 128][128, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_emb_parameters_weight_ = L_self_modules_emb_parameters_weight_
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
        hashed_indices: "i64[256, 1][1, 1]cuda:0" = l_x_ % 2000;  l_x_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.nn.functional.embedding(hashed_indices, l_self_modules_emb_parameters_weight_, None, None, 2.0, False, False);  hashed_indices = l_self_modules_emb_parameters_weight_ = None
        return (embedding,)
        