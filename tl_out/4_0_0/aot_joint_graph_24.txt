class joint_fn(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "f32[s0, 128][128, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s1)"; primals_6: "f32[128][1]cuda:0"; tangents_1: "f32[s0, 128][128, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
    
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
        mean: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
        add: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
        rsqrt: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt);  primals_1 = rsqrt = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  primals_6 = None
        mul_2: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul);  tangents_1 = mul = None
        sum_1: "f32[1, 128][128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_2, [0], True);  mul_2 = None
        unsqueeze: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
        view: "f32[128][1]cuda:0" = torch.ops.aten.view.default(unsqueeze, [128]);  unsqueeze = None
        return pytree.tree_unflatten([mul_1, primals_2, primals_3, primals_4, None, None, view], self._out_spec)
        