class GraphModule(torch.nn.Module):
    def forward(self, primals_1: "f32[s1, 384][1152, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s11)", primals_6: "f32[s1, 384][1152, 1]cuda:0", primals_7: "i64[257][1]cuda:0", primals_8: "f32[s3, 0][1, 1]cuda:0", primals_9: "f32[s4, 0][1, 1]cuda:0", primals_10: "Sym(s11)", primals_11: "f32[s1, 384][1152, 1]cuda:0", primals_12: "i64[257][1]cuda:0", primals_13: "f32[s3, 0][1, 1]cuda:0", primals_14: "f32[s4, 0][1, 1]cuda:0", primals_15: "Sym(s11)"):
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_1, [sym_size_int, 6, 64]);  primals_1 = None
        permute: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_6, [sym_size_int, 6, 64]);  primals_6 = None
        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_11, [sym_size_int, 6, 64]);  primals_11 = None
        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute, [1, 0, 2]);  permute = None
        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_12, torch.int32)
        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_3, 0);  permute_3 = None
        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0);  primals_4 = None
        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
        getitem: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
        getitem_1: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
        getitem_2: "i64[][]cuda:0" = _efficient_attention_forward[2]
        getitem_3: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem, 0)
        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_3, 0);  primals_3 = None
        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        permute_6: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        permute_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_6, [1, 0, 2]);  permute_6 = None
        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_7, [sym_size_int, 384]);  permute_7 = None
        return (view_3, primals_12, full, full_1, primals_12, primals_13, primals_14, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem, getitem_1, getitem_2, getitem_3, sym_size_int, sym_size_int_1)
        