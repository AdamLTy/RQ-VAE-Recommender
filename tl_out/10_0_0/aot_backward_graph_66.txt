class GraphModule(torch.nn.Module):
    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_12: "i64[257][1]cuda:0", primals_13: "f32[s3, 0][1, 1]cuda:0", primals_14: "f32[s4, 0][1, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_1: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_2: "i64[][]cuda:0", getitem_3: "i64[][]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cpu", tangents_4: "f32[s4, 0][1, 1]cpu"):
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [sym_size_int, 6, 64]);  tangents_1 = None
        permute_8: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_8, [1, 0, 2]);  permute_8 = None
        unsqueeze_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_9, 0);  permute_9 = None
        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_3, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_1, 0.0, getitem_2, getitem_3, 0, False);  unsqueeze_3 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem = convert_element_type = sym_size_int_1 = getitem_1 = getitem_2 = getitem_3 = None
        getitem_6: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
        getitem_7: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
        getitem_8: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_8, 0);  getitem_8 = None
        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_7, 0);  getitem_7 = None
        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_6, 0);  getitem_6 = None
        permute_10: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
        permute_11: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
        permute_12: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0, 2]);  permute_10 = None
        view_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_13, [sym_size_int, 384]);  permute_13 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0, 2]);  permute_11 = None
        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_14, [sym_size_int, 384]);  permute_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_12, [1, 0, 2]);  permute_12 = None
        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_15, [sym_size_int, 384]);  permute_15 = sym_size_int = None
        return (view_7, primals_12, primals_13, primals_14, None, view_6, primals_12, primals_13, primals_14, None, view_5, primals_12, primals_13, primals_14, None)
        