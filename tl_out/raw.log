V0303 09:09:41.798769 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", 0]}
V0303 09:09:41.799403 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/train_decoder.py", 1]}
V0303 09:09:41.799556 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/gin/config.py", 2]}
V0303 09:09:41.799684 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", 3]}
V0303 09:09:41.799858 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/model.py", 4]}
V0303 09:09:41.799998 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 421, "name": "forward", "filename": 4}]}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.800247 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "de79010a48f52078d87598f31bc64284"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992981800143.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.800418 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "1ecf7185c81102d47c998a9d7f05110e"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992981800143.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.817889 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 0, "size": 20480}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.818304 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 3, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 20, 4], "is_leaf": true, "stride": [80, 4, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba18a41180>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.818530 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4090>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.818683 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 0, "source": "L['batch'][3]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.825239 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 0, "size": 2048}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.825514 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 1], "is_leaf": true, "stride": [1, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5d60>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.825667 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 3, "source": "L['batch'][0]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.829267 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 0, "size": 1024000}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.829533 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2000, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0aa40>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.829706 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 5, "source": "L['self']._modules['user_id_embedder']._modules['emb']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.834391 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 0, "size": 163840}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.834656 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "stride": [80, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4180>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.834856 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 6, "source": "L['batch'][4]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.836678 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 0, "size": 163840}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.836975 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 9, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [5120, 4], "is_leaf": true, "stride": [4, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5db0>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.837186 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 8, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 4, "base": 9, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4040>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.837331 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 8, "source": "L['batch'][1]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.840847 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 0, "size": 8192}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.841141 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 14, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b40e0>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.841353 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 13, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "is_view": true, "stride": [4, 1], "storage": 5, "base": 14, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4270>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.841499 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 13, "source": "L['batch'][2]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.843029 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 0, "size": 8192}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.843354 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 16, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4220>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.843517 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 16, "source": "L['batch'][5]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.847002 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 0, "size": 524800}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.847267 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 19, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1025, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cf90>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.847424 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 19, "source": "L['self']._modules['sem_id_embedder']._modules['emb']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.858092 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 0, "size": 40960}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.858364 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 22, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [80, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0a9f0>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.858517 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 22, "source": "L['self']._modules['wpe']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.863614 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 0, "size": 512}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.863875 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 25, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cb80>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.864035 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 25, "source": "L['self']._parameters['bos_emb']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.866648 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 0, "size": 2048}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.866918 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 27, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12450>", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.867076 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 0, "id": 27, "source": "L['self']._modules['tte']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.874265 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 1, "size": 20480}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.874567 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 3, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 20, 4], "is_leaf": true, "stride": [80, 4, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba18a41180>", "describer_id": 1}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.874815 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 0, "base": 1, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4090>", "describer_id": 1}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.874978 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 1, "id": 0, "source": "L['batch'][3]"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.884028 12578 torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1, "has_payload": "80315c73d59024b07ddf57e93b05eb5b"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 94485852253984)                 
	| +- GuardManager: source=L['batch'], accessed_by=DictGetItemGuardAccessor(batch)
	| | +- TYPE_MATCH: ___check_type_id(L['batch'], 94485825490064)                
	| | +- LENGTH_CHECK: len(L['batch']) == 6                                        
	| | +- GuardManager: source=L['batch'][3], accessed_by=TupleGetItemGuardAccessor(3)
	| | | +- TENSOR_MATCH: check_tensor(L['batch'][3], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bool, device=0, requires_grad=False, size=[256, 80], stride=[80, 1])
	| | | +- NO_HASATTR: hasattr(L['batch'][3], '_dynamo_dynamic_indices') == False  
	
V0303 09:09:41.884317 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1, "has_payload": "d986aa751ab069c5c9c2fa2cd3429340"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992981884267.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.884502 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1, "has_payload": "7a0478985648caa876e45c7ae113f336"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992981884463.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 0,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.884790 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "0/0", "frame_key": "1", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/model.py", "co_firstlineno": 421, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 8, "shape_env_guard_count": 0, "graph_op_count": 0, "graph_node_count": 1, "graph_input_count": 1, "start_time": 1740992981.8001313, "entire_frame_compile_time_s": 0.08403801918029785, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": ["call torch._dynamo.disable() wrapped function <function padded_to_jagged_tensor at 0x7fba2cb93dc0>"], "dynamo_time_before_restart_s": 0.07224702835083008, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.885252 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", 5]}
V0303 09:09:41.885410 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 288, "name": "_predict", "filename": 4}]}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.885611 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "b3402fc0f940904125483bdcc0e8e6ae"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992981885549.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.885775 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0, "has_payload": "e08ee55ac6d12a53497ddcbb8911fdd6"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992981885549.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.889404 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 2, "size": 2048}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.889663 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5d60>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.889813 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 0, "source": "L['batch'][0]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.892365 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 2, "size": 1024000}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.892629 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2000, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0aa40>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.892786 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 1, "source": "L['self']._modules['user_id_embedder']._modules['emb']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.895626 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 2, "size": 163840}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.895886 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "stride": [80, 1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4180>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.896039 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 2, "source": "L['batch'][4]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.897212 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 2, "size": 163840}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.897506 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [5120, 4], "is_leaf": true, "stride": [4, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5db0>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.897717 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 3, "base": 4, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4040>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.897874 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 3, "source": "L['batch'][1]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.899471 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 2, "size": 20480}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.899763 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 3, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 20, 4], "is_leaf": true, "stride": [80, 4, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba18a41180>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.899985 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 4, "base": 6, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4090>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.900134 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 5, "source": "L['batch'][3]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.901919 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 2, "size": 8192}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.902215 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 8, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b40e0>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.902425 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "is_view": true, "stride": [4, 1], "storage": 5, "base": 8, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4270>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.902572 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 7, "source": "L['batch'][2]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.903743 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 2, "size": 8192}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.903993 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 9, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4220>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.904146 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 9, "source": "L['batch'][5]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.906909 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 2, "size": 524800}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.907184 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 10, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1025, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cf90>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.907347 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 10, "source": "L['self']._modules['sem_id_embedder']._modules['emb']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.913861 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 2, "size": 40960}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.914207 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 11, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [80, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0a9f0>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.914368 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 11, "source": "L['self']._modules['wpe']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.917306 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 2, "size": 512}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.917561 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 12, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cb80>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.917717 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 12, "source": "L['self']._parameters['bos_emb']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.919914 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 2, "size": 2048}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.920169 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 13, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12450>", "describer_id": 2}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.920320 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 2, "id": 13, "source": "L['self']._modules['tte']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:41.927083 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 3, "size": 2048}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.927349 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5d60>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.927507 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 0, "source": "L['batch'][0]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.930157 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 3, "size": 1024000}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.930412 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2000, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0aa40>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.930567 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 1, "source": "L['self']._modules['user_id_embedder']._modules['emb']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.933352 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 3, "size": 163840}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.933602 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "stride": [80, 1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4180>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.933751 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 2, "source": "L['batch'][4]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.934951 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 3, "size": 163840}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.935238 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [5120, 4], "is_leaf": true, "stride": [4, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5db0>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.935446 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 3, "base": 4, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4040>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.935592 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 3, "source": "L['batch'][1]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.937183 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 3, "size": 20480}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.937472 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 3, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 20, 4], "is_leaf": true, "stride": [80, 4, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba18a41180>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.937682 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 4, "base": 6, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4090>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.937847 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 5, "source": "L['batch'][3]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.939609 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 3, "size": 8192}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.939897 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 8, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b40e0>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.940108 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "is_view": true, "stride": [4, 1], "storage": 5, "base": 8, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4270>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.940251 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 7, "source": "L['batch'][2]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.941412 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 3, "size": 8192}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.941653 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 9, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4220>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.941801 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 9, "source": "L['batch'][5]"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.944416 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 3, "size": 524800}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.944666 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 10, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1025, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cf90>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.944871 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 10, "source": "L['self']._modules['sem_id_embedder']._modules['emb']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.951497 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 3, "size": 40960}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.951759 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 11, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [80, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0a9f0>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.951922 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 11, "source": "L['self']._modules['wpe']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.954864 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 3, "size": 512}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.955130 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 12, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cb80>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.955285 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 12, "source": "L['self']._parameters['bos_emb']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.957452 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 3, "size": 2048}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.957708 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 13, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12450>", "describer_id": 3}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.957869 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 3, "id": 13, "source": "L['self']._modules['tte']._parameters['weight']"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:41.967013 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_batch_0_": [256, 1], "l_self_modules_user_id_embedder_modules_emb_parameters_weight_": [2000, 128], "l_batch_4_": [256, 80], "l_batch_1_": [256, 80], "l_batch_3_": [256, 80], "l_batch_2_": [256, 4], "l_batch_5_": [256, 4], "l_self_modules_sem_id_embedder_modules_emb_parameters_weight_": [1025, 128], "l_self_modules_wpe_parameters_weight_": [80, 128], "l_self_parameters_bos_emb_": [128], "l_self_modules_tte_parameters_weight_": [4, 128], "hashed_indices": [256, 1], "user_emb": [256, 1, 128], "mul": [256, 80], "sem_ids": [256, 80], "invert": [256, 80], "mul_1": [256, 4], "sem_ids_fut": [256, 4], "sem_ids_fut_1": [256, 4, 128], "sem_ids_emb": [256, 80, 128], "seq_lengths": [256], "arange": [80], "pos": [1, 80], "wpe": [1, 80, 128], "add_2": [256, 80, 128], "input_embedding": [256, 81, 128], "input_embedding_fut": [256, 1, 128], "tte_fut": [256, 4, 128], "add_3": [256, 4, 128], "input_embedding_fut_1": [256, 5, 128], "add_4": [256]}}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "43049bd35ecf0d8f642fb3d1d41498c1"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_batch_0_: "i64[256, 1][1, 1]cuda:0", L_self_modules_user_id_embedder_modules_emb_parameters_weight_: "f32[2000, 128][128, 1]cuda:0", L_batch_4_: "i64[256, 80][80, 1]cuda:0", L_batch_1_: "i64[256, 80][80, 1]cuda:0", L_batch_3_: "b8[256, 80][80, 1]cuda:0", L_batch_2_: "i64[256, 4][4, 1]cuda:0", L_batch_5_: "i64[256, 4][4, 1]cuda:0", L_self_modules_sem_id_embedder_modules_emb_parameters_weight_: "f32[1025, 128][128, 1]cuda:0", L_self_modules_wpe_parameters_weight_: "f32[80, 128][128, 1]cuda:0", L_self_parameters_bos_emb_: "f32[128][1]cuda:0", L_self_modules_tte_parameters_weight_: "f32[4, 128][128, 1]cuda:0"):
	        l_batch_0_ = L_batch_0_
	        l_self_modules_user_id_embedder_modules_emb_parameters_weight_ = L_self_modules_user_id_embedder_modules_emb_parameters_weight_
	        l_batch_4_ = L_batch_4_
	        l_batch_1_ = L_batch_1_
	        l_batch_3_ = L_batch_3_
	        l_batch_2_ = L_batch_2_
	        l_batch_5_ = L_batch_5_
	        l_self_modules_sem_id_embedder_modules_emb_parameters_weight_ = L_self_modules_sem_id_embedder_modules_emb_parameters_weight_
	        l_self_modules_wpe_parameters_weight_ = L_self_modules_wpe_parameters_weight_
	        l_self_parameters_bos_emb_ = L_self_parameters_bos_emb_
	        l_self_modules_tte_parameters_weight_ = L_self_modules_tte_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        hashed_indices: "i64[256, 1][1, 1]cuda:0" = l_batch_0_ % 2000;  l_batch_0_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        user_emb: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.nn.functional.embedding(hashed_indices, l_self_modules_user_id_embedder_modules_emb_parameters_weight_, None, None, 2.0, False, False);  hashed_indices = l_self_modules_user_id_embedder_modules_emb_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = l_batch_4_ * 256;  l_batch_4_ = None
	        sem_ids: "i64[256, 80][80, 1]cuda:0" = mul + l_batch_1_;  mul = l_batch_1_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        invert: "b8[256, 80][80, 1]cuda:0" = ~l_batch_3_
	        sem_ids[invert] = 1024;  setitem = sem_ids;  invert = setitem = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = l_batch_5_ * 256
	        sem_ids_fut: "i64[256, 4][4, 1]cuda:0" = mul_1 + l_batch_2_;  mul_1 = l_batch_2_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        sem_ids_fut_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.nn.functional.embedding(sem_ids_fut, l_self_modules_sem_id_embedder_modules_emb_parameters_weight_, 1024, None, 2.0, False, False);  sem_ids_fut = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        sem_ids_emb: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.nn.functional.embedding(sem_ids, l_self_modules_sem_id_embedder_modules_emb_parameters_weight_, 1024, None, 2.0, False, False);  sem_ids = l_self_modules_sem_id_embedder_modules_emb_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:292 in _predict, code: seq_lengths = batch.seq_mask.sum(axis=1)
	        seq_lengths: "i64[256][1]cuda:0" = l_batch_3_.sum(axis = 1);  l_batch_3_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:299 in _predict, code: pos = torch.arange(N, device=sem_ids_emb.device).unsqueeze(0)
	        arange: "i64[80][1]cuda:0" = torch.arange(80, device = device(type='cuda', index=0))
	        pos: "i64[1, 80][80, 1]cuda:0" = arange.unsqueeze(0);  arange = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
	        wpe: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.nn.functional.embedding(pos, l_self_modules_wpe_parameters_weight_, None, None, 2.0, False, False);  pos = l_self_modules_wpe_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
	        add_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = wpe + sem_ids_emb;  wpe = sem_ids_emb = None
	        input_embedding: "f32[256, 81, 128][10368, 128, 1]cuda:0" = torch.cat([user_emb, add_2], axis = 1);  user_emb = add_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
	        input_embedding_fut: "f32[256, 1, 128][128, 128, 1]cuda:0" = l_self_parameters_bos_emb_.repeat(256, 1, 1);  l_self_parameters_bos_emb_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
	        tte_fut: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.nn.functional.embedding(l_batch_5_, l_self_modules_tte_parameters_weight_, None, None, 2.0, False, False);  l_batch_5_ = l_self_modules_tte_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:308 in _predict, code: sem_ids_emb_fut + tte_fut
	        add_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = sem_ids_fut_1 + tte_fut;  sem_ids_fut_1 = tte_fut = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
	        input_embedding_fut_1: "f32[256, 5, 128][640, 128, 1]cuda:0" = torch.cat([input_embedding_fut, add_3], axis = 1);  input_embedding_fut = add_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:313 in _predict, code: input_embedding = padded_to_jagged_tensor(input_embedding, lengths=seq_lengths+1)
	        add_4: "i64[256][1]cuda:0" = seq_lengths + 1;  seq_lengths = None
	        return (input_embedding, add_4, input_embedding_fut_1)
	        
V0303 09:09:41.967465 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "75a0322ebc569bfabda83d8d185a8750"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992981967407.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.967645 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "fb244f8e37107fc640c0138494c38033"}
	{
	"name": "backend_compile",
	"ts": 1740992981967407.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:41.979100 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "eb70d04486fb61d008eb86182a8c9aa8"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992981979042.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.078122 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "b74f5e14b752be0366e31ea55c31dc8d"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "i64[256, 1][1, 1]cuda:0"; primals_2: "f32[2000, 128][128, 1]cuda:0"; primals_3: "i64[256, 80][80, 1]cuda:0"; primals_4: "i64[256, 80][80, 1]cuda:0"; primals_5: "b8[256, 80][80, 1]cuda:0"; primals_6: "i64[256, 4][4, 1]cuda:0"; primals_7: "i64[256, 4][4, 1]cuda:0"; primals_8: "f32[1025, 128][128, 1]cuda:0"; primals_9: "f32[80, 128][128, 1]cuda:0"; primals_10: "f32[128][1]cuda:0"; primals_11: "f32[4, 128][128, 1]cuda:0"; tangents_1: "f32[256, 81, 128][10368, 128, 1]cuda:0"; tangents_2: "f32[256, 5, 128][640, 128, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        remainder: "i64[256, 1][1, 1]cuda:0" = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, 256);  primals_3 = None
	        add: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.add.Tensor(mul, primals_4);  mul = primals_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        bitwise_not: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.bitwise_not.default(primals_5)
	        _tensor_constant0 = self._tensor_constant0
	        lift_fresh_copy: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
	        index_put: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.index_put.default(add, [bitwise_not], lift_fresh_copy);  add = bitwise_not = lift_fresh_copy = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_7, 256)
	        add_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1, primals_6);  mul_1 = primals_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        embedding_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_8, add_1, 1024)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_8, index_put, 1024);  primals_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:292 in _predict, code: seq_lengths = batch.seq_mask.sum(axis=1)
	        sum_1: "i64[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(primals_5, [1]);  primals_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:299 in _predict, code: pos = torch.arange(N, device=sem_ids_emb.device).unsqueeze(0)
	        iota: "i64[80][1]cuda:0" = torch.ops.prims.iota.default(80, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
	        unsqueeze: "i64[1, 80][80, 1]cuda:0" = torch.ops.aten.unsqueeze.default(iota, 0);  iota = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
	        embedding_3: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_9, unsqueeze);  primals_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
	        add_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding_3, embedding_2);  embedding_3 = embedding_2 = None
	        cat: "f32[256, 81, 128][10368, 128, 1]cuda:0" = torch.ops.aten.cat.default([embedding, add_2], 1);  embedding = add_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
	        repeat: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.repeat.default(primals_10, [256, 1, 1]);  primals_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
	        embedding_4: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_11, primals_7);  primals_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:308 in _predict, code: sem_ids_emb_fut + tte_fut
	        add_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding_1, embedding_4);  embedding_1 = embedding_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
	        cat_1: "f32[256, 5, 128][640, 128, 1]cuda:0" = torch.ops.aten.cat.default([repeat, add_3], 1);  repeat = add_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:313 in _predict, code: input_embedding = padded_to_jagged_tensor(input_embedding, lengths=seq_lengths+1)
	        add_4: "i64[256][1]cuda:0" = torch.ops.aten.add.Tensor(sum_1, 1);  sum_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
	        slice_1: "f32[256, 1, 128][640, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_2, 1, 0, 1)
	        slice_2: "f32[256, 4, 128][640, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_2, 1, 1, 5);  tangents_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
	        eq: "b8[256, 4][4, 1]cuda:0" = torch.ops.aten.eq.Scalar(primals_7, -1)
	        unsqueeze_1: "b8[256, 4, 1][4, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
	        scalar_tensor: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_1, scalar_tensor, slice_2);  unsqueeze_1 = scalar_tensor = None
	        full: "f32[4, 128][128, 1]cuda:0" = torch.ops.aten.full.default([4, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_1: "f32[4, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full, [primals_7], where, True);  full = primals_7 = where = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
	        sum_2: "f32[1, 128][128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_1, [0]);  slice_1 = None
	        sum_3: "f32[128][1]cuda:0" = torch.ops.aten.sum.dim_IntList(sum_2, [0]);  sum_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
	        slice_3: "f32[256, 1, 128][10368, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_1, 1, 0, 1)
	        slice_4: "f32[256, 80, 128][10368, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_1, 1, 1, 81);  tangents_1 = None
	        sum_4: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_4, [0], True)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
	        eq_1: "b8[1, 80][80, 1]cuda:0" = torch.ops.aten.eq.Scalar(unsqueeze, -1)
	        unsqueeze_2: "b8[1, 80, 1][80, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_1, -1);  eq_1 = None
	        scalar_tensor_1: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_1: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_2, scalar_tensor_1, sum_4);  unsqueeze_2 = scalar_tensor_1 = sum_4 = None
	        full_1: "f32[80, 128][128, 1]cuda:0" = torch.ops.aten.full.default([80, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_2: "f32[80, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_1, [unsqueeze], where_1, True);  full_1 = unsqueeze = where_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        eq_2: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.eq.Scalar(index_put, 1024)
	        unsqueeze_3: "b8[256, 80, 1][80, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_2, -1);  eq_2 = None
	        scalar_tensor_2: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_3, scalar_tensor_2, slice_4);  unsqueeze_3 = scalar_tensor_2 = slice_4 = None
	        full_2: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.full.default([1025, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_3: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_2, [index_put], where_2, True);  full_2 = index_put = where_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        eq_3: "b8[256, 4][4, 1]cuda:0" = torch.ops.aten.eq.Scalar(add_1, 1024)
	        unsqueeze_4: "b8[256, 4, 1][4, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_3, -1);  eq_3 = None
	        scalar_tensor_3: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_4, scalar_tensor_3, slice_2);  unsqueeze_4 = scalar_tensor_3 = slice_2 = None
	        full_3: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.full.default([1025, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_4: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_3, [add_1], where_3, True);  full_3 = add_1 = where_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        add_5: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_3, index_put_4);  index_put_3 = index_put_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        eq_4: "b8[256, 1][1, 1]cuda:0" = torch.ops.aten.eq.Scalar(remainder, -1)
	        unsqueeze_5: "b8[256, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_4, -1);  eq_4 = None
	        scalar_tensor_4: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_4: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_5, scalar_tensor_4, slice_3);  unsqueeze_5 = scalar_tensor_4 = slice_3 = None
	        full_4: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.full.default([2000, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_5: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_4, [remainder], where_4, True);  full_4 = remainder = where_4 = None
	        return pytree.tree_unflatten([cat, add_4, cat_1, None, index_put_5, None, None, None, None, None, add_5, index_put_2, sum_3, index_put_1], self._out_spec)
	        
V0303 09:09:42.304957 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "3033c8b1dee349505c2b29d083021046"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "i64[256, 1][1, 1]cuda:0", primals_2: "f32[2000, 128][128, 1]cuda:0", primals_3: "i64[256, 80][80, 1]cuda:0", primals_4: "i64[256, 80][80, 1]cuda:0", primals_5: "b8[256, 80][80, 1]cuda:0", primals_6: "i64[256, 4][4, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0", primals_8: "f32[1025, 128][128, 1]cuda:0", primals_9: "f32[80, 128][128, 1]cuda:0", primals_10: "f32[128][1]cuda:0", primals_11: "f32[4, 128][128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        remainder: "i64[256, 1][1, 1]cuda:0" = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, 256);  primals_3 = None
	        add: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.add.Tensor(mul, primals_4);  mul = primals_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        bitwise_not: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.bitwise_not.default(primals_5)
	        full_default: "i64[][]cpu" = torch.ops.aten.full.default([], 1024, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        index_put: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.index_put.default(add, [bitwise_not], full_default);  add = bitwise_not = full_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_7, 256)
	        add_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1, primals_6);  mul_1 = primals_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        embedding_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_8, add_1, 1024)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_8, index_put, 1024);  primals_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:292 in _predict, code: seq_lengths = batch.seq_mask.sum(axis=1)
	        sum_1: "i64[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(primals_5, [1]);  primals_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:299 in _predict, code: pos = torch.arange(N, device=sem_ids_emb.device).unsqueeze(0)
	        iota: "i64[80][1]cuda:0" = torch.ops.prims.iota.default(80, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
	        unsqueeze: "i64[1, 80][80, 1]cuda:0" = torch.ops.aten.unsqueeze.default(iota, 0);  iota = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
	        embedding_3: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_9, unsqueeze);  primals_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
	        add_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding_3, embedding_2);  embedding_3 = embedding_2 = None
	        cat: "f32[256, 81, 128][10368, 128, 1]cuda:0" = torch.ops.aten.cat.default([embedding, add_2], 1);  embedding = add_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
	        repeat: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.repeat.default(primals_10, [256, 1, 1]);  primals_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
	        embedding_4: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_11, primals_7);  primals_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:308 in _predict, code: sem_ids_emb_fut + tte_fut
	        add_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding_1, embedding_4);  embedding_1 = embedding_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
	        cat_1: "f32[256, 5, 128][640, 128, 1]cuda:0" = torch.ops.aten.cat.default([repeat, add_3], 1);  repeat = add_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:313 in _predict, code: input_embedding = padded_to_jagged_tensor(input_embedding, lengths=seq_lengths+1)
	        add_4: "i64[256][1]cuda:0" = torch.ops.aten.add.Tensor(sum_1, 1);  sum_1 = None
	        return (cat, add_4, cat_1, primals_7, remainder, index_put, add_1, unsqueeze)
	        
V0303 09:09:42.306646 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "91455860bdcacf48bd47c211cb0e7bce"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_7: "i64[256, 4][4, 1]cuda:0", remainder: "i64[256, 1][1, 1]cuda:0", index_put: "i64[256, 80][80, 1]cuda:0", add_1: "i64[256, 4][4, 1]cuda:0", unsqueeze: "i64[1, 80][80, 1]cuda:0", tangents_1: "f32[256, 81, 128][10368, 128, 1]cuda:0", tangents_2: "f32[256, 5, 128][640, 128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
	        slice_1: "f32[256, 1, 128][640, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_2, 1, 0, 1)
	        slice_2: "f32[256, 4, 128][640, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_2, 1, 1, 5);  tangents_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
	        eq: "b8[256, 4][4, 1]cuda:0" = torch.ops.aten.eq.Scalar(primals_7, -1)
	        unsqueeze_1: "b8[256, 4, 1][4, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
	        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_1, full_default_1, slice_2);  unsqueeze_1 = None
	        full_default_2: "f32[4, 128][128, 1]cuda:0" = torch.ops.aten.full.default([4, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_1: "f32[4, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_2, [primals_7], where, True);  full_default_2 = primals_7 = where = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
	        sum_2: "f32[1, 128][128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_1, [0]);  slice_1 = None
	        sum_3: "f32[128][1]cuda:0" = torch.ops.aten.sum.dim_IntList(sum_2, [0]);  sum_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
	        slice_3: "f32[256, 1, 128][10368, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_1, 1, 0, 1)
	        slice_4: "f32[256, 80, 128][10368, 128, 1]cuda:0" = torch.ops.aten.slice.Tensor(tangents_1, 1, 1, 81);  tangents_1 = None
	        sum_4: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_4, [0], True)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
	        eq_1: "b8[1, 80][80, 1]cuda:0" = torch.ops.aten.eq.Scalar(unsqueeze, -1)
	        unsqueeze_2: "b8[1, 80, 1][80, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_1, -1);  eq_1 = None
	        where_1: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_2, full_default_1, sum_4);  unsqueeze_2 = sum_4 = None
	        full_default_4: "f32[80, 128][128, 1]cuda:0" = torch.ops.aten.full.default([80, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_2: "f32[80, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_4, [unsqueeze], where_1, True);  full_default_4 = unsqueeze = where_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        eq_2: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.eq.Scalar(index_put, 1024)
	        unsqueeze_3: "b8[256, 80, 1][80, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_2, -1);  eq_2 = None
	        where_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_3, full_default_1, slice_4);  unsqueeze_3 = slice_4 = None
	        full_default_6: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.full.default([1025, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_3: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_6, [index_put], where_2, True);  index_put = where_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        eq_3: "b8[256, 4][4, 1]cuda:0" = torch.ops.aten.eq.Scalar(add_1, 1024)
	        unsqueeze_4: "b8[256, 4, 1][4, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_3, -1);  eq_3 = None
	        where_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_4, full_default_1, slice_2);  unsqueeze_4 = slice_2 = None
	        index_put_4: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_6, [add_1], where_3, True);  full_default_6 = add_1 = where_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        add_5: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_3, index_put_4);  index_put_3 = index_put_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        eq_4: "b8[256, 1][1, 1]cuda:0" = torch.ops.aten.eq.Scalar(remainder, -1)
	        unsqueeze_5: "b8[256, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_4, -1);  eq_4 = None
	        where_4: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_5, full_default_1, slice_3);  unsqueeze_5 = full_default_1 = slice_3 = None
	        full_default_10: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.full.default([2000, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_5: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_10, [remainder], where_4, True);  full_default_10 = remainder = where_4 = None
	        return (None, index_put_5, None, None, None, None, None, add_5, index_put_2, sum_3, index_put_1)
	        
V0303 09:09:42.307215 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "23e2817c86ea9789ad0a5c12bfd7f569"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992982306863.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.307573 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "042475060b51573c32cebd497b9d928f"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992982307469.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.307747 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "9ee3f0dc2b5d91b7461ab2e211700e3a"}
	{
	"name": "inductor_compile",
	"ts": 1740992982307469.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.775799 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "5b132946b1344c0c5cb14317f57e3ac1"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11):
	        remainder = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        embedding = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        mul = torch.ops.aten.mul.Tensor(primals_3, 256);  primals_3 = None
	        add = torch.ops.aten.add.Tensor(mul, primals_4);  mul = primals_4 = None
	        bitwise_not = torch.ops.aten.bitwise_not.default(primals_5)
	        full_default = torch.ops.aten.full.default([], 1024, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        index_put = torch.ops.aten.index_put.default(add, [bitwise_not], full_default);  add = bitwise_not = full_default = None
	        mul_1 = torch.ops.aten.mul.Tensor(primals_7, 256)
	        add_1 = torch.ops.aten.add.Tensor(mul_1, primals_6);  mul_1 = primals_6 = None
	        embedding_1 = torch.ops.aten.embedding.default(primals_8, add_1, 1024)
	        embedding_2 = torch.ops.aten.embedding.default(primals_8, index_put, 1024);  primals_8 = None
	        sum_1 = torch.ops.aten.sum.dim_IntList(primals_5, [1]);  primals_5 = None
	        iota = torch.ops.prims.iota.default(80, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
	        unsqueeze = torch.ops.aten.unsqueeze.default(iota, 0);  iota = None
	        embedding_3 = torch.ops.aten.embedding.default(primals_9, unsqueeze);  primals_9 = None
	        add_2 = torch.ops.aten.add.Tensor(embedding_3, embedding_2);  embedding_3 = embedding_2 = None
	        cat = torch.ops.aten.cat.default([embedding, add_2], 1);  embedding = add_2 = None
	        repeat = torch.ops.aten.repeat.default(primals_10, [256, 1, 1]);  primals_10 = None
	        embedding_4 = torch.ops.aten.embedding.default(primals_11, primals_7);  primals_11 = None
	        add_3 = torch.ops.aten.add.Tensor(embedding_1, embedding_4);  embedding_1 = embedding_4 = None
	        cat_1 = torch.ops.aten.cat.default([repeat, add_3], 1);  repeat = add_3 = None
	        add_4 = torch.ops.aten.add.Tensor(sum_1, 1);  sum_1 = None
	        return (cat, add_4, cat_1, primals_7, remainder, index_put, add_1, unsqueeze)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 2048, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf0, (256, 1), dtype=torch.int64, is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 1024000, device=device(type='cuda', index=0))
	    reader.tensor(buf1, (2000, 128), is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 163840, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf2, (256, 80), dtype=torch.int64, is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 163840, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf3, (256, 80), dtype=torch.int64, is_leaf=True)  # primals_4
	    buf4 = reader.storage(None, 20480, device=device(type='cuda', index=0), dtype_hint=torch.bool)
	    reader.tensor(buf4, (256, 80), dtype=torch.bool, is_leaf=True)  # primals_5
	    buf5 = reader.storage(None, 8192, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf5, (256, 4), dtype=torch.int64, is_leaf=True)  # primals_6
	    buf6 = reader.storage(None, 8192, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf6, (256, 4), dtype=torch.int64, is_leaf=True)  # primals_7
	    buf7 = reader.storage(None, 524800, device=device(type='cuda', index=0))
	    reader.tensor(buf7, (1025, 128), is_leaf=True)  # primals_8
	    buf8 = reader.storage(None, 40960, device=device(type='cuda', index=0))
	    reader.tensor(buf8, (80, 128), is_leaf=True)  # primals_9
	    buf9 = reader.storage(None, 512, device=device(type='cuda', index=0))
	    reader.tensor(buf9, (128,), is_leaf=True)  # primals_10
	    buf10 = reader.storage(None, 2048, device=device(type='cuda', index=0))
	    reader.tensor(buf10, (4, 128), is_leaf=True)  # primals_11
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='real', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='real', check_str=None)
	        # mod(*args)
V0303 09:09:42.807964 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "2c6d9f1e765451ef04786b906379bf98"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "i64[256, 1][1, 1]cuda:0", primals_2: "f32[2000, 128][128, 1]cuda:0", primals_3: "i64[256, 80][80, 1]cuda:0", primals_4: "i64[256, 80][80, 1]cuda:0", primals_5: "b8[256, 80][80, 1]cuda:0", primals_6: "i64[256, 4][4, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0", primals_8: "f32[1025, 128][128, 1]cuda:0", primals_9: "f32[80, 128][128, 1]cuda:0", primals_10: "f32[128][1]cuda:0", primals_11: "f32[4, 128][128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        remainder: "i64[256, 1][1, 1]cuda:0" = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_3, 256);  primals_3 = None
	        add: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.add.Tensor(mul, primals_4);  mul = primals_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        bitwise_not: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.bitwise_not.default(primals_5)
	        full_default: "i64[][]cpu" = torch.ops.aten.full.default([], 1024, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        index_put: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.index_put_.default(add, [bitwise_not], full_default);  add = bitwise_not = full_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_7, 256)
	        add_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1, primals_6);  mul_1 = primals_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        embedding_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_8, add_1, 1024)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_8, index_put, 1024);  primals_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:292 in _predict, code: seq_lengths = batch.seq_mask.sum(axis=1)
	        sum_1: "i64[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(primals_5, [1]);  primals_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:299 in _predict, code: pos = torch.arange(N, device=sem_ids_emb.device).unsqueeze(0)
	        iota: "i64[80][1]cuda:0" = torch.ops.prims.iota.default(80, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
	        unsqueeze: "i64[1, 80][80, 1]cuda:0" = torch.ops.aten.unsqueeze.default(iota, 0);  iota = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
	        embedding_3: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_9, unsqueeze);  primals_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
	        add_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding_3, embedding_2);  embedding_3 = embedding_2 = None
	        cat: "f32[256, 81, 128][10368, 128, 1]cuda:0" = torch.ops.aten.cat.default([embedding, add_2], 1);  embedding = add_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
	        repeat: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.repeat.default(primals_10, [256, 1, 1]);  primals_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
	        embedding_4: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_11, primals_7);  primals_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:308 in _predict, code: sem_ids_emb_fut + tte_fut
	        add_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.add.Tensor(embedding_1, embedding_4);  embedding_1 = embedding_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
	        cat_1: "f32[256, 5, 128][640, 128, 1]cuda:0" = torch.ops.aten.cat.default([repeat, add_3], 1);  repeat = add_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:313 in _predict, code: input_embedding = padded_to_jagged_tensor(input_embedding, lengths=seq_lengths+1)
	        add_4: "i64[256][1]cuda:0" = torch.ops.aten.add.Tensor(sum_1, 1);  sum_1 = None
	        return (cat, add_4, cat_1, primals_7, remainder, index_put, add_1, unsqueeze)
	        
V0303 09:09:42.812829 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "f51d1e312f84c685c3c62ca63bc530e0"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992982812696.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.859790 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "4c40e28e0418d3061fa217cd9f2b13c4"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992982859714.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.860205 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "c82c13722196896accb96b440d56932e"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992982860133.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.860391 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "cce76d6df909357b94061828774298d1"}
	{
	"name": "code_gen",
	"ts": 1740992982860133.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.862417 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "029a40322d65fc5f820f953f9d9df831"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992982862347.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.922939 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "fd2a0ba6d6fcec945db2a8e1d397443b"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992982922887.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:42.923192 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "b40398c0d402fced879bdbcf899a70a8"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992982923127.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.082509 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "7603972a3791b7d5821ae336c4272f8a"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983082249.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.083088 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "06422f9246e0992bffbc54905b735ad0"}
	{
	"name": "code_gen",
	"ts": 1740992983083029.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.083307 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "9846363f8ba26bb6b2c9a6adc8905b64"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983083261.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.084056 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "7374f386635911a379e43be9dc3a96d3"}
	{
	"name": "inductor_compile",
	"ts": 1740992983084005.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.084266 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "5bc1009f1e5d1be1ddf7a20a5938d3c9"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983084221.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.085133 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "fcec1164fb1b3468624d3cb4301b293b"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983085080.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.088232 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "a51e6cc574d1564e8e15bea5d77d8e99"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983088177.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.088634 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "d9ca695d3da7610fc86489c1c1bfe783"}
	{
	"name": "backend_compile",
	"ts": 1740992983088587.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.088829 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "abad16c77fc569043231dfa716b9a8ab"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983088784.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.089620 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "cee9fefa8bbeea351932afa5e8e25829"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983089570.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.089817 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1, "has_payload": "6826df1bfe09a4616007d3460e7ae4c4"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983089774.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 1,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.090175 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "1/0", "frame_key": "2", "co_name": "_predict", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/model.py", "co_firstlineno": 288, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992981.8855422, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpsodhu1ka/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpsodhu1ka/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpsodhu1ka', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 1.2044765949249268, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 1, "frame_compile_id": 0, "attempt": 1}
V0303 09:09:43.095926 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py", 6]}
V0303 09:09:43.096162 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 289, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 50, "name": "forward", "filename": 6}]}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.096499 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "51f0226ef2c237c9abd37f5f0ff27cf7"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983096390.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.096747 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "bb5e9478c78a90cc3cc479bb1b7bd7ba"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983096390.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.099513 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 8, "size": 2048}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.099896 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 1], "is_leaf": true, "stride": [1, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5d60>", "describer_id": 8}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.100133 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 8, "id": 0, "source": "L['x']"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.107470 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 8, "size": 1024000}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.107838 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [2000, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0aa40>", "describer_id": 8}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.108009 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 8, "id": 1, "source": "L['self']._modules['emb']._parameters['weight']"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.111245 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_x_": [256, 1], "l_self_modules_emb_parameters_weight_": [2000, 128], "hashed_indices": [256, 1], "embedding": [256, 1, 128]}}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "ffa731ca0fa006879a91b44c9fae9498"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "i64[256, 1][1, 1]cuda:0", L_self_modules_emb_parameters_weight_: "f32[2000, 128][128, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_emb_parameters_weight_ = L_self_modules_emb_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        hashed_indices: "i64[256, 1][1, 1]cuda:0" = l_x_ % 2000;  l_x_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.nn.functional.embedding(hashed_indices, l_self_modules_emb_parameters_weight_, None, None, 2.0, False, False);  hashed_indices = l_self_modules_emb_parameters_weight_ = None
	        return (embedding,)
	        
V0303 09:09:43.111708 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "e8128337de1e460ea704338fa81bbb5f"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983111637.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.111894 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "7e9442b522aeb9a2fb18b3977112288f"}
	{
	"name": "backend_compile",
	"ts": 1740992983111637.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.113908 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "331b46bc4332a26dedebe9d5c63563ae"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983113845.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.133847 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "d03fff73f4fe3dbf0a966592627316d0"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "i64[256, 1][1, 1]cuda:0"; primals_2: "f32[2000, 128][128, 1]cuda:0"; tangents_1: "f32[256, 1, 128][128, 128, 1]cuda:0"; 
	    
	        primals_1, primals_2, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        remainder: "i64[256, 1][1, 1]cuda:0" = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        eq: "b8[256, 1][1, 1]cuda:0" = torch.ops.aten.eq.Scalar(remainder, -1)
	        unsqueeze: "b8[256, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
	        scalar_tensor: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze, scalar_tensor, tangents_1);  unsqueeze = scalar_tensor = tangents_1 = None
	        full: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.full.default([2000, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full, [remainder], where, True);  full = remainder = where = None
	        return pytree.tree_unflatten([embedding, None, index_put], self._out_spec)
	        
V0303 09:09:43.141827 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "7a5f1bc437296a81e20808c77acaaabf"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "i64[256, 1][1, 1]cuda:0", primals_2: "f32[2000, 128][128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        remainder: "i64[256, 1][1, 1]cuda:0" = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        return (embedding, remainder)
	        
V0303 09:09:43.142439 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "5eba123ee933e9130868829210e162e1"}
	class GraphModule(torch.nn.Module):
	    def forward(self, remainder: "i64[256, 1][1, 1]cuda:0", tangents_1: "f32[256, 1, 128][128, 128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        eq: "b8[256, 1][1, 1]cuda:0" = torch.ops.aten.eq.Scalar(remainder, -1)
	        unsqueeze: "b8[256, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
	        full_default: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze, full_default, tangents_1);  unsqueeze = full_default = tangents_1 = None
	        full_default_1: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.full.default([2000, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put: "f32[2000, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_1, [remainder], where, True);  full_default_1 = remainder = where = None
	        return (None, index_put)
	        
V0303 09:09:43.142837 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "102c631ac1b79385156367baec346d18"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983142630.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.143124 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "fc1f80f244a6c58e44bcfd2935bd950b"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983143064.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.143301 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "f553f7e0073f13960b99549b30809fed"}
	{
	"name": "inductor_compile",
	"ts": 1740992983143064.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.148150 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "db83a80774da7347fc29e05f6fbfa077"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2):
	        remainder = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        embedding = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        return (embedding, remainder)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 2048, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf0, (256, 1), dtype=torch.int64, is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 1024000, device=device(type='cuda', index=0))
	    reader.tensor(buf1, (2000, 128), is_leaf=True)  # primals_2
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='real', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='real', check_str=None)
	        # mod(*args)
V0303 09:09:43.150165 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "7a5f1bc437296a81e20808c77acaaabf"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "i64[256, 1][1, 1]cuda:0", primals_2: "f32[2000, 128][128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
	        remainder: "i64[256, 1][1, 1]cuda:0" = torch.ops.aten.remainder.Scalar(primals_1, 2000);  primals_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
	        embedding: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_2, remainder);  primals_2 = None
	        return (embedding, remainder)
	        
V0303 09:09:43.150698 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "b744c9d8014467d489fb2f58998fdc1e"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992983150632.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.152908 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "5d07d1bf048d492418fda15284b1b460"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992983152856.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.153194 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "39de706ac244972683d9c49a518d0d10"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983153135.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.153459 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "6b152686048693fb6383ff263c36598b"}
	{
	"name": "code_gen",
	"ts": 1740992983153135.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.154159 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "c46e0ae70c6c888fe72c34b8e202cf7d"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992983154093.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.163681 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "e880a2d801b94401fa6d32cd60a2eb40"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992983163632.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.163930 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "9e86c90f4527c35bb76eea2bfa5f4c4d"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983163869.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.282196 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "32b1e13e13dd1ad07c850dd922d003be"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983281914.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.282781 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "d01b72fe7eb019bf6428276bef88d48a"}
	{
	"name": "code_gen",
	"ts": 1740992983282715.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.283259 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "e69ff09c58e95fcf2584217a85828227"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983282949.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.283987 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "9a13c99c119230bd9edcfe025c780c43"}
	{
	"name": "inductor_compile",
	"ts": 1740992983283934.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.284192 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "f0e8105fb55cfca7d3206b3d86782870"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983284150.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.284960 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "c5ffb369de41533e3bf37fa972566360"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983284908.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.287990 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "4c8c3d16c58f1d3f5ca863ee8d20d9f5"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983287936.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.288390 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "25141df5d631dc5ed4c67e15ba993e0d"}
	{
	"name": "backend_compile",
	"ts": 1740992983288343.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.288580 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "4555b375a35b130a9811e62ba7ff37af"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983288539.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.289216 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "10ddfbba291e4ca996193bcf075a6f4e"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983289168.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.289411 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0, "has_payload": "ab242a89d1a5d6565073a7a0aa72942f"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983289369.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 2,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.289752 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "2/0", "frame_key": "3", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py", "co_firstlineno": 50, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992983.096377, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmppna1ys0p/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmppna1ys0p/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmppna1ys0p', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.1932237148284912, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 2, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.310746 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 290, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 28, "name": "forward", "filename": 6}]}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.311091 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "7b388b3e7e05398456d3cc8d28b76143"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983310987.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.311284 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "6de6f5eecf0e40d63ded247666d4a24b"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983310987.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.313870 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 10, "size": 163840}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.314173 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "stride": [80, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4180>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.314334 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 0, "source": "L['batch'][4]"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.316134 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 10, "size": 163840}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.316448 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [5120, 4], "is_leaf": true, "stride": [4, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba01ad5db0>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.316682 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 1, "base": 2, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4040>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.316839 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 1, "source": "L['batch'][1]"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.318640 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 10, "size": 20480}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.318941 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 20, 4], "is_leaf": true, "stride": [80, 4, 1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fba18a41180>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.319161 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.bool", "device": "device(type='cuda', index=0)", "size": [256, 80], "is_leaf": true, "is_view": true, "stride": [80, 1], "storage": 2, "base": 4, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4090>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.319310 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 3, "source": "L['batch'][3]"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.321095 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 10, "size": 8192}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.321395 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b40e0>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.321623 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "is_view": true, "stride": [4, 1], "storage": 3, "base": 6, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4270>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.321815 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 5, "source": "L['batch'][2]"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.323107 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 10, "size": 8192}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.323365 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4220>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.323520 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 7, "source": "L['batch'][5]"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.327201 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 10, "size": 524800}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.327478 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 8, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1025, 128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [128, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cf90>", "describer_id": 10}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.327638 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 10, "id": 8, "source": "L['self']._modules['emb']._parameters['weight']"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.333705 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_batch_4_": [256, 80], "l_batch_1_": [256, 80], "l_batch_3_": [256, 80], "l_batch_2_": [256, 4], "l_batch_5_": [256, 4], "l_self_modules_emb_parameters_weight_": [1025, 128], "mul": [256, 80], "sem_ids": [256, 80], "invert": [256, 80], "mul_1": [256, 4], "sem_ids_fut": [256, 4], "sem_ids_fut_1": [256, 4, 128], "embedding_1": [256, 80, 128]}}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "624b69770129469f47114dfa28d4d04f"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_batch_4_: "i64[256, 80][80, 1]cuda:0", L_batch_1_: "i64[256, 80][80, 1]cuda:0", L_batch_3_: "b8[256, 80][80, 1]cuda:0", L_batch_2_: "i64[256, 4][4, 1]cuda:0", L_batch_5_: "i64[256, 4][4, 1]cuda:0", L_self_modules_emb_parameters_weight_: "f32[1025, 128][128, 1]cuda:0"):
	        l_batch_4_ = L_batch_4_
	        l_batch_1_ = L_batch_1_
	        l_batch_3_ = L_batch_3_
	        l_batch_2_ = L_batch_2_
	        l_batch_5_ = L_batch_5_
	        l_self_modules_emb_parameters_weight_ = L_self_modules_emb_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = l_batch_4_ * 256;  l_batch_4_ = None
	        sem_ids: "i64[256, 80][80, 1]cuda:0" = mul + l_batch_1_;  mul = l_batch_1_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        invert: "b8[256, 80][80, 1]cuda:0" = ~l_batch_3_;  l_batch_3_ = None
	        sem_ids[invert] = 1024;  setitem = sem_ids;  invert = setitem = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = l_batch_5_ * 256;  l_batch_5_ = None
	        sem_ids_fut: "i64[256, 4][4, 1]cuda:0" = mul_1 + l_batch_2_;  mul_1 = l_batch_2_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        sem_ids_fut_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.nn.functional.embedding(sem_ids_fut, l_self_modules_emb_parameters_weight_, 1024, None, 2.0, False, False);  sem_ids_fut = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_1: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.nn.functional.embedding(sem_ids, l_self_modules_emb_parameters_weight_, 1024, None, 2.0, False, False);  sem_ids = l_self_modules_emb_parameters_weight_ = None
	        return (embedding_1, sem_ids_fut_1)
	        
V0303 09:09:43.334182 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "bf73b7fc23aadb3d017d6694c061c0e8"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983334115.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.334367 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "91fd70f020e749141a2c7308c5c5abce"}
	{
	"name": "backend_compile",
	"ts": 1740992983334115.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.339719 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "5746337100de5b2b75826c6eacfd186c"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983339658.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.381683 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b0d07fb783b1732337f779de1420580e"}
	class joint_helper(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "i64[256, 80][80, 1]cuda:0"; primals_2: "i64[256, 80][80, 1]cuda:0"; primals_3: "b8[256, 80][80, 1]cuda:0"; primals_4: "i64[256, 4][4, 1]cuda:0"; primals_5: "i64[256, 4][4, 1]cuda:0"; primals_6: "f32[1025, 128][128, 1]cuda:0"; tangents_1: "f32[256, 80, 128][10240, 128, 1]cuda:0"; tangents_2: "f32[256, 4, 128][512, 128, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, 256);  primals_1 = None
	        add: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.add.Tensor(mul, primals_2);  mul = primals_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        bitwise_not: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.bitwise_not.default(primals_3);  primals_3 = None
	        _tensor_constant0 = self._tensor_constant0
	        lift_fresh_copy: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
	        index_put: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.index_put.default(add, [bitwise_not], lift_fresh_copy);  add = bitwise_not = lift_fresh_copy = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_5, 256);  primals_5 = None
	        add_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1, primals_4);  mul_1 = primals_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        embedding: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_6, add_1, 1024)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_1: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_6, index_put, 1024);  primals_6 = None
	        eq: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.eq.Scalar(index_put, 1024)
	        unsqueeze: "b8[256, 80, 1][80, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
	        scalar_tensor: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze, scalar_tensor, tangents_1);  unsqueeze = scalar_tensor = tangents_1 = None
	        full: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.full.default([1025, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_1: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full, [index_put], where, True);  full = index_put = where = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        eq_1: "b8[256, 4][4, 1]cuda:0" = torch.ops.aten.eq.Scalar(add_1, 1024)
	        unsqueeze_1: "b8[256, 4, 1][4, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_1, -1);  eq_1 = None
	        scalar_tensor_1: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_1, scalar_tensor_1, tangents_2);  unsqueeze_1 = scalar_tensor_1 = tangents_2 = None
	        full_1: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.full.default([1025, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_2: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_1, [add_1], where_1, True);  full_1 = add_1 = where_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        add_2: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_1, index_put_2);  index_put_1 = index_put_2 = None
	        return pytree.tree_unflatten([embedding_1, embedding, None, None, None, None, None, add_2], self._out_spec)
	        
V0303 09:09:43.396331 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b826dd65b0b17485e6b2aa648e63737b"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "i64[256, 80][80, 1]cuda:0", primals_2: "i64[256, 80][80, 1]cuda:0", primals_3: "b8[256, 80][80, 1]cuda:0", primals_4: "i64[256, 4][4, 1]cuda:0", primals_5: "i64[256, 4][4, 1]cuda:0", primals_6: "f32[1025, 128][128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, 256);  primals_1 = None
	        add: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.add.Tensor(mul, primals_2);  mul = primals_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        bitwise_not: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.bitwise_not.default(primals_3);  primals_3 = None
	        full_default: "i64[][]cpu" = torch.ops.aten.full.default([], 1024, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        index_put: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.index_put.default(add, [bitwise_not], full_default);  add = bitwise_not = full_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_5, 256);  primals_5 = None
	        add_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1, primals_4);  mul_1 = primals_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        embedding: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_6, add_1, 1024)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_1: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_6, index_put, 1024);  primals_6 = None
	        return (embedding_1, embedding, index_put, add_1)
	        
V0303 09:09:43.397177 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "5cd84269ed21845e4e48e6f8d9c098b1"}
	class GraphModule(torch.nn.Module):
	    def forward(self, index_put: "i64[256, 80][80, 1]cuda:0", add_1: "i64[256, 4][4, 1]cuda:0", tangents_1: "f32[256, 80, 128][10240, 128, 1]cuda:0", tangents_2: "f32[256, 4, 128][512, 128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        eq: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.eq.Scalar(index_put, 1024)
	        unsqueeze: "b8[256, 80, 1][80, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
	        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze, full_default_1, tangents_1);  unsqueeze = tangents_1 = None
	        full_default_2: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.full.default([1025, 128], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        index_put_1: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_2, [index_put], where, True);  index_put = where = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        eq_1: "b8[256, 4][4, 1]cuda:0" = torch.ops.aten.eq.Scalar(add_1, 1024)
	        unsqueeze_1: "b8[256, 4, 1][4, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(eq_1, -1);  eq_1 = None
	        where_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.where.self(unsqueeze_1, full_default_1, tangents_2);  unsqueeze_1 = full_default_1 = tangents_2 = None
	        index_put_2: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.index_put.default(full_default_2, [add_1], where_1, True);  full_default_2 = add_1 = where_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        add_2: "f32[1025, 128][128, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_1, index_put_2);  index_put_1 = index_put_2 = None
	        return (None, None, None, None, None, add_2)
	        
V0303 09:09:43.397580 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "38424f7967956b62214de575f0022781"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983397374.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.397887 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "68c662d9b3ed7526ca8f08bb9b6ea221"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983397819.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.398065 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f677871d35e2d5b601299212c49d291b"}
	{
	"name": "inductor_compile",
	"ts": 1740992983397819.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.405200 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "245d0d7c3461c7a308407328fedac1aa"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6):
	        mul = torch.ops.aten.mul.Tensor(primals_1, 256);  primals_1 = None
	        add = torch.ops.aten.add.Tensor(mul, primals_2);  mul = primals_2 = None
	        bitwise_not = torch.ops.aten.bitwise_not.default(primals_3);  primals_3 = None
	        full_default = torch.ops.aten.full.default([], 1024, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        index_put = torch.ops.aten.index_put.default(add, [bitwise_not], full_default);  add = bitwise_not = full_default = None
	        mul_1 = torch.ops.aten.mul.Tensor(primals_5, 256);  primals_5 = None
	        add_1 = torch.ops.aten.add.Tensor(mul_1, primals_4);  mul_1 = primals_4 = None
	        embedding = torch.ops.aten.embedding.default(primals_6, add_1, 1024)
	        embedding_1 = torch.ops.aten.embedding.default(primals_6, index_put, 1024);  primals_6 = None
	        return (embedding_1, embedding, index_put, add_1)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 163840, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf0, (256, 80), dtype=torch.int64, is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 163840, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (256, 80), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 20480, device=device(type='cuda', index=0), dtype_hint=torch.bool)
	    reader.tensor(buf2, (256, 80), dtype=torch.bool, is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 8192, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf3, (256, 4), dtype=torch.int64, is_leaf=True)  # primals_4
	    buf4 = reader.storage(None, 8192, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf4, (256, 4), dtype=torch.int64, is_leaf=True)  # primals_5
	    buf5 = reader.storage(None, 524800, device=device(type='cuda', index=0))
	    reader.tensor(buf5, (1025, 128), is_leaf=True)  # primals_6
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='real', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='real', check_str=None)
	        # mod(*args)
V0303 09:09:43.410806 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "3cb319b2e8942a9328eea50555ad28c0"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "i64[256, 80][80, 1]cuda:0", primals_2: "i64[256, 80][80, 1]cuda:0", primals_3: "b8[256, 80][80, 1]cuda:0", primals_4: "i64[256, 4][4, 1]cuda:0", primals_5: "i64[256, 4][4, 1]cuda:0", primals_6: "f32[1025, 128][128, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
	        mul: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, 256);  primals_1 = None
	        add: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.add.Tensor(mul, primals_2);  mul = primals_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
	        bitwise_not: "b8[256, 80][80, 1]cuda:0" = torch.ops.aten.bitwise_not.default(primals_3);  primals_3 = None
	        full_default: "i64[][]cpu" = torch.ops.aten.full.default([], 1024, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        index_put: "i64[256, 80][80, 1]cuda:0" = torch.ops.aten.index_put_.default(add, [bitwise_not], full_default);  add = bitwise_not = full_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
	        mul_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_5, 256);  primals_5 = None
	        add_1: "i64[256, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1, primals_4);  mul_1 = primals_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
	        embedding: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_6, add_1, 1024)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
	        embedding_1: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.ops.aten.embedding.default(primals_6, index_put, 1024);  primals_6 = None
	        return (embedding_1, embedding, index_put, add_1)
	        
V0303 09:09:43.411375 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "b078e661cdbdcaf1aec7116fd646f488"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992983411309.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.417716 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "577f4fe984cbbb3cb579d19669421d31"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992983417663.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.418056 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "4a3bb1a81123f016850863cab24b832a"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983417991.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.418237 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "958dd75f49bb05ab18feba547ec6f345"}
	{
	"name": "code_gen",
	"ts": 1740992983417991.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.419084 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "823e9690b8e129ee6cffea9b674caf09"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992983419024.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.434653 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "052121fec434af760a357f59d88ada13"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992983434602.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.434927 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "e80f0f8262c57c7dc2723f29c6593acd"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983434865.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.555553 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "5082e7a9ad5d4fced912380fcd6659aa"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983555297.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.556139 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "8f27e8e0cb4dfb191dbdee99548d58d5"}
	{
	"name": "code_gen",
	"ts": 1740992983556077.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.556360 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "7cd778153d6408e79e83fe6cf94d41ca"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983556314.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.557081 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "7efab59c2051de806155ceee0b18c4ca"}
	{
	"name": "inductor_compile",
	"ts": 1740992983557031.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.557286 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "a8ae3efb1ab206209e0b94a453a547cf"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983557245.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.558032 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "da8ac21c50c351d9bc4f1281782357ee"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983557982.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.561321 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f89b7ee2a470bcf781c2a811c47cac62"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983561268.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.561711 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "f99dbfc379814ba8735a2d62f6bf0b5d"}
	{
	"name": "backend_compile",
	"ts": 1740992983561665.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.561908 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "8687652acd0e4dd73716065cbaf9cbe3"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983561868.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.562587 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "9923115fb1bdceac47045ee8fee1a1ba"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983562539.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.562778 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0, "has_payload": "bd8ba170e0db97ef46c1c4149e64353b"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983562738.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 3,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.563113 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "3/0", "frame_key": "4", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py", "co_firstlineno": 28, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992983.3109763, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmptz83dpct/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmptz83dpct/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmptz83dpct', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.25199103355407715, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 3, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.590029 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py", 7]}
V0303 09:09:43.590233 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 318, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 30, "name": "forward", "filename": 7}]}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.590475 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "b97b720af1bd4643eb1f5b0bb6986be0"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983590392.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.590647 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "a6c465e697465bb1715ccdbb504e1781"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983590392.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.593463 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 12, "size": 2097152}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.593747 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 12, "size": 2097152}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.594022 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 128], "is_leaf": true, "stride": [128, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41323b0>", "describer_id": 12}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.594193 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 12, "size": 2056}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.594415 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 12}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.594577 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 12, "size": 0}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.594950 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 12}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.595121 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 12, "size": 0}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.595344 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 12}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.596022 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 128], "layout": "torch.jagged", "is_leaf": true, "is_view": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["128*j1", 128, 1], "storage": 0, "base": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "creation_meta": "CreationMeta.NO_GRAD_MODE", "ctx": "{'requires_grad': False, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9f4132720>", "describer_id": 12}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.596210 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 4, "source": "L['x']"}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.652196 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 12, "size": 512}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.652472 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 9, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [128], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b0cdb0>", "describer_id": 12}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.652628 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 12, "id": 9, "source": "L['self']._parameters['weight']"}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.658674 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_parameters_weight_": [128]}}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "fbb6c4ff0cda783a98a0e638b2e8c57d"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "f32[256, s1, 128][128*s1, 128, 1]cuda:0", s1: "Sym(s1)", L_self_parameters_weight_: "f32[128][1]cuda:0"):
	        l_x_ = L_x_
	        l_self_parameters_weight_ = L_self_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_1: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = l_x_.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = float_1.pow(2)
	        mean: "f32[256, s1, 1][s1, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        add: "f32[256, s1, 1][s1, 1, 1]cuda:0" = mean + 1e-06;  mean = None
	        rsqrt: "f32[256, s1, 1][s1, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        mul: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = float_1 * rsqrt;  float_1 = rsqrt = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = mul.type_as(l_x_);  mul = l_x_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = output * l_self_parameters_weight_;  output = l_self_parameters_weight_ = None
	        return (mul_1,)
	        
V0303 09:09:43.659108 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "13040d78ed5676150ecee2616e25bd83"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983659042.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.659285 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "a8f5c46a2552654cbac97cdba73c01a3"}
	{
	"name": "backend_compile",
	"ts": 1740992983659042.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.688279 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "059f83fd95a349b1d480f0846ece0741"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983688218.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.774889 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "95cc1d8ae67a9dc68befe9a9a42d14d6"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s0, 128][128, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s1)"; primals_6: "f32[128][1]cuda:0"; tangents_1: "f32[s0, 128][128, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt);  primals_1 = rsqrt = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  primals_6 = None
	        mul_2: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul);  tangents_1 = mul = None
	        sum_1: "f32[1, 128][128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_2, [0], True);  mul_2 = None
	        unsqueeze: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view: "f32[128][1]cuda:0" = torch.ops.aten.view.default(unsqueeze, [128]);  unsqueeze = None
	        return pytree.tree_unflatten([mul_1, primals_2, primals_3, primals_4, None, None, view], self._out_spec)
	        
V0303 09:09:43.785547 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "91b1d93cddf3a0f19820681b6ae744eb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s0, 128][128, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s1)", primals_6: "f32[128][1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = primals_6 = None
	        return (mul_1, primals_2, primals_3, primals_4, primals_1, rsqrt)
	        
V0303 09:09:43.786254 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "40b7c5712dae9845f2a1cc46bb2fe54b"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s0, 128][128, 1]cuda:0", rsqrt: "f32[s0, 1][1, 1]cuda:0", tangents_1: "f32[s0, 128][128, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cuda:0", tangents_4: "f32[s4, 0][1, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt);  primals_1 = rsqrt = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_2: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul);  tangents_1 = mul = None
	        sum_1: "f32[1, 128][128, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_2, [0], True);  mul_2 = None
	        unsqueeze: "f32[1, 1, 128][128, 128, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view: "f32[128][1]cuda:0" = torch.ops.aten.view.default(unsqueeze, [128]);  unsqueeze = None
	        return (None, None, view)
	        
V0303 09:09:43.786647 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "8b57d4db7488304b5dbc94d7b061d4e0"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983786444.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.786931 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "3c1f6a082edc5f5846a33f6f9177cb66"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983786872.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.787099 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "fcd8f2e9cab5941dc030efd98ab48148"}
	{
	"name": "inductor_compile",
	"ts": 1740992983786872.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.793612 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "137570565cb6abc124b9211bbaed8769"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6):
	        pow_1 = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt = torch.ops.aten.rsqrt.default(add);  add = None
	        mul = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        mul_1 = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = primals_6 = None
	        return (mul_1, primals_2, primals_3, primals_4, primals_1, rsqrt)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 512*s0, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s0, 128), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j1)  # primals_5
	    buf4 = reader.storage(None, 512, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (128,), is_leaf=True)  # primals_6
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:09:43.802449 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "91b1d93cddf3a0f19820681b6ae744eb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s0, 128][128, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s1)", primals_6: "f32[128][1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = primals_6 = None
	        return (mul_1, primals_2, primals_3, primals_4, primals_1, rsqrt)
	        
V0303 09:09:43.802986 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "f49fb5758577ec673ff9c1096c4efba7"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992983802922.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.811305 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "e3fc8749e770c6a745650b1ab2ff41b5"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992983811253.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.811652 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "837fdc20c3c5dc9691e9dfe1fba940ab"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983811591.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.811826 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "3004df2ce56e00d1a644707309a06079"}
	{
	"name": "code_gen",
	"ts": 1740992983811591.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.812499 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "938d94a4fd4f2ef74a40e2c69662610b"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992983812433.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.823569 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "65f1b5e19d0c5d0dae28386668849cfe"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992983823517.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.823813 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "2ee784b95b44ca229520ef790f6da39f"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983823752.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.978231 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "f7e422c3d8ef9db6d6507cdb714f47e4"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992983977936.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.978820 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "50058962db9748832e137d3eea9cf9bc"}
	{
	"name": "code_gen",
	"ts": 1740992983978754.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.979038 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "893bfd6382dc10705e2c9aa4b054f171"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992983978991.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.979766 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "47637c5b0a58187798ab1cb901d74080"}
	{
	"name": "inductor_compile",
	"ts": 1740992983979713.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.979972 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "472fdb0b3a66674060c0148d40783d60"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992983979928.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.980702 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "6c0d3d0ffc326fe9b2bc8f7b6c5c4d38"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992983980649.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.983823 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "35474afb0290094452fc390be6e584e0"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992983983769.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.984275 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "529746c01d30f293b617dc38ff0bfdc5"}
	{
	"name": "backend_compile",
	"ts": 1740992983984226.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.984467 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "3cb9494c9a85025cbc33fe2d436fd432"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992983984427.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.985139 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "8e611b729fd9e52aa85fc9616243cf24"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983985089.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.985344 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0, "has_payload": "89c6a4bc28971adef11a8eeee6475727"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983985300.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 4,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.985687 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "4/0", "frame_key": "6", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py", "co_firstlineno": 30, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992983.5903823, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp733we275/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmp733we275/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp733we275', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.3951537609100342, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 4, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.990627 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 318, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 27, "name": "_norm", "filename": 7}]}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.990942 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "90ac486167f93c195d88d60ff5c72d0d"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992983990834.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.991186 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "b33d77398446650581af7dfda99bf17d"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992983990834.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:43.995098 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 14, "size": 2097152}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.995493 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 14, "size": 2097152}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.995843 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 128], "is_leaf": true, "stride": [128, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41323b0>", "describer_id": 14}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.996074 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 14, "size": 2056}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.996396 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 14}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.996635 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 14, "size": 0}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.996946 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 14}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.997170 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 14, "size": 0}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.997479 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 14}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.998766 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 128], "layout": "torch.jagged", "is_leaf": true, "is_view": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["128*j1", 128, 1], "storage": 0, "base": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "creation_meta": "CreationMeta.NO_GRAD_MODE", "ctx": "{'requires_grad': False, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9f4132720>", "describer_id": 14}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:43.999008 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 14, "id": 4, "source": "L['x']"}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.033267 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {}}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "60e107a908fbfaa0e62c9e6129116d4d"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "f32[256, s1, 128][128*s1, 128, 1]cuda:0", s1: "Sym(s1)"):
	        l_x_ = L_x_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = l_x_.pow(2)
	        mean: "f32[256, s1, 1][s1, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        add: "f32[256, s1, 1][s1, 1, 1]cuda:0" = mean + 1e-06;  mean = None
	        rsqrt: "f32[256, s1, 1][s1, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        mul: "f32[256, s1, 128][128*s1, 128, 1]cuda:0" = l_x_ * rsqrt;  l_x_ = rsqrt = None
	        return (mul,)
	        
V0303 09:09:44.033814 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "e8225bbfffdcf79d0f535d8722824382"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992984033740.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.034012 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "4386754d980c4fea3a139b718e5bfb5a"}
	{
	"name": "backend_compile",
	"ts": 1740992984033740.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.049359 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "5d6fa9b29162a2148c01c2f858935d6f"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992984049300.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.102445 12578 torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "2a4be0ffd42b480c0caac788c3a61765"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "f32[s0, 128][128, 1]cuda:0", arg1_1: "i64[257][1]cuda:0", arg2_1: "f32[s3, 0][1, 1]cuda:0", arg3_1: "f32[s4, 0][1, 1]cuda:0", arg4_1: "Sym(s1)"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(arg0_1, 2)
	        mean: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(arg0_1, rsqrt);  arg0_1 = rsqrt = None
	        return (mul, arg1_1, arg2_1, arg3_1)
	        
V0303 09:09:44.102867 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "13750758e29a50e159b14a3252e00753"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992984102665.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.104122 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "f1183188958d0d0bf0b674cb565c5d22"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992984104059.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.104312 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "650599077aa2af6ebe86fcd7ed8db792"}
	{
	"name": "inductor_compile",
	"ts": 1740992984104059.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.110332 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "9b290daa295fa30f50febed338f2165c"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1):
	        pow_1 = torch.ops.aten.pow.Tensor_Scalar(arg0_1, 2)
	        mean = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt = torch.ops.aten.rsqrt.default(add);  add = None
	        mul = torch.ops.aten.mul.Tensor(arg0_1, rsqrt);  arg0_1 = rsqrt = None
	        return (mul, arg1_1, arg2_1, arg3_1)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 512*s0, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s0, 128), is_leaf=True)  # arg0_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # arg1_1
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # arg2_1
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # arg3_1
	    reader.symint(j1)  # arg4_1
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:09:44.118364 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "2a4be0ffd42b480c0caac788c3a61765"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "f32[s0, 128][128, 1]cuda:0", arg1_1: "i64[257][1]cuda:0", arg2_1: "f32[s3, 0][1, 1]cuda:0", arg3_1: "f32[s4, 0][1, 1]cuda:0", arg4_1: "Sym(s1)"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(arg0_1, 2)
	        mean: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s0, 128][128, 1]cuda:0" = torch.ops.aten.mul.Tensor(arg0_1, rsqrt);  arg0_1 = rsqrt = None
	        return (mul, arg1_1, arg2_1, arg3_1)
	        
V0303 09:09:44.118931 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "d0807ab1d9241861a3fa2fd9458a4a31"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992984118866.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.124205 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "f18836d32de2195afec7a664594354c8"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992984124153.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.124537 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "c12eef1ae96cd4e09e61c3038a114eae"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992984124476.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.124818 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "ed2f0b1cfd4d3dd0aaba5a84f3d17908"}
	{
	"name": "code_gen",
	"ts": 1740992984124476.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.125509 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "3ee2371b4ebd15eeb24b2e4254aa296e"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992984125445.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.133043 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "111e6ce851aa035e40b91a7b1663b31e"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992984132992.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.133295 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "cfe649d8f62ff70740069dfbde2563d7"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992984133230.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.260493 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "77c16f7dc666ff48701b8f5e67d70ec7"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992984260231.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.261091 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "5b2d20d11f0cf0dd30ef5321061dc05d"}
	{
	"name": "code_gen",
	"ts": 1740992984261024.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.261312 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "7823a991e58b2da59620cf7c661738a5"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992984261263.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.262062 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "f82d48687b091c5c420d377b808af6b9"}
	{
	"name": "inductor_compile",
	"ts": 1740992984262008.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.262274 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "652fd09d2741f2121251b4c040a880d4"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992984262229.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.263015 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "eb3abcb7180c78532a7eef77f08e459c"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992984262963.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.266003 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "a1aae47d624068fac278676182b965b4"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992984265950.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.266461 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "8048ae4960e184cc2d95ac69b26de091"}
	{
	"name": "backend_compile",
	"ts": 1740992984266415.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.266654 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "735cb732c66007fb2dc923c6834b68c1"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992984266612.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.267302 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "0df758dbc402db8f1b1b5e993be3d823"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992984267253.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.267495 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0, "has_payload": "875dc6b6f5483ec57fb991527c6f121f"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992984267454.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 5,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.267839 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "5/0", "frame_key": "7", "co_name": "_norm", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py", "co_firstlineno": 27, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992983.99082, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpljoplyj_/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpljoplyj_/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpljoplyj_', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.27686333656311035, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 5, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.292158 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py", 8]}
V0303 09:09:44.292395 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 174, "name": "forward", "filename": 8}]}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.292662 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "374f747356c9918219cdf1d0f5c35526"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992984292565.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.292848 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "ba941f901a2b8074b8ec9768c18df411"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992984292565.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:44.304108 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 16, "size": 6291456}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.304415 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 16, "size": 6291456}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.304680 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [384, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4069d10>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.304900 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 16, "size": 2056}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.305138 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.305305 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 16, "size": 0}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.305533 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.305695 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 16, "size": 0}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.305923 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.306516 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j1", 384, 1], "storage": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9f4069630>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.306686 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 4, "source": "L['context']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.337190 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.337469 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12090>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.337635 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 6, "source": "L['self']._modules['encoder']._modules['layers']._modules['0']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.350348 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.350617 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12c70>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.350781 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 7, "source": "L['self']._modules['encoder']._modules['layers']._modules['0']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.409362 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.409642 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 33, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12d60>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.409812 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 33, "source": "L['self']._modules['encoder']._modules['layers']._modules['0']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.430279 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.430552 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 37, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e50>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.430725 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 37, "source": "L['self']._modules['encoder']._modules['layers']._modules['0']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.438121 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.438396 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 38, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e00>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.438567 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 38, "source": "L['self']._modules['encoder']._modules['layers']._modules['0']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.456664 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.456941 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 41, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b123b0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.457108 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 41, "source": "L['self']._modules['encoder']._modules['layers']._modules['0']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.483560 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 11, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.483832 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 45, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 11, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12270>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.483996 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 45, "source": "L['self']._modules['encoder']._modules['layers']._modules['1']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.494590 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 12, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.494887 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 46, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 12, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12ea0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.495053 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 46, "source": "L['self']._modules['encoder']._modules['layers']._modules['1']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.538020 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 13, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.538289 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 71, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 13, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12f90>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.538451 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 71, "source": "L['self']._modules['encoder']._modules['layers']._modules['1']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.558635 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 14, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.558911 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 74, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 14, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b124a0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.559073 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 74, "source": "L['self']._modules['encoder']._modules['layers']._modules['1']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.566042 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 15, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.566312 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 75, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 15, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12b30>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.566565 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 75, "source": "L['self']._modules['encoder']._modules['layers']._modules['1']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.581299 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 16, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.581566 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 77, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 16, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b122c0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.581729 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 77, "source": "L['self']._modules['encoder']._modules['layers']._modules['1']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.607147 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 17, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.607420 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 80, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 17, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17950>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.607579 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 80, "source": "L['self']._modules['encoder']._modules['layers']._modules['2']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.617939 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 18, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.618258 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 81, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 18, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17bd0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.618422 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 81, "source": "L['self']._modules['encoder']._modules['layers']._modules['2']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.660065 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 19, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.660339 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 106, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 19, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17680>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.660498 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 106, "source": "L['self']._modules['encoder']._modules['layers']._modules['2']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.679837 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 20, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.680108 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 109, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 20, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17d60>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.680272 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 109, "source": "L['self']._modules['encoder']._modules['layers']._modules['2']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.687295 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 21, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.687569 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 110, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 21, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17c70>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.687827 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 110, "source": "L['self']._modules['encoder']._modules['layers']._modules['2']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.702524 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 22, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.702790 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 112, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 22, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17b30>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.702953 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 112, "source": "L['self']._modules['encoder']._modules['layers']._modules['2']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.728293 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 23, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.728559 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 115, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 23, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b179a0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.728719 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 115, "source": "L['self']._modules['encoder']._modules['layers']._modules['3']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.739062 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 24, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.739381 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 116, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 24, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17810>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.739549 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 116, "source": "L['self']._modules['encoder']._modules['layers']._modules['3']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.783788 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 25, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.784077 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 141, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 25, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17130>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.784240 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 141, "source": "L['self']._modules['encoder']._modules['layers']._modules['3']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.804317 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 26, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.804585 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 144, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 26, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17540>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.804782 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 144, "source": "L['self']._modules['encoder']._modules['layers']._modules['3']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.811633 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 27, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.811896 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 145, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 27, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17860>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.812142 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 145, "source": "L['self']._modules['encoder']._modules['layers']._modules['3']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.826941 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 28, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.827210 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 147, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 28, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17900>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.827379 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 147, "source": "L['self']._modules['encoder']._modules['layers']._modules['3']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.844515 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 29, "describer_id": 16, "size": 1966080}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.844849 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 30, "describer_id": 16, "size": 1966080}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.845099 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 149, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1280, 384], "is_leaf": true, "stride": [384, 1], "storage": 30, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4069310>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.845272 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 31, "describer_id": 16, "size": 2056}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.845494 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 150, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "2", "stride": [1], "storage": 31, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132450>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.845666 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 32, "describer_id": 16, "size": 0}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.845889 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 151, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [5, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 32, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f411cef0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.846128 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 33, "describer_id": 16, "size": 0}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.846367 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 152, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [5, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 33, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f411c0e0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.846941 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 153, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j2", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j2", 384, 1], "storage": 29, "attrs": {"_values": 149, "_offsets": 150, "_min_seqlen_tensor": 151, "_max_seqlen_tensor": 152}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9f406da90>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.847102 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 153, "source": "L['x']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.872702 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 34, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.872969 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 155, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 34, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17040>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.873130 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 155, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.883838 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 35, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.884098 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 156, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 35, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b179f0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.884262 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 156, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.934124 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 36, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.934396 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 181, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 36, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b177c0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.934559 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 181, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.952867 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 37, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.953134 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 184, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 37, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17360>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.953297 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 184, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['cross_attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.963659 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 38, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.963932 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 185, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 38, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17270>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.964095 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 185, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['cross_attention']._modules['q']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.968001 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 39, "describer_id": 16, "size": 1179648}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.968267 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 187, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [768, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 39, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b171d0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:44.968433 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 187, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['cross_attention']._modules['kv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.010597 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 40, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.010877 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 212, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 40, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b174f0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.011042 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 212, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['cross_attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.031240 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 41, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.031514 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 215, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 41, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17590>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.031682 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 215, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.038587 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 42, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.038856 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 216, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 42, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17450>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.039102 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 216, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.055844 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 43, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.056123 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 218, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 43, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b176d0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.056286 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 218, "source": "L['self']._modules['decoder']._modules['layers']._modules['0']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.081602 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 44, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.081878 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 221, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 44, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17090>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.082042 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 221, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.092400 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 45, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.092663 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 222, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 45, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17720>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.092823 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 222, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.134090 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 46, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.134362 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 247, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 46, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b170e0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.134528 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 247, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.152573 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 47, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.152841 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 250, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 47, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17ea0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.153006 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 250, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['cross_attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.163322 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 48, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.163585 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 251, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 48, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17cc0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.163756 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 251, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['cross_attention']._modules['q']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.167630 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 49, "describer_id": 16, "size": 1179648}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.167895 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 253, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [768, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 49, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17d10>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.168055 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 253, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['cross_attention']._modules['kv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.208740 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 50, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.209016 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 277, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 50, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17db0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.209183 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 277, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['cross_attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.228928 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 51, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.229203 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 280, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 51, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17630>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.229374 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 280, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.236356 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 52, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.236628 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 281, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 52, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17400>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.237283 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 281, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.252382 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 53, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.252649 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 283, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 53, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b172c0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.252824 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 283, "source": "L['self']._modules['decoder']._modules['layers']._modules['1']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.538616 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 54, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.539010 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 286, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 54, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad50e0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.539180 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 286, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.550240 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 55, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.550584 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 287, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 55, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17f40>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.550754 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 287, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.593955 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 56, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.594237 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 312, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 56, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17f90>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.594404 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 312, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.613162 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 57, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.613430 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 315, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 57, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5310>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.613603 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 315, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['cross_attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.624108 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 58, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.624377 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 316, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 58, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5180>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.624539 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 316, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['cross_attention']._modules['q']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.628473 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 59, "describer_id": 16, "size": 1179648}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.628741 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 318, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [768, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 59, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad51d0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.628906 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 318, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['cross_attention']._modules['kv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.669924 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 60, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.670198 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 342, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 60, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5220>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.670361 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 342, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['cross_attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.690140 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 61, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.690412 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 345, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 61, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17ef0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.690587 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 345, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.697740 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 62, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.698109 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 346, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 62, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5040>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.698277 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 346, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.713244 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 63, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.713518 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 348, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 63, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5090>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.713688 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 348, "source": "L['self']._modules['decoder']._modules['layers']._modules['2']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.739523 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 64, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.739795 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 351, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 64, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad54f0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.739958 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 351, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.750862 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 65, "describer_id": 16, "size": 1769472}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.751131 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 352, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 65, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad53b0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.751296 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 352, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.794090 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 66, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.794367 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 377, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 66, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5400>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.794531 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 377, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.813016 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 67, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.813291 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 380, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 67, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5720>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.813461 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 380, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['cross_attn_norm']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.823952 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 68, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.824223 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 381, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 68, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5590>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.824390 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 381, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['cross_attention']._modules['q']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.828330 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 69, "describer_id": 16, "size": 1179648}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.828600 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 383, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [768, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 69, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad55e0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.828764 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 383, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['cross_attention']._modules['kv']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.870186 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 70, "describer_id": 16, "size": 589824}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.870461 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 407, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 70, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5630>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.870631 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 407, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['cross_attention']._modules['proj']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.890406 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 71, "describer_id": 16, "size": 1536}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.890679 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 410, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 71, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5360>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.890844 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 410, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.897775 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 72, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.898056 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 411, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 72, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5450>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.898314 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 411, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.913238 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 73, "describer_id": 16, "size": 1572864}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.913511 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 413, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 73, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad54a0>", "describer_id": 16}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:45.913687 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 16, "id": 413, "source": "L['self']._modules['decoder']._modules['layers']._modules['3']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:46.021202 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_modules_encoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_encoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_encoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_encoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_decoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_0_modules_cross_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_q_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_kv_parameters_weight_": [768, 384], "l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_decoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_1_modules_cross_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_q_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_kv_parameters_weight_": [768, 384], "l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_decoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_2_modules_cross_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_q_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_kv_parameters_weight_": [768, 384], "l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_decoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_3_modules_cross_attn_norm_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_q_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_kv_parameters_weight_": [768, 384], "l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024]}}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "11fa1442014c04118ebcd0b39729556c"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_context_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_encoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_x_: "f32[256, s5, 384][384*s5, 384, 1]cuda:0", s5: "Sym(s5)", L_self_modules_decoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_cross_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_q_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_kv_parameters_weight_: "f32[768, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_cross_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_q_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_kv_parameters_weight_: "f32[768, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_cross_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_q_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_kv_parameters_weight_: "f32[768, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_cross_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_q_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_kv_parameters_weight_: "f32[768, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0"):
	        l_context_ = L_context_
	        l_self_modules_encoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_x_ = L_x_
	        l_self_modules_decoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_cross_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_cross_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_q_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_kv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_kv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_cross_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_cross_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_q_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_kv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_kv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_cross_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_cross_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_q_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_kv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_kv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_cross_attn_norm_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_cross_attn_norm_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_q_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_kv_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_kv_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_proj_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_proj_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_context_.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1.pow(2)
	        mean: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        add: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean + 1e-06;  mean = None
	        rsqrt: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        mul: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1 * rsqrt;  float_1 = rsqrt = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul.type_as(l_context_);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output * l_self_modules_encoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_;  output = l_self_modules_encoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_1, 0.3, True, False);  mul_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_, None);  dropout = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk = linear.chunk(3, dim = -1);  linear = None
	        queries: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[0]
	        keys: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[1]
	        values: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[2];  chunk = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries.unflatten(-1, [6, 64]);  queries = None
	        queries_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_1: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys.unflatten(-1, [6, 64]);  keys = None
	        keys_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_2: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values.unflatten(-1, [6, 64]);  values = None
	        values_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_1, keys_1, values_1, dropout_p = False, is_causal = False);  queries_1 = keys_1 = values_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_3: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
	        context_vec_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_1, l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_, None);  context_vec_1 = l_self_modules_encoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_context_ + context_vec_2;  l_context_ = context_vec_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2.pow(2)
	        mean_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_2.mean(-1, keepdim = True);  pow_2 = None
	        add_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_1 + 1e-06;  mean_1 = None
	        rsqrt_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_2);  add_2 = None
	        mul_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2 * rsqrt_1;  float_2 = rsqrt_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_2.type_as(attn_out);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_1 * l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_;  output_1 = l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_2: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_1, l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_1 = l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_3: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_2, inplace = False);  input_2 = None
	        input_4: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_3, 0.3, True, False);  input_3 = None
	        input_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_4, l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_4 = l_self_modules_encoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_5, 0.3, True, False);  input_5 = None
	        proj_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out + input_6;  attn_out = input_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_3.pow(2)
	        mean_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_3.mean(-1, keepdim = True);  pow_3 = None
	        add_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_2 + 1e-06;  mean_2 = None
	        rsqrt_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_4);  add_4 = None
	        mul_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_3 * rsqrt_2;  float_3 = rsqrt_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_4.type_as(proj_out);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_2 * l_self_modules_encoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_;  output_2 = l_self_modules_encoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_5, 0.3, True, False);  mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_4: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_3, l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_, None);  dropout_3 = l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_1 = linear_4.chunk(3, dim = -1);  linear_4 = None
	        queries_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[0]
	        keys_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[1]
	        values_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[2];  chunk_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_3: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_2.unflatten(-1, [6, 64]);  queries_2 = None
	        queries_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_3.transpose(1, 2);  unflatten_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_4: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_2.unflatten(-1, [6, 64]);  keys_2 = None
	        keys_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_4.transpose(1, 2);  unflatten_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_5: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_2.unflatten(-1, [6, 64]);  values_2 = None
	        values_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_5.transpose(1, 2);  unflatten_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_3: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_3, keys_3, values_3, dropout_p = False, is_causal = False);  queries_3 = keys_3 = values_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_7: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_3.transpose(1, 2);  context_vec_3 = None
	        context_vec_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_7.flatten(-2);  transpose_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_4, l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_, None);  context_vec_4 = l_self_modules_encoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out + context_vec_5;  proj_out = context_vec_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_1.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_4.pow(2)
	        mean_3: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_4.mean(-1, keepdim = True);  pow_4 = None
	        add_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_3 + 1e-06;  mean_3 = None
	        rsqrt_3: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_6);  add_6 = None
	        mul_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_4 * rsqrt_3;  float_4 = rsqrt_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_6.type_as(attn_out_1);  mul_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_3 * l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_;  output_3 = l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_8: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_7, l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_7 = l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_9: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_8, inplace = False);  input_8 = None
	        input_10: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_9, 0.3, True, False);  input_9 = None
	        input_11: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_10, l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_10 = l_self_modules_encoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_12: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_11, 0.3, True, False);  input_11 = None
	        proj_out_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_1 + input_12;  attn_out_1 = input_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_1.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_5.pow(2)
	        mean_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_5.mean(-1, keepdim = True);  pow_5 = None
	        add_8: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_4 + 1e-06;  mean_4 = None
	        rsqrt_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_8);  add_8 = None
	        mul_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_5 * rsqrt_4;  float_5 = rsqrt_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_8.type_as(proj_out_1);  mul_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_9: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_4 * l_self_modules_encoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_;  output_4 = l_self_modules_encoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_9, 0.3, True, False);  mul_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_8: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_6, l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_, None);  dropout_6 = l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_2 = linear_8.chunk(3, dim = -1);  linear_8 = None
	        queries_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[0]
	        keys_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[1]
	        values_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[2];  chunk_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_6: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_4.unflatten(-1, [6, 64]);  queries_4 = None
	        queries_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_6.transpose(1, 2);  unflatten_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_7: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_4.unflatten(-1, [6, 64]);  keys_4 = None
	        keys_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_7.transpose(1, 2);  unflatten_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_8: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_4.unflatten(-1, [6, 64]);  values_4 = None
	        values_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_8.transpose(1, 2);  unflatten_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_6: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_5, keys_5, values_5, dropout_p = False, is_causal = False);  queries_5 = keys_5 = values_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_11: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_6.transpose(1, 2);  context_vec_6 = None
	        context_vec_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_11.flatten(-2);  transpose_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_7, l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_, None);  context_vec_7 = l_self_modules_encoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_1 + context_vec_8;  proj_out_1 = context_vec_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_2.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_6.pow(2)
	        mean_5: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_6.mean(-1, keepdim = True);  pow_6 = None
	        add_10: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_5 + 1e-06;  mean_5 = None
	        rsqrt_5: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_10);  add_10 = None
	        mul_10: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_6 * rsqrt_5;  float_6 = rsqrt_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_10.type_as(attn_out_2);  mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_13: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_5 * l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_;  output_5 = l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_14: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_13, l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_13 = l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_15: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_14, inplace = False);  input_14 = None
	        input_16: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_15, 0.3, True, False);  input_15 = None
	        input_17: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_16, l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_16 = l_self_modules_encoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_18: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_17, 0.3, True, False);  input_17 = None
	        proj_out_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_2 + input_18;  attn_out_2 = input_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_2.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_7.pow(2)
	        mean_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_7.mean(-1, keepdim = True);  pow_7 = None
	        add_12: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_6 + 1e-06;  mean_6 = None
	        rsqrt_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_12);  add_12 = None
	        mul_12: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_7 * rsqrt_6;  float_7 = rsqrt_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_12.type_as(proj_out_2);  mul_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_13: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_6 * l_self_modules_encoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_;  output_6 = l_self_modules_encoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_9: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_13, 0.3, True, False);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_12: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_9, l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_, None);  dropout_9 = l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_3 = linear_12.chunk(3, dim = -1);  linear_12 = None
	        queries_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[0]
	        keys_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[1]
	        values_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[2];  chunk_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_9: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_6.unflatten(-1, [6, 64]);  queries_6 = None
	        queries_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_9.transpose(1, 2);  unflatten_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_10: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_6.unflatten(-1, [6, 64]);  keys_6 = None
	        keys_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_10.transpose(1, 2);  unflatten_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_11: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_6.unflatten(-1, [6, 64]);  values_6 = None
	        values_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_11.transpose(1, 2);  unflatten_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_9: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_7, keys_7, values_7, dropout_p = False, is_causal = False);  queries_7 = keys_7 = values_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_15: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_9.transpose(1, 2);  context_vec_9 = None
	        context_vec_10: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_15.flatten(-2);  transpose_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_11: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_10, l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_, None);  context_vec_10 = l_self_modules_encoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_2 + context_vec_11;  proj_out_2 = context_vec_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_3.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_8.pow(2)
	        mean_7: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_8.mean(-1, keepdim = True);  pow_8 = None
	        add_14: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_7 + 1e-06;  mean_7 = None
	        rsqrt_7: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_14);  add_14 = None
	        mul_14: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_8 * rsqrt_7;  float_8 = rsqrt_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_14.type_as(attn_out_3);  mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_19: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_7 * l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_;  output_7 = l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_20: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_19, l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_19 = l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_21: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_20, inplace = False);  input_20 = None
	        input_22: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_21, 0.3, True, False);  input_21 = None
	        input_23: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_22, l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_22 = l_self_modules_encoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_24: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_23, 0.3, True, False);  input_23 = None
	        proj_out_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_3 + input_24;  attn_out_3 = input_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_9: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = l_x_.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_9: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_9.pow(2)
	        mean_8: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_9.mean(-1, keepdim = True);  pow_9 = None
	        add_16: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_8 + 1e-06;  mean_8 = None
	        rsqrt_8: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_16);  add_16 = None
	        mul_16: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_9 * rsqrt_8;  float_9 = rsqrt_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_8: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_16.type_as(l_x_);  mul_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_17: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_8 * l_self_modules_decoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_;  output_8 = l_self_modules_decoder_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_12: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_17, 0.3, True, False);  mul_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_16: "f32[256, s5, 1152][1152*s5, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_12, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_, None);  dropout_12 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_4 = linear_16.chunk(3, dim = -1);  linear_16 = None
	        queries_8: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_4[0]
	        keys_8: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_4[1]
	        values_8: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_4[2];  chunk_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_12: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = queries_8.unflatten(-1, [6, 64]);  queries_8 = None
	        queries_9: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_12.transpose(1, 2);  unflatten_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_13: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = keys_8.unflatten(-1, [6, 64]);  keys_8 = None
	        keys_9: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_13.transpose(1, 2);  unflatten_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_14: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = values_8.unflatten(-1, [6, 64]);  values_8 = None
	        values_9: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_14.transpose(1, 2);  unflatten_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_12: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_9, keys_9, values_9, dropout_p = False, is_causal = True);  queries_9 = keys_9 = values_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_19: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_12.transpose(1, 2);  context_vec_12 = None
	        context_vec_13: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_19.flatten(-2);  transpose_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_14: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_13, l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_, None);  context_vec_13 = l_self_modules_decoder_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_4: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = l_x_ + context_vec_14;  context_vec_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_10: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = l_x_.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_10: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_10.pow(2)
	        mean_9: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_10.mean(-1, keepdim = True);  pow_10 = None
	        add_18: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_9 + 1e-06;  mean_9 = None
	        rsqrt_9: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_18);  add_18 = None
	        mul_18: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_10 * rsqrt_9;  float_10 = rsqrt_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_9: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_18.type_as(l_x_);  mul_18 = l_x_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_19: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_9 * l_self_modules_decoder_modules_layers_modules_0_modules_cross_attn_norm_parameters_weight_;  output_9 = l_self_modules_decoder_modules_layers_modules_0_modules_cross_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        dropout_13: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_19, 0.3, True, False);  mul_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        queries_10: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(dropout_13, l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_q_parameters_weight_, None);  dropout_13 = l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_q_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        linear_19: "f32[256, s0, 768][768*s0, 768, 1]cuda:0" = torch._C._nn.linear(proj_out_3, l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_kv_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_kv_parameters_weight_ = None
	        chunk_5 = linear_19.chunk(2, dim = -1);  linear_19 = None
	        keys_10: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_5[0]
	        values_10: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_5[1];  chunk_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_15: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = queries_10.unflatten(-1, [6, 64]);  queries_10 = None
	        queries_11: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = unflatten_15.transpose(1, 2);  unflatten_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_16: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = keys_10.unflatten(-1, [6, 64]);  keys_10 = None
	        keys_11: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_16.transpose(1, 2);  unflatten_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_17: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = values_10.unflatten(-1, [6, 64]);  values_10 = None
	        values_11: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_17.transpose(1, 2);  unflatten_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_15: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_11, keys_11, values_11, dropout_p = False, is_causal = False);  queries_11 = keys_11 = values_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_23: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_15.transpose(1, 2);  context_vec_15 = None
	        context_vec_16: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_23.flatten(-2);  transpose_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_17: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_16, l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_proj_parameters_weight_, None);  context_vec_16 = l_self_modules_decoder_modules_layers_modules_0_modules_cross_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        attn_out_5: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_4 + context_vec_17;  attn_out_4 = context_vec_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_11: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_5.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_11: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_11.pow(2)
	        mean_10: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_11.mean(-1, keepdim = True);  pow_11 = None
	        add_20: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_10 + 1e-06;  mean_10 = None
	        rsqrt_10: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_20);  add_20 = None
	        mul_20: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_11 * rsqrt_10;  float_11 = rsqrt_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_10: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_20.type_as(attn_out_5);  mul_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_25: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_10 * l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_;  output_10 = l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_26: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch._C._nn.linear(input_25, l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_25 = l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_27: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.silu(input_26, inplace = False);  input_26 = None
	        input_28: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_27, 0.3, True, False);  input_27 = None
	        input_29: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(input_28, l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_28 = l_self_modules_decoder_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_30: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(input_29, 0.3, True, False);  input_29 = None
	        proj_out_4: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_5 + input_30;  attn_out_5 = input_30 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_12: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_4.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_12: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_12.pow(2)
	        mean_11: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_12.mean(-1, keepdim = True);  pow_12 = None
	        add_22: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_11 + 1e-06;  mean_11 = None
	        rsqrt_11: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_22);  add_22 = None
	        mul_22: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_12 * rsqrt_11;  float_12 = rsqrt_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_11: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_22.type_as(proj_out_4);  mul_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_11 * l_self_modules_decoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_;  output_11 = l_self_modules_decoder_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_16: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_23, 0.3, True, False);  mul_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_23: "f32[256, s5, 1152][1152*s5, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_16, l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_, None);  dropout_16 = l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_6 = linear_23.chunk(3, dim = -1);  linear_23 = None
	        queries_12: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_6[0]
	        keys_12: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_6[1]
	        values_12: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_6[2];  chunk_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_18: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = queries_12.unflatten(-1, [6, 64]);  queries_12 = None
	        queries_13: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_18.transpose(1, 2);  unflatten_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_19: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = keys_12.unflatten(-1, [6, 64]);  keys_12 = None
	        keys_13: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_19.transpose(1, 2);  unflatten_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_20: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = values_12.unflatten(-1, [6, 64]);  values_12 = None
	        values_13: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_20.transpose(1, 2);  unflatten_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_18: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_13, keys_13, values_13, dropout_p = False, is_causal = True);  queries_13 = keys_13 = values_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_27: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_18.transpose(1, 2);  context_vec_18 = None
	        context_vec_19: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_27.flatten(-2);  transpose_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_20: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_19, l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_, None);  context_vec_19 = l_self_modules_decoder_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_6: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_4 + context_vec_20;  context_vec_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_13: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_4.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_13: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_13.pow(2)
	        mean_12: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_13.mean(-1, keepdim = True);  pow_13 = None
	        add_24: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_12 + 1e-06;  mean_12 = None
	        rsqrt_12: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_24);  add_24 = None
	        mul_24: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_13 * rsqrt_12;  float_13 = rsqrt_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_12: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_24.type_as(proj_out_4);  mul_24 = proj_out_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_25: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_12 * l_self_modules_decoder_modules_layers_modules_1_modules_cross_attn_norm_parameters_weight_;  output_12 = l_self_modules_decoder_modules_layers_modules_1_modules_cross_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        dropout_17: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_25, 0.3, True, False);  mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        queries_14: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(dropout_17, l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_q_parameters_weight_, None);  dropout_17 = l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_q_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        linear_26: "f32[256, s0, 768][768*s0, 768, 1]cuda:0" = torch._C._nn.linear(proj_out_3, l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_kv_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_kv_parameters_weight_ = None
	        chunk_7 = linear_26.chunk(2, dim = -1);  linear_26 = None
	        keys_14: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_7[0]
	        values_14: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_7[1];  chunk_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_21: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = queries_14.unflatten(-1, [6, 64]);  queries_14 = None
	        queries_15: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = unflatten_21.transpose(1, 2);  unflatten_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_22: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = keys_14.unflatten(-1, [6, 64]);  keys_14 = None
	        keys_15: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_22.transpose(1, 2);  unflatten_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_23: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = values_14.unflatten(-1, [6, 64]);  values_14 = None
	        values_15: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_23.transpose(1, 2);  unflatten_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_21: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_15, keys_15, values_15, dropout_p = False, is_causal = False);  queries_15 = keys_15 = values_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_31: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_21.transpose(1, 2);  context_vec_21 = None
	        context_vec_22: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_31.flatten(-2);  transpose_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_23: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_22, l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_proj_parameters_weight_, None);  context_vec_22 = l_self_modules_decoder_modules_layers_modules_1_modules_cross_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        attn_out_7: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_6 + context_vec_23;  attn_out_6 = context_vec_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_14: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_7.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_14: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_14.pow(2)
	        mean_13: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_14.mean(-1, keepdim = True);  pow_14 = None
	        add_26: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_13 + 1e-06;  mean_13 = None
	        rsqrt_13: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_26);  add_26 = None
	        mul_26: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_14 * rsqrt_13;  float_14 = rsqrt_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_13: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_26.type_as(attn_out_7);  mul_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_31: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_13 * l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_;  output_13 = l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_32: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch._C._nn.linear(input_31, l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_31 = l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_33: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.silu(input_32, inplace = False);  input_32 = None
	        input_34: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_33, 0.3, True, False);  input_33 = None
	        input_35: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(input_34, l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_34 = l_self_modules_decoder_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_36: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(input_35, 0.3, True, False);  input_35 = None
	        proj_out_5: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_7 + input_36;  attn_out_7 = input_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_15: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_5.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_15: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_15.pow(2)
	        mean_14: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_15.mean(-1, keepdim = True);  pow_15 = None
	        add_28: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_14 + 1e-06;  mean_14 = None
	        rsqrt_14: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_28);  add_28 = None
	        mul_28: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_15 * rsqrt_14;  float_15 = rsqrt_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_14: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_28.type_as(proj_out_5);  mul_28 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_29: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_14 * l_self_modules_decoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_;  output_14 = l_self_modules_decoder_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_20: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_29, 0.3, True, False);  mul_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_30: "f32[256, s5, 1152][1152*s5, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_20, l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_, None);  dropout_20 = l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_8 = linear_30.chunk(3, dim = -1);  linear_30 = None
	        queries_16: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_8[0]
	        keys_16: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_8[1]
	        values_16: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_8[2];  chunk_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_24: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = queries_16.unflatten(-1, [6, 64]);  queries_16 = None
	        queries_17: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_24.transpose(1, 2);  unflatten_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_25: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = keys_16.unflatten(-1, [6, 64]);  keys_16 = None
	        keys_17: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_25.transpose(1, 2);  unflatten_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_26: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = values_16.unflatten(-1, [6, 64]);  values_16 = None
	        values_17: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_26.transpose(1, 2);  unflatten_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_24: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_17, keys_17, values_17, dropout_p = False, is_causal = True);  queries_17 = keys_17 = values_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_35: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_24.transpose(1, 2);  context_vec_24 = None
	        context_vec_25: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_35.flatten(-2);  transpose_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_26: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_25, l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_, None);  context_vec_25 = l_self_modules_decoder_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_8: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_5 + context_vec_26;  context_vec_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_16: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_5.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_16: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_16.pow(2)
	        mean_15: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_16.mean(-1, keepdim = True);  pow_16 = None
	        add_30: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_15 + 1e-06;  mean_15 = None
	        rsqrt_15: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_30);  add_30 = None
	        mul_30: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_16 * rsqrt_15;  float_16 = rsqrt_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_15: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_30.type_as(proj_out_5);  mul_30 = proj_out_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_31: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_15 * l_self_modules_decoder_modules_layers_modules_2_modules_cross_attn_norm_parameters_weight_;  output_15 = l_self_modules_decoder_modules_layers_modules_2_modules_cross_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        dropout_21: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_31, 0.3, True, False);  mul_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        queries_18: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(dropout_21, l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_q_parameters_weight_, None);  dropout_21 = l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_q_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        linear_33: "f32[256, s0, 768][768*s0, 768, 1]cuda:0" = torch._C._nn.linear(proj_out_3, l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_kv_parameters_weight_, None);  l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_kv_parameters_weight_ = None
	        chunk_9 = linear_33.chunk(2, dim = -1);  linear_33 = None
	        keys_18: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_9[0]
	        values_18: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_9[1];  chunk_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_27: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = queries_18.unflatten(-1, [6, 64]);  queries_18 = None
	        queries_19: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = unflatten_27.transpose(1, 2);  unflatten_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_28: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = keys_18.unflatten(-1, [6, 64]);  keys_18 = None
	        keys_19: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_28.transpose(1, 2);  unflatten_28 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_29: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = values_18.unflatten(-1, [6, 64]);  values_18 = None
	        values_19: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_29.transpose(1, 2);  unflatten_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_27: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_19, keys_19, values_19, dropout_p = False, is_causal = False);  queries_19 = keys_19 = values_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_39: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_27.transpose(1, 2);  context_vec_27 = None
	        context_vec_28: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_39.flatten(-2);  transpose_39 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_29: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_28, l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_proj_parameters_weight_, None);  context_vec_28 = l_self_modules_decoder_modules_layers_modules_2_modules_cross_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        attn_out_9: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_8 + context_vec_29;  attn_out_8 = context_vec_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_17: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_9.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_17: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_17.pow(2)
	        mean_16: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_17.mean(-1, keepdim = True);  pow_17 = None
	        add_32: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_16 + 1e-06;  mean_16 = None
	        rsqrt_16: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_32);  add_32 = None
	        mul_32: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_17 * rsqrt_16;  float_17 = rsqrt_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_16: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_32.type_as(attn_out_9);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_37: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_16 * l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_;  output_16 = l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_38: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch._C._nn.linear(input_37, l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_37 = l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_39: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.silu(input_38, inplace = False);  input_38 = None
	        input_40: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_39, 0.3, True, False);  input_39 = None
	        input_41: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(input_40, l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_40 = l_self_modules_decoder_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_42: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(input_41, 0.3, True, False);  input_41 = None
	        proj_out_6: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_9 + input_42;  attn_out_9 = input_42 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_18: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_6.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_18: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_18.pow(2)
	        mean_17: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_18.mean(-1, keepdim = True);  pow_18 = None
	        add_34: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_17 + 1e-06;  mean_17 = None
	        rsqrt_17: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_34);  add_34 = None
	        mul_34: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_18 * rsqrt_17;  float_18 = rsqrt_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_17: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_34.type_as(proj_out_6);  mul_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_35: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_17 * l_self_modules_decoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_;  output_17 = l_self_modules_decoder_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_24: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_35, 0.3, True, False);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_37: "f32[256, s5, 1152][1152*s5, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_24, l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_, None);  dropout_24 = l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_10 = linear_37.chunk(3, dim = -1);  linear_37 = None
	        queries_20: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_10[0]
	        keys_20: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_10[1]
	        values_20: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0" = chunk_10[2];  chunk_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_30: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = queries_20.unflatten(-1, [6, 64]);  queries_20 = None
	        queries_21: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_30.transpose(1, 2);  unflatten_30 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_31: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = keys_20.unflatten(-1, [6, 64]);  keys_20 = None
	        keys_21: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_31.transpose(1, 2);  unflatten_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_32: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = values_20.unflatten(-1, [6, 64]);  values_20 = None
	        values_21: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten_32.transpose(1, 2);  unflatten_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_30: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_21, keys_21, values_21, dropout_p = False, is_causal = True);  queries_21 = keys_21 = values_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_43: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_30.transpose(1, 2);  context_vec_30 = None
	        context_vec_31: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_43.flatten(-2);  transpose_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_32: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_31, l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_, None);  context_vec_31 = l_self_modules_decoder_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_10: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_6 + context_vec_32;  context_vec_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_19: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = proj_out_6.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_19: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_19.pow(2)
	        mean_18: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_19.mean(-1, keepdim = True);  pow_19 = None
	        add_36: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_18 + 1e-06;  mean_18 = None
	        rsqrt_18: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_36);  add_36 = None
	        mul_36: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_19 * rsqrt_18;  float_19 = rsqrt_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_18: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_36.type_as(proj_out_6);  mul_36 = proj_out_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_37: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_18 * l_self_modules_decoder_modules_layers_modules_3_modules_cross_attn_norm_parameters_weight_;  output_18 = l_self_modules_decoder_modules_layers_modules_3_modules_cross_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        dropout_25: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_37, 0.3, True, False);  mul_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        queries_22: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(dropout_25, l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_q_parameters_weight_, None);  dropout_25 = l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_q_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        linear_40: "f32[256, s0, 768][768*s0, 768, 1]cuda:0" = torch._C._nn.linear(proj_out_3, l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_kv_parameters_weight_, None);  proj_out_3 = l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_kv_parameters_weight_ = None
	        chunk_11 = linear_40.chunk(2, dim = -1);  linear_40 = None
	        keys_22: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_11[0]
	        values_22: "f32[256, s0, 384][768*s0, 768, 1]cuda:0" = chunk_11[1];  chunk_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_33: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = queries_22.unflatten(-1, [6, 64]);  queries_22 = None
	        queries_23: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = unflatten_33.transpose(1, 2);  unflatten_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_34: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = keys_22.unflatten(-1, [6, 64]);  keys_22 = None
	        keys_23: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_34.transpose(1, 2);  unflatten_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_35: "f32[256, s0, 6, 64][768*s0, 768, 64, 1]cuda:0" = values_22.unflatten(-1, [6, 64]);  values_22 = None
	        values_23: "f32[256, 6, s0, 64][768*s0, 64, 768, 1]cuda:0" = unflatten_35.transpose(1, 2);  unflatten_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_33: "f32[256, 6, s5, 64][384*s5, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_23, keys_23, values_23, dropout_p = False, is_causal = False);  queries_23 = keys_23 = values_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_47: "f32[256, s5, 6, 64][384*s5, 384, 64, 1]cuda:0" = context_vec_33.transpose(1, 2);  context_vec_33 = None
	        context_vec_34: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = transpose_47.flatten(-2);  transpose_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_35: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_34, l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_proj_parameters_weight_, None);  context_vec_34 = l_self_modules_decoder_modules_layers_modules_3_modules_cross_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        attn_out_11: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_10 + context_vec_35;  attn_out_10 = context_vec_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_20: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_11.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_20: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_20.pow(2)
	        mean_19: "f32[256, s5, 1][s5, 1, 1]cuda:0" = pow_20.mean(-1, keepdim = True);  pow_20 = None
	        add_38: "f32[256, s5, 1][s5, 1, 1]cuda:0" = mean_19 + 1e-06;  mean_19 = None
	        rsqrt_19: "f32[256, s5, 1][s5, 1, 1]cuda:0" = torch.rsqrt(add_38);  add_38 = None
	        mul_38: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = float_20 * rsqrt_19;  float_20 = rsqrt_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_19: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = mul_38.type_as(attn_out_11);  mul_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_43: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = output_19 * l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_;  output_19 = l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_44: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch._C._nn.linear(input_43, l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_43 = l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_45: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.silu(input_44, inplace = False);  input_44 = None
	        input_46: "f32[256, s5, 1024][1024*s5, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_45, 0.3, True, False);  input_45 = None
	        input_47: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch._C._nn.linear(input_46, l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_46 = l_self_modules_decoder_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_48: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = torch.nn.functional.dropout(input_47, 0.3, True, False);  input_47 = None
	        proj_out_7: "f32[256, s5, 384][384*s5, 384, 1]cuda:0" = attn_out_11 + input_48;  attn_out_11 = input_48 = None
	        return (proj_out_7,)
	        
V0303 09:09:46.022000 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "9604d9717ea99cdeb3a2ba834a17e705"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992986021921.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:46.022185 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "29baac1e168d92bbddda008a4717eed0"}
	{
	"name": "backend_compile",
	"ts": 1740992986021921.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:46.185123 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "b42a143a8d797e72c6f7ae9132197a49"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992986185058.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:52.767443 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "fdc90c3b07d88505f80a3d63821ba191"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[384][1]cuda:0"; primals_7: "f32[1152, 384][384, 1]cuda:0"; primals_8: "f32[384, 384][384, 1]cuda:0"; primals_9: "f32[384][1]cuda:0"; primals_10: "f32[1024, 384][384, 1]cuda:0"; primals_11: "f32[384, 1024][1024, 1]cuda:0"; primals_12: "f32[384][1]cuda:0"; primals_13: "f32[1152, 384][384, 1]cuda:0"; primals_14: "f32[384, 384][384, 1]cuda:0"; primals_15: "f32[384][1]cuda:0"; primals_16: "f32[1024, 384][384, 1]cuda:0"; primals_17: "f32[384, 1024][1024, 1]cuda:0"; primals_18: "f32[384][1]cuda:0"; primals_19: "f32[1152, 384][384, 1]cuda:0"; primals_20: "f32[384, 384][384, 1]cuda:0"; primals_21: "f32[384][1]cuda:0"; primals_22: "f32[1024, 384][384, 1]cuda:0"; primals_23: "f32[384, 1024][1024, 1]cuda:0"; primals_24: "f32[384][1]cuda:0"; primals_25: "f32[1152, 384][384, 1]cuda:0"; primals_26: "f32[384, 384][384, 1]cuda:0"; primals_27: "f32[384][1]cuda:0"; primals_28: "f32[1024, 384][384, 1]cuda:0"; primals_29: "f32[384, 1024][1024, 1]cuda:0"; primals_30: "f32[s6, 384][384, 1]cuda:0"; primals_31: "i64[257][1]cuda:0"; primals_32: "f32[s8, 0][1, 1]cuda:0"; primals_33: "f32[s9, 0][1, 1]cuda:0"; primals_34: "Sym(s5)"; primals_35: "f32[384][1]cuda:0"; primals_36: "f32[1152, 384][384, 1]cuda:0"; primals_37: "f32[384, 384][384, 1]cuda:0"; primals_38: "f32[384][1]cuda:0"; primals_39: "f32[384, 384][384, 1]cuda:0"; primals_40: "f32[768, 384][384, 1]cuda:0"; primals_41: "f32[384, 384][384, 1]cuda:0"; primals_42: "f32[384][1]cuda:0"; primals_43: "f32[1024, 384][384, 1]cuda:0"; primals_44: "f32[384, 1024][1024, 1]cuda:0"; primals_45: "f32[384][1]cuda:0"; primals_46: "f32[1152, 384][384, 1]cuda:0"; primals_47: "f32[384, 384][384, 1]cuda:0"; primals_48: "f32[384][1]cuda:0"; primals_49: "f32[384, 384][384, 1]cuda:0"; primals_50: "f32[768, 384][384, 1]cuda:0"; primals_51: "f32[384, 384][384, 1]cuda:0"; primals_52: "f32[384][1]cuda:0"; primals_53: "f32[1024, 384][384, 1]cuda:0"; primals_54: "f32[384, 1024][1024, 1]cuda:0"; primals_55: "f32[384][1]cuda:0"; primals_56: "f32[1152, 384][384, 1]cuda:0"; primals_57: "f32[384, 384][384, 1]cuda:0"; primals_58: "f32[384][1]cuda:0"; primals_59: "f32[384, 384][384, 1]cuda:0"; primals_60: "f32[768, 384][384, 1]cuda:0"; primals_61: "f32[384, 384][384, 1]cuda:0"; primals_62: "f32[384][1]cuda:0"; primals_63: "f32[1024, 384][384, 1]cuda:0"; primals_64: "f32[384, 1024][1024, 1]cuda:0"; primals_65: "f32[384][1]cuda:0"; primals_66: "f32[1152, 384][384, 1]cuda:0"; primals_67: "f32[384, 384][384, 1]cuda:0"; primals_68: "f32[384][1]cuda:0"; primals_69: "f32[384, 384][384, 1]cuda:0"; primals_70: "f32[768, 384][384, 1]cuda:0"; primals_71: "f32[384, 384][384, 1]cuda:0"; primals_72: "f32[384][1]cuda:0"; primals_73: "f32[1024, 384][384, 1]cuda:0"; primals_74: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s6, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s8, 0][1, 1]cuda:0"; tangents_4: "f32[s9, 0][1, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        alias: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
	        alias_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
	        alias_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        rand: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        alias_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        alias_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        alias_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_6: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_6, [1, 0, 2]);  alias_6 = None
	        alias_7: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_7, [1, 0, 2]);  alias_7 = None
	        alias_8: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_8, [1, 0, 2]);  alias_8 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_11: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
	        alias_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
	        alias_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_11, 0);  alias_11 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_12, 0);  alias_12 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_13, 0);  alias_13 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        alias_14: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_3)
	        alias_15: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0);  getitem_3 = None
	        alias_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_17: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_7);  permute_7 = None
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_17, [1, 0, 2]);  alias_17 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        alias_18: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_1)
	        alias_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
	        alias_20: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        rand_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_1, 0.3);  rand_1 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_2, 0.3);  rand_2 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        alias_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_2)
	        alias_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
	        alias_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_3, 0.3);  rand_3 = None
	        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
	        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
	        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        alias_24: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_4);  view_4 = None
	        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        alias_25: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_5);  view_5 = None
	        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        alias_26: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_6);  view_6 = None
	        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_26, [1, 0, 2]);  alias_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_13);  permute_13 = None
	        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_27, [1, 0, 2]);  alias_27 = None
	        alias_28: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_14);  permute_14 = None
	        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_28, [1, 0, 2]);  alias_28 = None
	        alias_29: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_15);  permute_15 = None
	        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_29, [1, 0, 2]);  alias_29 = None
	        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_3: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_32: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_16);  permute_16 = None
	        alias_33: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_17);  permute_17 = None
	        alias_34: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_18);  permute_18 = None
	        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_32, 0);  alias_32 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_33, 0);  alias_33 = None
	        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_34, 0);  alias_34 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_3, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
	        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
	        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
	        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        alias_35: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_12)
	        alias_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_35);  alias_35 = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0);  getitem_12 = None
	        alias_37: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_37, [1, 0, 2]);  alias_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_38: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_19);  permute_19 = None
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_38, [1, 0, 2]);  alias_38 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
	        mm_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_7, permute_21);  permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
	        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        alias_39: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_3)
	        alias_40: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_39);  alias_39 = None
	        alias_41: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_40);  alias_40 = None
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        rand_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_4, 0.3);  rand_4 = None
	        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_5, 0.3);  rand_5 = None
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        alias_42: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_4)
	        alias_43: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_42);  alias_42 = None
	        alias_44: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_43);  alias_43 = None
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_6, 0.3);  rand_6 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
	        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
	        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        alias_45: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_8);  view_8 = None
	        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_45, [1, 0, 2]);  alias_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        alias_46: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_9);  view_9 = None
	        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_46, [1, 0, 2]);  alias_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        alias_47: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_10);  view_10 = None
	        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_47, [1, 0, 2]);  alias_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_48: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_25);  permute_25 = None
	        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_48, [1, 0, 2]);  alias_48 = None
	        alias_49: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_26);  permute_26 = None
	        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_49, [1, 0, 2]);  alias_49 = None
	        alias_50: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_27);  permute_27 = None
	        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_50, [1, 0, 2]);  alias_50 = None
	        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_5: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_53: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_28);  permute_28 = None
	        alias_54: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_29);  permute_29 = None
	        alias_55: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_30);  permute_30 = None
	        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_53, 0);  alias_53 = None
	        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_54, 0);  alias_54 = None
	        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_55, 0);  alias_55 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_5, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
	        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
	        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
	        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        alias_56: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_21)
	        alias_57: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_56);  alias_56 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0);  getitem_21 = None
	        alias_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_58, [1, 0, 2]);  alias_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_59: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_31);  permute_31 = None
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_59, [1, 0, 2]);  alias_59 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
	        mm_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_11, permute_33);  permute_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
	        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        alias_60: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_5)
	        alias_61: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_60);  alias_60 = None
	        alias_62: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_61);  alias_61 = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        rand_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_7, 0.3);  rand_7 = None
	        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_8, 0.3);  rand_8 = None
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        alias_63: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_6)
	        alias_64: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_63);  alias_63 = None
	        alias_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_64);  alias_64 = None
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_9, 0.3);  rand_9 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
	        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
	        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        alias_66: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_12);  view_12 = None
	        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_66, [1, 0, 2]);  alias_66 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        alias_67: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_13);  view_13 = None
	        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_67, [1, 0, 2]);  alias_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        alias_68: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_14);  view_14 = None
	        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_68, [1, 0, 2]);  alias_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_69: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_37);  permute_37 = None
	        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_69, [1, 0, 2]);  alias_69 = None
	        alias_70: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_38);  permute_38 = None
	        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_70, [1, 0, 2]);  alias_70 = None
	        alias_71: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_39);  permute_39 = None
	        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_71, [1, 0, 2]);  alias_71 = None
	        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_7: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_74: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_40);  permute_40 = None
	        alias_75: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_41);  permute_41 = None
	        alias_76: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_42);  permute_42 = None
	        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_74, 0);  alias_74 = None
	        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_75, 0);  alias_75 = None
	        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_76, 0);  alias_76 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_7, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
	        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
	        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
	        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        alias_77: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_30)
	        alias_78: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_77);  alias_77 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0);  getitem_30 = None
	        alias_79: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_79, [1, 0, 2]);  alias_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_43);  permute_43 = None
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_80, [1, 0, 2]);  alias_80 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
	        mm_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_15, permute_45);  permute_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
	        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        alias_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_7)
	        alias_82: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_81);  alias_81 = None
	        alias_83: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_82);  alias_82 = None
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        rand_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_10, 0.3);  rand_10 = None
	        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_11, 0.3);  rand_11 = None
	        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_9: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_9, [1], True);  pow_9 = None
	        add_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_8, 1e-06);  mean_8 = None
	        rsqrt_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_16);  add_16 = None
	        alias_84: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_8)
	        alias_85: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_84);  alias_84 = None
	        alias_86: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_85);  alias_85 = None
	        mul_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_8)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_45: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_44, primals_35)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int_3: "Sym(s6)" = torch.ops.aten.sym_size.int(primals_30, 0)
	        rand_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_12: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_12, 0.3);  rand_12 = None
	        mul_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_12, mul_45);  mul_45 = None
	        mul_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_46, 1.4285714285714286);  mul_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_48: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_36, [1, 0])
	        mm_16: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_47, permute_48);  permute_48 = None
	        split_4 = torch.ops.aten.split.Tensor(mm_16, 384, 1);  mm_16 = None
	        getitem_36: "f32[s6, 384][1152, 1]cuda:0" = split_4[0]
	        getitem_37: "f32[s6, 384][1152, 1]cuda:0" = split_4[1]
	        getitem_38: "f32[s6, 384][1152, 1]cuda:0" = split_4[2];  split_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_16: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_36, [sym_size_int_3, 6, 64]);  getitem_36 = None
	        alias_87: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_16);  view_16 = None
	        permute_49: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_87, [1, 0, 2]);  alias_87 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_17: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_37, [sym_size_int_3, 6, 64]);  getitem_37 = None
	        alias_88: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_17);  view_17 = None
	        permute_50: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_88, [1, 0, 2]);  alias_88 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_18: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_38, [sym_size_int_3, 6, 64]);  getitem_38 = None
	        alias_89: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_18);  view_18 = None
	        permute_51: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_89, [1, 0, 2]);  alias_89 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_90: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_49);  permute_49 = None
	        permute_52: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_90, [1, 0, 2]);  alias_90 = None
	        alias_91: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_50);  permute_50 = None
	        permute_53: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_91, [1, 0, 2]);  alias_91 = None
	        alias_92: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_51);  permute_51 = None
	        permute_54: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_92, [1, 0, 2]);  alias_92 = None
	        convert_element_type_8: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_9: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        alias_95: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_52);  permute_52 = None
	        alias_96: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_53);  permute_53 = None
	        alias_97: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_54);  permute_54 = None
	        unsqueeze_12: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_95, 0);  alias_95 = None
	        unsqueeze_13: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_96, 0);  alias_96 = None
	        unsqueeze_14: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_97, 0);  alias_97 = None
	        sym_size_int_4: "Sym(s9)" = torch.ops.aten.sym_size.int(primals_33, 0)
	        _efficient_attention_forward_4 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_12, unsqueeze_13, unsqueeze_14, None, convert_element_type_8, convert_element_type_9, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_39: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_4[0]
	        getitem_40: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_4[1]
	        getitem_41: "i64[][]cuda:0" = _efficient_attention_forward_4[2]
	        getitem_42: "i64[][]cuda:0" = _efficient_attention_forward_4[3];  _efficient_attention_forward_4 = None
	        alias_98: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_39)
	        alias_99: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_98);  alias_98 = None
	        squeeze_4: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_39, 0);  getitem_39 = None
	        alias_100: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_4);  squeeze_4 = None
	        permute_55: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_100, [1, 0, 2]);  alias_100 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_101: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_55);  permute_55 = None
	        permute_56: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_101, [1, 0, 2]);  alias_101 = None
	        view_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_56, [sym_size_int_3, 384]);  permute_56 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_57: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_37, [1, 0])
	        mm_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_19, permute_57);  permute_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_30, mm_17);  mm_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_9: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_10, [1], True);  pow_10 = None
	        add_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_9, 1e-06);  mean_9 = None
	        rsqrt_9: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_18);  add_18 = None
	        alias_102: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_9)
	        alias_103: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_102);  alias_102 = None
	        alias_104: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_103);  alias_103 = None
	        mul_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_49: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_48, primals_38)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        rand_13: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_13: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_13, 0.3);  rand_13 = None
	        mul_50: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_13, mul_49);  mul_49 = None
	        mul_51: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_50, 1.4285714285714286);  mul_50 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_58: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_39, [1, 0])
	        mm_18: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_51, permute_58);  permute_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_59: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_40, [1, 0])
	        mm_19: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_59);  permute_59 = None
	        split_5 = torch.ops.aten.split.Tensor(mm_19, 384, 1);  mm_19 = None
	        getitem_45: "f32[s1, 384][768, 1]cuda:0" = split_5[0]
	        getitem_46: "f32[s1, 384][768, 1]cuda:0" = split_5[1];  split_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_20: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [sym_size_int_3, 6, 64]);  mm_18 = None
	        alias_105: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_20);  view_20 = None
	        permute_60: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_105, [1, 0, 2]);  alias_105 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_21: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_45, [sym_size_int, 6, 64]);  getitem_45 = None
	        alias_106: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_21);  view_21 = None
	        permute_61: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_106, [1, 0, 2]);  alias_106 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_22: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_46, [sym_size_int, 6, 64]);  getitem_46 = None
	        alias_107: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_22);  view_22 = None
	        permute_62: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_107, [1, 0, 2]);  alias_107 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_108: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_60);  permute_60 = None
	        permute_63: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_108, [1, 0, 2]);  alias_108 = None
	        alias_109: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_61);  permute_61 = None
	        permute_64: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_109, [1, 0, 2]);  alias_109 = None
	        alias_110: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_62);  permute_62 = None
	        permute_65: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_110, [1, 0, 2]);  alias_110 = None
	        convert_element_type_10: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_11: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_113: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_63);  permute_63 = None
	        alias_114: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_64);  permute_64 = None
	        alias_115: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_65);  permute_65 = None
	        unsqueeze_15: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_113, 0);  alias_113 = None
	        unsqueeze_16: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_114, 0);  alias_114 = None
	        unsqueeze_17: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_115, 0);  alias_115 = None
	        _efficient_attention_forward_5 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_15, unsqueeze_16, unsqueeze_17, None, convert_element_type_10, convert_element_type_11, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_47: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_5[0]
	        getitem_48: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_5[1]
	        getitem_49: "i64[][]cuda:0" = _efficient_attention_forward_5[2]
	        getitem_50: "i64[][]cuda:0" = _efficient_attention_forward_5[3];  _efficient_attention_forward_5 = None
	        alias_116: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_47)
	        alias_117: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_116);  alias_116 = None
	        squeeze_5: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_47, 0);  getitem_47 = None
	        alias_118: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_5);  squeeze_5 = None
	        permute_66: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_118, [1, 0, 2]);  alias_118 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_119: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_66);  permute_66 = None
	        permute_67: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_119, [1, 0, 2]);  alias_119 = None
	        view_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_67, [sym_size_int_3, 384]);  permute_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_68: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_41, [1, 0])
	        mm_20: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_23, permute_68);  permute_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_17, mm_20);  add_17 = mm_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_19, 2)
	        mean_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_11, [1], True);  pow_11 = None
	        add_20: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_10, 1e-06);  mean_10 = None
	        rsqrt_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_20);  add_20 = None
	        alias_120: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_10)
	        alias_121: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_120);  alias_120 = None
	        alias_122: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_121);  alias_121 = None
	        mul_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_19, rsqrt_10)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_53: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, primals_42)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_69: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_43, [1, 0])
	        mm_21: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_53, permute_69);  permute_69 = None
	        sigmoid_4: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_21)
	        mul_54: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_21, sigmoid_4);  sigmoid_4 = None
	        rand_14: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_14: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_14, 0.3);  rand_14 = None
	        mul_55: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_14, mul_54);  mul_54 = None
	        mul_56: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, 1.4285714285714286);  mul_55 = None
	        permute_70: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_44, [1, 0])
	        mm_22: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_56, permute_70);  permute_70 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_15: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_15, 0.3);  rand_15 = None
	        mul_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_15, mm_22);  mm_22 = None
	        mul_58: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_57, 1.4285714285714286);  mul_57 = None
	        add_21: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, mul_58);  mul_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_11: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_12, [1], True);  pow_12 = None
	        add_22: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None
	        rsqrt_11: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
	        alias_123: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_11)
	        alias_124: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_123);  alias_123 = None
	        alias_125: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_124);  alias_124 = None
	        mul_59: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_11)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_59, primals_45)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_16: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_16: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_16, 0.3);  rand_16 = None
	        mul_61: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_16, mul_60);  mul_60 = None
	        mul_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_61, 1.4285714285714286);  mul_61 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_71: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_46, [1, 0])
	        mm_23: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_62, permute_71);  permute_71 = None
	        split_6 = torch.ops.aten.split.Tensor(mm_23, 384, 1);  mm_23 = None
	        getitem_53: "f32[s6, 384][1152, 1]cuda:0" = split_6[0]
	        getitem_54: "f32[s6, 384][1152, 1]cuda:0" = split_6[1]
	        getitem_55: "f32[s6, 384][1152, 1]cuda:0" = split_6[2];  split_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_24: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_53, [sym_size_int_3, 6, 64]);  getitem_53 = None
	        alias_126: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_24);  view_24 = None
	        permute_72: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_126, [1, 0, 2]);  alias_126 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_25: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_54, [sym_size_int_3, 6, 64]);  getitem_54 = None
	        alias_127: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_25);  view_25 = None
	        permute_73: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_127, [1, 0, 2]);  alias_127 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_26: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_55, [sym_size_int_3, 6, 64]);  getitem_55 = None
	        alias_128: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_26);  view_26 = None
	        permute_74: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_128, [1, 0, 2]);  alias_128 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_129: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_72);  permute_72 = None
	        permute_75: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_129, [1, 0, 2]);  alias_129 = None
	        alias_130: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_73);  permute_73 = None
	        permute_76: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_130, [1, 0, 2]);  alias_130 = None
	        alias_131: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_74);  permute_74 = None
	        permute_77: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_131, [1, 0, 2]);  alias_131 = None
	        convert_element_type_12: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_13: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        alias_134: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_75);  permute_75 = None
	        alias_135: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_76);  permute_76 = None
	        alias_136: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_77);  permute_77 = None
	        unsqueeze_18: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_134, 0);  alias_134 = None
	        unsqueeze_19: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_135, 0);  alias_135 = None
	        unsqueeze_20: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_136, 0);  alias_136 = None
	        _efficient_attention_forward_6 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_18, unsqueeze_19, unsqueeze_20, None, convert_element_type_12, convert_element_type_13, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_56: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_6[0]
	        getitem_57: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_6[1]
	        getitem_58: "i64[][]cuda:0" = _efficient_attention_forward_6[2]
	        getitem_59: "i64[][]cuda:0" = _efficient_attention_forward_6[3];  _efficient_attention_forward_6 = None
	        alias_137: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_56)
	        alias_138: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_137);  alias_137 = None
	        squeeze_6: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_56, 0);  getitem_56 = None
	        alias_139: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_6);  squeeze_6 = None
	        permute_78: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_139, [1, 0, 2]);  alias_139 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_140: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_78);  permute_78 = None
	        permute_79: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_140, [1, 0, 2]);  alias_140 = None
	        view_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_79, [sym_size_int_3, 384]);  permute_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_80: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_47, [1, 0])
	        mm_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_27, permute_80);  permute_80 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_21, mm_24);  mm_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_13: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_13, [1], True);  pow_13 = None
	        add_24: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None
	        rsqrt_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_24);  add_24 = None
	        alias_141: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_12)
	        alias_142: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_141);  alias_141 = None
	        alias_143: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_142);  alias_142 = None
	        mul_63: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_12)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_64: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_63, primals_48)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        rand_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_17: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_17, 0.3);  rand_17 = None
	        mul_65: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_17, mul_64);  mul_64 = None
	        mul_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, 1.4285714285714286);  mul_65 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_81: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_49, [1, 0])
	        mm_25: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_66, permute_81);  permute_81 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_82: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_50, [1, 0])
	        mm_26: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_82);  permute_82 = None
	        split_7 = torch.ops.aten.split.Tensor(mm_26, 384, 1);  mm_26 = None
	        getitem_62: "f32[s1, 384][768, 1]cuda:0" = split_7[0]
	        getitem_63: "f32[s1, 384][768, 1]cuda:0" = split_7[1];  split_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_28: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_25, [sym_size_int_3, 6, 64]);  mm_25 = None
	        alias_144: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_28);  view_28 = None
	        permute_83: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_144, [1, 0, 2]);  alias_144 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_29: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_62, [sym_size_int, 6, 64]);  getitem_62 = None
	        alias_145: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_29);  view_29 = None
	        permute_84: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_145, [1, 0, 2]);  alias_145 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_30: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_63, [sym_size_int, 6, 64]);  getitem_63 = None
	        alias_146: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_30);  view_30 = None
	        permute_85: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_146, [1, 0, 2]);  alias_146 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_147: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_83);  permute_83 = None
	        permute_86: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_147, [1, 0, 2]);  alias_147 = None
	        alias_148: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_84);  permute_84 = None
	        permute_87: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_148, [1, 0, 2]);  alias_148 = None
	        alias_149: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_85);  permute_85 = None
	        permute_88: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_149, [1, 0, 2]);  alias_149 = None
	        convert_element_type_14: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_15: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_152: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_86);  permute_86 = None
	        alias_153: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_87);  permute_87 = None
	        alias_154: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_88);  permute_88 = None
	        unsqueeze_21: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_152, 0);  alias_152 = None
	        unsqueeze_22: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_153, 0);  alias_153 = None
	        unsqueeze_23: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_154, 0);  alias_154 = None
	        _efficient_attention_forward_7 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_21, unsqueeze_22, unsqueeze_23, None, convert_element_type_14, convert_element_type_15, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_64: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_7[0]
	        getitem_65: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_7[1]
	        getitem_66: "i64[][]cuda:0" = _efficient_attention_forward_7[2]
	        getitem_67: "i64[][]cuda:0" = _efficient_attention_forward_7[3];  _efficient_attention_forward_7 = None
	        alias_155: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_64)
	        alias_156: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_155);  alias_155 = None
	        squeeze_7: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_64, 0);  getitem_64 = None
	        alias_157: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_7);  squeeze_7 = None
	        permute_89: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_157, [1, 0, 2]);  alias_157 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_158: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_89);  permute_89 = None
	        permute_90: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_158, [1, 0, 2]);  alias_158 = None
	        view_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_90, [sym_size_int_3, 384]);  permute_90 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_91: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_51, [1, 0])
	        mm_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_31, permute_91);  permute_91 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_25: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, mm_27);  add_23 = mm_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_14: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_25, 2)
	        mean_13: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_14, [1], True);  pow_14 = None
	        add_26: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None
	        rsqrt_13: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_26);  add_26 = None
	        alias_159: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_13)
	        alias_160: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_159);  alias_159 = None
	        alias_161: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_160);  alias_160 = None
	        mul_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, rsqrt_13)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_67, primals_52)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_92: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_53, [1, 0])
	        mm_28: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_68, permute_92);  permute_92 = None
	        sigmoid_5: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_28)
	        mul_69: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_28, sigmoid_5);  sigmoid_5 = None
	        rand_18: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_18: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_18, 0.3);  rand_18 = None
	        mul_70: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_18, mul_69);  mul_69 = None
	        mul_71: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_70, 1.4285714285714286);  mul_70 = None
	        permute_93: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_54, [1, 0])
	        mm_29: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_71, permute_93);  permute_93 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_19: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_19, 0.3);  rand_19 = None
	        mul_72: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_19, mm_29);  mm_29 = None
	        mul_73: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_72, 1.4285714285714286);  mul_72 = None
	        add_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_25, mul_73);  mul_73 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_15, [1], True);  pow_15 = None
	        add_28: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None
	        rsqrt_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_28);  add_28 = None
	        alias_162: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_14)
	        alias_163: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_162);  alias_162 = None
	        alias_164: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_163);  alias_163 = None
	        mul_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_14)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_75: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_74, primals_55)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_20: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_20: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_20, 0.3);  rand_20 = None
	        mul_76: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_20, mul_75);  mul_75 = None
	        mul_77: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_76, 1.4285714285714286);  mul_76 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_94: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_56, [1, 0])
	        mm_30: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_77, permute_94);  permute_94 = None
	        split_8 = torch.ops.aten.split.Tensor(mm_30, 384, 1);  mm_30 = None
	        getitem_70: "f32[s6, 384][1152, 1]cuda:0" = split_8[0]
	        getitem_71: "f32[s6, 384][1152, 1]cuda:0" = split_8[1]
	        getitem_72: "f32[s6, 384][1152, 1]cuda:0" = split_8[2];  split_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_32: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_70, [sym_size_int_3, 6, 64]);  getitem_70 = None
	        alias_165: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_32);  view_32 = None
	        permute_95: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_165, [1, 0, 2]);  alias_165 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_33: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_71, [sym_size_int_3, 6, 64]);  getitem_71 = None
	        alias_166: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_33);  view_33 = None
	        permute_96: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_166, [1, 0, 2]);  alias_166 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_34: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_72, [sym_size_int_3, 6, 64]);  getitem_72 = None
	        alias_167: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_34);  view_34 = None
	        permute_97: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_167, [1, 0, 2]);  alias_167 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_168: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_95);  permute_95 = None
	        permute_98: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_168, [1, 0, 2]);  alias_168 = None
	        alias_169: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_96);  permute_96 = None
	        permute_99: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_169, [1, 0, 2]);  alias_169 = None
	        alias_170: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_97);  permute_97 = None
	        permute_100: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_170, [1, 0, 2]);  alias_170 = None
	        convert_element_type_16: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_17: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        alias_173: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_98);  permute_98 = None
	        alias_174: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_99);  permute_99 = None
	        alias_175: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_100);  permute_100 = None
	        unsqueeze_24: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_173, 0);  alias_173 = None
	        unsqueeze_25: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_174, 0);  alias_174 = None
	        unsqueeze_26: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_175, 0);  alias_175 = None
	        _efficient_attention_forward_8 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_24, unsqueeze_25, unsqueeze_26, None, convert_element_type_16, convert_element_type_17, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_73: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_8[0]
	        getitem_74: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_8[1]
	        getitem_75: "i64[][]cuda:0" = _efficient_attention_forward_8[2]
	        getitem_76: "i64[][]cuda:0" = _efficient_attention_forward_8[3];  _efficient_attention_forward_8 = None
	        alias_176: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_73)
	        alias_177: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_176);  alias_176 = None
	        squeeze_8: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_73, 0);  getitem_73 = None
	        alias_178: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_8);  squeeze_8 = None
	        permute_101: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_178, [1, 0, 2]);  alias_178 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_179: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_101);  permute_101 = None
	        permute_102: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_179, [1, 0, 2]);  alias_179 = None
	        view_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_102, [sym_size_int_3, 384]);  permute_102 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_103: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_57, [1, 0])
	        mm_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_35, permute_103);  permute_103 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_29: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, mm_31);  mm_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_16: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_15: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_16, [1], True);  pow_16 = None
	        add_30: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None
	        rsqrt_15: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_30);  add_30 = None
	        alias_180: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_15)
	        alias_181: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_180);  alias_180 = None
	        alias_182: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_181);  alias_181 = None
	        mul_78: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_15)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_79: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_78, primals_58)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        rand_21: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_21: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_21, 0.3);  rand_21 = None
	        mul_80: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_21, mul_79);  mul_79 = None
	        mul_81: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, 1.4285714285714286);  mul_80 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_104: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_59, [1, 0])
	        mm_32: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_81, permute_104);  permute_104 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_105: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_60, [1, 0])
	        mm_33: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_105);  permute_105 = None
	        split_9 = torch.ops.aten.split.Tensor(mm_33, 384, 1);  mm_33 = None
	        getitem_79: "f32[s1, 384][768, 1]cuda:0" = split_9[0]
	        getitem_80: "f32[s1, 384][768, 1]cuda:0" = split_9[1];  split_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_36: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_32, [sym_size_int_3, 6, 64]);  mm_32 = None
	        alias_183: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_36);  view_36 = None
	        permute_106: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_183, [1, 0, 2]);  alias_183 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_37: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_79, [sym_size_int, 6, 64]);  getitem_79 = None
	        alias_184: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_37);  view_37 = None
	        permute_107: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_184, [1, 0, 2]);  alias_184 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_38: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_80, [sym_size_int, 6, 64]);  getitem_80 = None
	        alias_185: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_38);  view_38 = None
	        permute_108: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_185, [1, 0, 2]);  alias_185 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_186: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_106);  permute_106 = None
	        permute_109: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_186, [1, 0, 2]);  alias_186 = None
	        alias_187: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_107);  permute_107 = None
	        permute_110: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_187, [1, 0, 2]);  alias_187 = None
	        alias_188: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_108);  permute_108 = None
	        permute_111: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_188, [1, 0, 2]);  alias_188 = None
	        convert_element_type_18: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_19: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_191: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_109);  permute_109 = None
	        alias_192: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_110);  permute_110 = None
	        alias_193: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_111);  permute_111 = None
	        unsqueeze_27: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_191, 0);  alias_191 = None
	        unsqueeze_28: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_192, 0);  alias_192 = None
	        unsqueeze_29: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_193, 0);  alias_193 = None
	        _efficient_attention_forward_9 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_27, unsqueeze_28, unsqueeze_29, None, convert_element_type_18, convert_element_type_19, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_81: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_9[0]
	        getitem_82: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_9[1]
	        getitem_83: "i64[][]cuda:0" = _efficient_attention_forward_9[2]
	        getitem_84: "i64[][]cuda:0" = _efficient_attention_forward_9[3];  _efficient_attention_forward_9 = None
	        alias_194: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_81)
	        alias_195: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_194);  alias_194 = None
	        squeeze_9: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_81, 0);  getitem_81 = None
	        alias_196: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_9);  squeeze_9 = None
	        permute_112: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_196, [1, 0, 2]);  alias_196 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_197: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_112);  permute_112 = None
	        permute_113: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_197, [1, 0, 2]);  alias_197 = None
	        view_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_113, [sym_size_int_3, 384]);  permute_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_114: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_61, [1, 0])
	        mm_34: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_39, permute_114);  permute_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_29, mm_34);  add_29 = mm_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_31, 2)
	        mean_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_17, [1], True);  pow_17 = None
	        add_32: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None
	        rsqrt_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_32);  add_32 = None
	        alias_198: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_16)
	        alias_199: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_198);  alias_198 = None
	        alias_200: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_199);  alias_199 = None
	        mul_82: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_31, rsqrt_16)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_83: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_82, primals_62)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_115: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_63, [1, 0])
	        mm_35: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_83, permute_115);  permute_115 = None
	        sigmoid_6: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_35)
	        mul_84: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_35, sigmoid_6);  sigmoid_6 = None
	        rand_22: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_22: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_22, 0.3);  rand_22 = None
	        mul_85: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_22, mul_84);  mul_84 = None
	        mul_86: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, 1.4285714285714286);  mul_85 = None
	        permute_116: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_64, [1, 0])
	        mm_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_86, permute_116);  permute_116 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_23: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_23, 0.3);  rand_23 = None
	        mul_87: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_23, mm_36);  mm_36 = None
	        mul_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, 1.4285714285714286);  mul_87 = None
	        add_33: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_31, mul_88);  mul_88 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_18: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_17: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_18, [1], True);  pow_18 = None
	        add_34: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None
	        rsqrt_17: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_34);  add_34 = None
	        alias_201: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_17)
	        alias_202: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_201);  alias_201 = None
	        alias_203: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_202);  alias_202 = None
	        mul_89: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_17)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_89, primals_65)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_24: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_24, 0.3);  rand_24 = None
	        mul_91: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_24, mul_90);  mul_90 = None
	        mul_92: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_91, 1.4285714285714286);  mul_91 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_117: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_66, [1, 0])
	        mm_37: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_92, permute_117);  permute_117 = None
	        split_10 = torch.ops.aten.split.Tensor(mm_37, 384, 1);  mm_37 = None
	        getitem_87: "f32[s6, 384][1152, 1]cuda:0" = split_10[0]
	        getitem_88: "f32[s6, 384][1152, 1]cuda:0" = split_10[1]
	        getitem_89: "f32[s6, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_40: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_87, [sym_size_int_3, 6, 64]);  getitem_87 = None
	        alias_204: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_40);  view_40 = None
	        permute_118: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_204, [1, 0, 2]);  alias_204 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_41: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_88, [sym_size_int_3, 6, 64]);  getitem_88 = None
	        alias_205: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_41);  view_41 = None
	        permute_119: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_205, [1, 0, 2]);  alias_205 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_42: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_89, [sym_size_int_3, 6, 64]);  getitem_89 = None
	        alias_206: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_42);  view_42 = None
	        permute_120: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_206, [1, 0, 2]);  alias_206 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_207: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_118);  permute_118 = None
	        permute_121: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_207, [1, 0, 2]);  alias_207 = None
	        alias_208: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_119);  permute_119 = None
	        permute_122: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_208, [1, 0, 2]);  alias_208 = None
	        alias_209: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_120);  permute_120 = None
	        permute_123: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_209, [1, 0, 2]);  alias_209 = None
	        convert_element_type_20: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_21: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        alias_212: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_121);  permute_121 = None
	        alias_213: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_122);  permute_122 = None
	        alias_214: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_123);  permute_123 = None
	        unsqueeze_30: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_212, 0);  alias_212 = None
	        unsqueeze_31: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_213, 0);  alias_213 = None
	        unsqueeze_32: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_214, 0);  alias_214 = None
	        _efficient_attention_forward_10 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_30, unsqueeze_31, unsqueeze_32, None, convert_element_type_20, convert_element_type_21, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_90: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_10[0]
	        getitem_91: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_10[1]
	        getitem_92: "i64[][]cuda:0" = _efficient_attention_forward_10[2]
	        getitem_93: "i64[][]cuda:0" = _efficient_attention_forward_10[3];  _efficient_attention_forward_10 = None
	        alias_215: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_90)
	        alias_216: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_215);  alias_215 = None
	        squeeze_10: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_90, 0);  getitem_90 = None
	        alias_217: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_10);  squeeze_10 = None
	        permute_124: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_217, [1, 0, 2]);  alias_217 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_218: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_124);  permute_124 = None
	        permute_125: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_218, [1, 0, 2]);  alias_218 = None
	        view_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_125, [sym_size_int_3, 384]);  permute_125 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_126: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_67, [1, 0])
	        mm_38: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_43, permute_126);  permute_126 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_33, mm_38);  mm_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_19, [1], True);  pow_19 = None
	        add_36: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None
	        rsqrt_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_36);  add_36 = None
	        alias_219: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_18)
	        alias_220: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_219);  alias_219 = None
	        alias_221: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_220);  alias_220 = None
	        mul_93: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_18)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_94: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_93, primals_68)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        rand_25: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_25: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_25, 0.3);  rand_25 = None
	        mul_95: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_25, mul_94);  mul_94 = None
	        mul_96: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_95, 1.4285714285714286);  mul_95 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_127: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_69, [1, 0])
	        mm_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_96, permute_127);  permute_127 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_128: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_70, [1, 0])
	        mm_40: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_128);  permute_128 = None
	        split_11 = torch.ops.aten.split.Tensor(mm_40, 384, 1);  mm_40 = None
	        getitem_96: "f32[s1, 384][768, 1]cuda:0" = split_11[0]
	        getitem_97: "f32[s1, 384][768, 1]cuda:0" = split_11[1];  split_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_44: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_39, [sym_size_int_3, 6, 64]);  mm_39 = None
	        alias_222: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_44);  view_44 = None
	        permute_129: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_222, [1, 0, 2]);  alias_222 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_45: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_96, [sym_size_int, 6, 64]);  getitem_96 = None
	        alias_223: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_45);  view_45 = None
	        permute_130: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_223, [1, 0, 2]);  alias_223 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_46: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_97, [sym_size_int, 6, 64]);  getitem_97 = None
	        alias_224: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_46);  view_46 = None
	        permute_131: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(alias_224, [1, 0, 2]);  alias_224 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_225: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_129);  permute_129 = None
	        permute_132: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_225, [1, 0, 2]);  alias_225 = None
	        alias_226: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_130);  permute_130 = None
	        permute_133: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_226, [1, 0, 2]);  alias_226 = None
	        alias_227: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.alias.default(permute_131);  permute_131 = None
	        permute_134: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_227, [1, 0, 2]);  alias_227 = None
	        convert_element_type_22: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_23: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_230: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_132);  permute_132 = None
	        alias_231: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_133);  permute_133 = None
	        alias_232: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_134);  permute_134 = None
	        unsqueeze_33: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_230, 0);  alias_230 = None
	        unsqueeze_34: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_231, 0);  alias_231 = None
	        unsqueeze_35: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_232, 0);  alias_232 = None
	        _efficient_attention_forward_11 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_33, unsqueeze_34, unsqueeze_35, None, convert_element_type_22, convert_element_type_23, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_98: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_11[0]
	        getitem_99: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_11[1]
	        getitem_100: "i64[][]cuda:0" = _efficient_attention_forward_11[2]
	        getitem_101: "i64[][]cuda:0" = _efficient_attention_forward_11[3];  _efficient_attention_forward_11 = None
	        alias_233: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_98)
	        alias_234: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_233);  alias_233 = None
	        squeeze_11: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0);  getitem_98 = None
	        alias_235: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_11);  squeeze_11 = None
	        permute_135: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_235, [1, 0, 2]);  alias_235 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_236: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_135);  permute_135 = None
	        permute_136: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_236, [1, 0, 2]);  alias_236 = None
	        view_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_136, [sym_size_int_3, 384]);  permute_136 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_137: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_71, [1, 0])
	        mm_41: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_47, permute_137);  permute_137 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_37: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_35, mm_41);  add_35 = mm_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_20: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_37, 2)
	        mean_19: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_20, [1], True);  pow_20 = None
	        add_38: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None
	        rsqrt_19: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_38);  add_38 = None
	        alias_237: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_19)
	        alias_238: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_237);  alias_237 = None
	        alias_239: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_238);  alias_238 = None
	        mul_97: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_37, rsqrt_19)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_98: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_97, primals_72)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_138: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_73, [1, 0])
	        mm_42: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_98, permute_138);  permute_138 = None
	        sigmoid_7: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_42)
	        mul_99: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, sigmoid_7);  sigmoid_7 = None
	        rand_26: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_26: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_26, 0.3);  rand_26 = None
	        mul_100: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_26, mul_99);  mul_99 = None
	        mul_101: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_100, 1.4285714285714286);  mul_100 = None
	        permute_139: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_74, [1, 0])
	        mm_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_101, permute_139);  permute_139 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int_3, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_27: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_27, 0.3);  rand_27 = None
	        mul_102: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_27, mm_43);  mm_43 = None
	        mul_103: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, 1.4285714285714286);  mul_102 = None
	        add_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_37, mul_103);  mul_103 = None
	        convert_element_type_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_27, torch.float32);  gt_27 = None
	        mul_104: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_24, 1.4285714285714286);  convert_element_type_24 = None
	        mul_105: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_104);  mul_104 = None
	        clone: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_105, memory_format = torch.contiguous_format);  mul_105 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_44: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone, primals_74);  primals_74 = None
	        permute_140: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(clone, [1, 0]);  clone = None
	        mm_45: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_140, mul_101);  permute_140 = mul_101 = None
	        convert_element_type_25: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_26, torch.float32);  gt_26 = None
	        mul_106: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_25, 1.4285714285714286);  convert_element_type_25 = None
	        mul_107: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_44, mul_106);  mm_44 = mul_106 = None
	        clone_1: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_107, memory_format = torch.contiguous_format);  mul_107 = None
	        sigmoid_8: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_42)
	        full_24: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_24, sigmoid_8);  full_24 = None
	        mul_108: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, sub);  mm_42 = sub = None
	        add_40: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_108, 1);  mul_108 = None
	        mul_109: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_8, add_40);  sigmoid_8 = add_40 = None
	        mul_110: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_1, mul_109);  clone_1 = mul_109 = None
	        mm_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_110, primals_73);  primals_73 = None
	        permute_142: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_110, [1, 0]);  mul_110 = None
	        mm_47: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_142, mul_98);  permute_142 = mul_98 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_111: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, mul_97);  mul_97 = None
	        mul_112: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, primals_72);  mm_46 = primals_72 = None
	        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_111, [0], True);  mul_111 = None
	        unsqueeze_36: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view_48: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_36, [384]);  unsqueeze_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_113: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, add_37)
	        mul_114: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, rsqrt_19);  mul_112 = rsqrt_19 = None
	        sum_2: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_113, [1], True);  mul_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_41: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_114);  tangents_1 = mul_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_240: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_239);  alias_239 = None
	        alias_241: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_240);  alias_240 = None
	        alias_242: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_241);  alias_241 = None
	        pow_21: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_242, 3);  alias_242 = None
	        mul_115: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
	        mul_116: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, pow_21);  mul_115 = pow_21 = None
	        expand: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_116, [-1, 384]);  mul_116 = None
	        div: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
	        pow_22: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_37, 1.0);  add_37 = None
	        mul_117: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_22, 2.0);  pow_22 = None
	        mul_118: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_117);  div = mul_117 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_42: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_41, mul_118);  add_41 = mul_118 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_42, primals_71);  primals_71 = None
	        permute_143: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_42, [1, 0])
	        mm_49: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_143, view_47);  permute_143 = view_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_49: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_48, [sym_size_int_3, 6, 64]);  mm_48 = None
	        alias_243: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_49);  view_49 = None
	        permute_144: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_243, [1, 0, 2]);  alias_243 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_244: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_144);  permute_144 = None
	        permute_145: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_244, [1, 0, 2]);  alias_244 = None
	        alias_245: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_145);  permute_145 = None
	        unsqueeze_37: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_245, 0);  alias_245 = None
	        alias_246: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_234);  alias_234 = None
	        alias_247: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_246);  alias_246 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_37, unsqueeze_33, unsqueeze_34, unsqueeze_35, None, alias_247, convert_element_type_22, convert_element_type_23, sym_size_int_4, sym_size_int_1, getitem_99, 0.0, getitem_100, getitem_101, 0, False);  unsqueeze_37 = unsqueeze_33 = unsqueeze_34 = unsqueeze_35 = alias_247 = convert_element_type_22 = convert_element_type_23 = getitem_99 = getitem_100 = getitem_101 = None
	        getitem_104: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_105: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_106: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_106, 0);  getitem_106 = None
	        squeeze_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_105, 0);  getitem_105 = None
	        squeeze_14: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_104, 0);  getitem_104 = None
	        alias_248: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_12);  squeeze_12 = None
	        permute_146: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_248, [1, 0, 2]);  alias_248 = None
	        alias_249: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_13);  squeeze_13 = None
	        permute_147: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_249, [1, 0, 2]);  alias_249 = None
	        alias_250: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_14);  squeeze_14 = None
	        permute_148: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_250, [1, 0, 2]);  alias_250 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_251: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_146);  permute_146 = None
	        permute_149: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_251, [1, 0, 2]);  alias_251 = None
	        view_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_149, [sym_size_int, 384]);  permute_149 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_252: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_147);  permute_147 = None
	        permute_150: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_252, [1, 0, 2]);  alias_252 = None
	        view_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_150, [sym_size_int, 384]);  permute_150 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_253: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_148);  permute_148 = None
	        permute_151: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_253, [1, 0, 2]);  alias_253 = None
	        view_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_151, [sym_size_int_3, 384]);  permute_151 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        full_25: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 768], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_12 = torch.ops.aten.split.Tensor(full_25, 384, 1)
	        getitem_108: "f32[s1, 384][768, 1]cuda:0" = split_12[0];  split_12 = None
	        alias_254: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_108);  getitem_108 = None
	        alias_255: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_51);  view_51 = None
	        copy: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_254, alias_255);  alias_254 = alias_255 = None
	        slice_scatter: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_25, copy, 1, 0, 384);  full_25 = copy = None
	        alias_258: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_50);  view_50 = None
	        split_15 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_115: "f32[s1, 384][768, 1]cuda:0" = split_15[1];  split_15 = None
	        alias_259: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_115);  getitem_115 = None
	        copy_1: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_259, alias_258);  alias_259 = alias_258 = None
	        slice_scatter_1: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        mm_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_1, primals_70);  primals_70 = None
	        permute_153: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_1, [1, 0]);  slice_scatter_1 = None
	        mm_51: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_153, add_15);  permute_153 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_52, primals_69);  primals_69 = None
	        permute_154: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_52, [1, 0]);  view_52 = None
	        mm_53: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_154, mul_96);  permute_154 = mul_96 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_26: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_25, torch.float32);  gt_25 = None
	        mul_119: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_26, 1.4285714285714286);  convert_element_type_26 = None
	        mul_120: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_52, mul_119);  mm_52 = mul_119 = None
	        clone_2: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_120, memory_format = torch.contiguous_format);  mul_120 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_121: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, mul_93);  mul_93 = None
	        mul_122: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, primals_68);  clone_2 = primals_68 = None
	        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_121, [0], True);  mul_121 = None
	        unsqueeze_38: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
	        view_53: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_38, [384]);  unsqueeze_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_123: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_122, add_33)
	        mul_124: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_122, rsqrt_18);  mul_122 = rsqrt_18 = None
	        sum_4: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_123, [1], True);  mul_123 = None
	        alias_261: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_221);  alias_221 = None
	        alias_262: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_261);  alias_261 = None
	        alias_263: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_262);  alias_262 = None
	        pow_23: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_263, 3);  alias_263 = None
	        mul_125: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
	        mul_126: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_125, pow_23);  mul_125 = pow_23 = None
	        expand_1: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_126, [-1, 384]);  mul_126 = None
	        div_1: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
	        pow_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 1.0)
	        mul_127: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_24, 2.0);  pow_24 = None
	        mul_128: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_127);  div_1 = mul_127 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_124, mul_128);  mul_124 = mul_128 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_43, add_42);  add_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_54: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_42, primals_67);  primals_67 = None
	        permute_155: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_42, [1, 0]);  add_42 = None
	        mm_55: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_155, view_43);  permute_155 = view_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_54: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_54, [sym_size_int_3, 6, 64]);  mm_54 = None
	        alias_264: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_54);  view_54 = None
	        permute_156: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_264, [1, 0, 2]);  alias_264 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_265: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_156);  permute_156 = None
	        permute_157: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_265, [1, 0, 2]);  alias_265 = None
	        alias_266: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_157);  permute_157 = None
	        unsqueeze_39: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_266, 0);  alias_266 = None
	        alias_267: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_216);  alias_216 = None
	        alias_268: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_267);  alias_267 = None
	        _efficient_attention_backward_1 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_39, unsqueeze_30, unsqueeze_31, unsqueeze_32, None, alias_268, convert_element_type_20, convert_element_type_21, sym_size_int_4, sym_size_int_4, getitem_91, 0.0, getitem_92, getitem_93, 1, False);  unsqueeze_39 = unsqueeze_30 = unsqueeze_31 = unsqueeze_32 = alias_268 = convert_element_type_20 = convert_element_type_21 = getitem_91 = getitem_92 = getitem_93 = None
	        getitem_120: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[0]
	        getitem_121: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[1]
	        getitem_122: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[2];  _efficient_attention_backward_1 = None
	        squeeze_15: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_122, 0);  getitem_122 = None
	        squeeze_16: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_121, 0);  getitem_121 = None
	        squeeze_17: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_120, 0);  getitem_120 = None
	        alias_269: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_15);  squeeze_15 = None
	        permute_158: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_269, [1, 0, 2]);  alias_269 = None
	        alias_270: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_16);  squeeze_16 = None
	        permute_159: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_270, [1, 0, 2]);  alias_270 = None
	        alias_271: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_17);  squeeze_17 = None
	        permute_160: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_271, [1, 0, 2]);  alias_271 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_272: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_158);  permute_158 = None
	        permute_161: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_272, [1, 0, 2]);  alias_272 = None
	        view_55: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_161, [sym_size_int_3, 384]);  permute_161 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_273: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_159);  permute_159 = None
	        permute_162: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_273, [1, 0, 2]);  alias_273 = None
	        view_56: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_162, [sym_size_int_3, 384]);  permute_162 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_274: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_160);  permute_160 = None
	        permute_163: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_274, [1, 0, 2]);  alias_274 = None
	        view_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_163, [sym_size_int_3, 384]);  permute_163 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_26: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_18 = torch.ops.aten.split.Tensor(full_26, 384, 1)
	        getitem_124: "f32[s6, 384][1152, 1]cuda:0" = split_18[0];  split_18 = None
	        alias_275: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_124);  getitem_124 = None
	        alias_276: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_57);  view_57 = None
	        copy_2: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_275, alias_276);  alias_275 = alias_276 = None
	        slice_scatter_2: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_26, copy_2, 1, 0, 384);  full_26 = copy_2 = None
	        alias_279: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_56);  view_56 = None
	        split_21 = torch.ops.aten.split.Tensor(slice_scatter_2, 384, 1)
	        getitem_134: "f32[s6, 384][1152, 1]cuda:0" = split_21[1];  split_21 = None
	        alias_280: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_134);  getitem_134 = None
	        copy_3: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_280, alias_279);  alias_280 = alias_279 = None
	        slice_scatter_3: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_2, copy_3, 1, 384, 768);  slice_scatter_2 = copy_3 = None
	        alias_283: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_55);  view_55 = None
	        split_24 = torch.ops.aten.split.Tensor(slice_scatter_3, 384, 1)
	        getitem_144: "f32[s6, 384][1152, 1]cuda:0" = split_24[2];  split_24 = None
	        alias_284: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_144);  getitem_144 = None
	        copy_4: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_284, alias_283);  alias_284 = alias_283 = None
	        slice_scatter_4: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, copy_4, 1, 768, 1152);  slice_scatter_3 = copy_4 = None
	        mm_56: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_4, primals_66);  primals_66 = None
	        permute_165: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_4, [1, 0]);  slice_scatter_4 = None
	        mm_57: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_165, mul_92);  permute_165 = mul_92 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_24, torch.float32);  gt_24 = None
	        mul_129: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_27, 1.4285714285714286);  convert_element_type_27 = None
	        mul_130: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_56, mul_129);  mm_56 = mul_129 = None
	        clone_3: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_130, memory_format = torch.contiguous_format);  mul_130 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_131: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_3, mul_89);  mul_89 = None
	        mul_132: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_3, primals_65);  clone_3 = primals_65 = None
	        sum_5: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_131, [0], True);  mul_131 = None
	        unsqueeze_40: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_5, 0);  sum_5 = None
	        view_58: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_40, [384]);  unsqueeze_40 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_133: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_132, add_33)
	        mul_134: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_132, rsqrt_17);  mul_132 = rsqrt_17 = None
	        sum_6: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_133, [1], True);  mul_133 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_45: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_44, mul_134);  add_44 = mul_134 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_286: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_203);  alias_203 = None
	        alias_287: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_286);  alias_286 = None
	        alias_288: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_287);  alias_287 = None
	        pow_25: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_288, 3);  alias_288 = None
	        mul_135: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_6, -0.5);  sum_6 = None
	        mul_136: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_135, pow_25);  mul_135 = pow_25 = None
	        expand_2: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_136, [-1, 384]);  mul_136 = None
	        div_2: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_2, 384);  expand_2 = None
	        pow_26: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 1.0);  add_33 = None
	        mul_137: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_26, 2.0);  pow_26 = None
	        mul_138: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_2, mul_137);  div_2 = mul_137 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_45, mul_138);  add_45 = mul_138 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_28: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_23, torch.float32);  gt_23 = None
	        mul_139: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_28, 1.4285714285714286);  convert_element_type_28 = None
	        mul_140: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_46, mul_139);  mul_139 = None
	        clone_4: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_140, memory_format = torch.contiguous_format);  mul_140 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_58: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_4, primals_64);  primals_64 = None
	        permute_166: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_4, [1, 0]);  clone_4 = None
	        mm_59: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_166, mul_86);  permute_166 = mul_86 = None
	        convert_element_type_29: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_22, torch.float32);  gt_22 = None
	        mul_141: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_29, 1.4285714285714286);  convert_element_type_29 = None
	        mul_142: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_58, mul_141);  mm_58 = mul_141 = None
	        clone_5: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_142, memory_format = torch.contiguous_format);  mul_142 = None
	        sigmoid_9: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_35)
	        full_27: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_1: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_27, sigmoid_9);  full_27 = None
	        mul_143: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_35, sub_1);  mm_35 = sub_1 = None
	        add_47: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_143, 1);  mul_143 = None
	        mul_144: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_9, add_47);  sigmoid_9 = add_47 = None
	        mul_145: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_5, mul_144);  clone_5 = mul_144 = None
	        mm_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_145, primals_63);  primals_63 = None
	        permute_168: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_145, [1, 0]);  mul_145 = None
	        mm_61: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_168, mul_83);  permute_168 = mul_83 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_146: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_60, mul_82);  mul_82 = None
	        mul_147: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_60, primals_62);  mm_60 = primals_62 = None
	        sum_7: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_146, [0], True);  mul_146 = None
	        unsqueeze_41: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_7, 0);  sum_7 = None
	        view_59: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_41, [384]);  unsqueeze_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_148: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, add_31)
	        mul_149: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, rsqrt_16);  mul_147 = rsqrt_16 = None
	        sum_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_148, [1], True);  mul_148 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_46, mul_149);  add_46 = mul_149 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_289: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_200);  alias_200 = None
	        alias_290: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_289);  alias_289 = None
	        alias_291: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_290);  alias_290 = None
	        pow_27: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_291, 3);  alias_291 = None
	        mul_150: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_8, -0.5);  sum_8 = None
	        mul_151: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_150, pow_27);  mul_150 = pow_27 = None
	        expand_3: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_151, [-1, 384]);  mul_151 = None
	        div_3: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_3, 384);  expand_3 = None
	        pow_28: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_31, 1.0);  add_31 = None
	        mul_152: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_28, 2.0);  pow_28 = None
	        mul_153: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_3, mul_152);  div_3 = mul_152 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_49: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_48, mul_153);  add_48 = mul_153 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_49, primals_61);  primals_61 = None
	        permute_169: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_49, [1, 0])
	        mm_63: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_169, view_39);  permute_169 = view_39 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_60: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_62, [sym_size_int_3, 6, 64]);  mm_62 = None
	        alias_292: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_60);  view_60 = None
	        permute_170: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_292, [1, 0, 2]);  alias_292 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_293: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_170);  permute_170 = None
	        permute_171: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_293, [1, 0, 2]);  alias_293 = None
	        alias_294: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_171);  permute_171 = None
	        unsqueeze_42: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_294, 0);  alias_294 = None
	        alias_295: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_195);  alias_195 = None
	        alias_296: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_295);  alias_295 = None
	        _efficient_attention_backward_2 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_42, unsqueeze_27, unsqueeze_28, unsqueeze_29, None, alias_296, convert_element_type_18, convert_element_type_19, sym_size_int_4, sym_size_int_1, getitem_82, 0.0, getitem_83, getitem_84, 0, False);  unsqueeze_42 = unsqueeze_27 = unsqueeze_28 = unsqueeze_29 = alias_296 = convert_element_type_18 = convert_element_type_19 = getitem_82 = getitem_83 = getitem_84 = None
	        getitem_151: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[0]
	        getitem_152: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[1]
	        getitem_153: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[2];  _efficient_attention_backward_2 = None
	        squeeze_18: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_153, 0);  getitem_153 = None
	        squeeze_19: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_152, 0);  getitem_152 = None
	        squeeze_20: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_151, 0);  getitem_151 = None
	        alias_297: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_18);  squeeze_18 = None
	        permute_172: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_297, [1, 0, 2]);  alias_297 = None
	        alias_298: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_19);  squeeze_19 = None
	        permute_173: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_298, [1, 0, 2]);  alias_298 = None
	        alias_299: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_20);  squeeze_20 = None
	        permute_174: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_299, [1, 0, 2]);  alias_299 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_300: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_172);  permute_172 = None
	        permute_175: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_300, [1, 0, 2]);  alias_300 = None
	        view_61: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_175, [sym_size_int, 384]);  permute_175 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_301: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_173);  permute_173 = None
	        permute_176: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_301, [1, 0, 2]);  alias_301 = None
	        view_62: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_176, [sym_size_int, 384]);  permute_176 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_302: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_174);  permute_174 = None
	        permute_177: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_302, [1, 0, 2]);  alias_302 = None
	        view_63: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_177, [sym_size_int_3, 384]);  permute_177 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        full_28: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 768], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_27 = torch.ops.aten.split.Tensor(full_28, 384, 1)
	        getitem_155: "f32[s1, 384][768, 1]cuda:0" = split_27[0];  split_27 = None
	        alias_303: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_155);  getitem_155 = None
	        alias_304: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_62);  view_62 = None
	        copy_5: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_303, alias_304);  alias_303 = alias_304 = None
	        slice_scatter_5: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_28, copy_5, 1, 0, 384);  full_28 = copy_5 = None
	        alias_307: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_61);  view_61 = None
	        split_30 = torch.ops.aten.split.Tensor(slice_scatter_5, 384, 1)
	        getitem_162: "f32[s1, 384][768, 1]cuda:0" = split_30[1];  split_30 = None
	        alias_308: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_162);  getitem_162 = None
	        copy_6: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_308, alias_307);  alias_308 = alias_307 = None
	        slice_scatter_6: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_5, copy_6, 1, 384, 768);  slice_scatter_5 = copy_6 = None
	        mm_64: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_6, primals_60);  primals_60 = None
	        permute_179: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_6, [1, 0]);  slice_scatter_6 = None
	        mm_65: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_179, add_15);  permute_179 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        add_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mm_50, mm_64);  mm_50 = mm_64 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_63, primals_59);  primals_59 = None
	        permute_180: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_63, [1, 0]);  view_63 = None
	        mm_67: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_180, mul_81);  permute_180 = mul_81 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_30: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_21, torch.float32);  gt_21 = None
	        mul_154: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_30, 1.4285714285714286);  convert_element_type_30 = None
	        mul_155: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_66, mul_154);  mm_66 = mul_154 = None
	        clone_6: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_155, memory_format = torch.contiguous_format);  mul_155 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_156: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_6, mul_78);  mul_78 = None
	        mul_157: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_6, primals_58);  clone_6 = primals_58 = None
	        sum_9: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_156, [0], True);  mul_156 = None
	        unsqueeze_43: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_9, 0);  sum_9 = None
	        view_64: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_43, [384]);  unsqueeze_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_158: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_157, add_27)
	        mul_159: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_157, rsqrt_15);  mul_157 = rsqrt_15 = None
	        sum_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_158, [1], True);  mul_158 = None
	        alias_310: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_182);  alias_182 = None
	        alias_311: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_310);  alias_310 = None
	        alias_312: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_311);  alias_311 = None
	        pow_29: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_312, 3);  alias_312 = None
	        mul_160: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_10, -0.5);  sum_10 = None
	        mul_161: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_160, pow_29);  mul_160 = pow_29 = None
	        expand_4: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_161, [-1, 384]);  mul_161 = None
	        div_4: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_4, 384);  expand_4 = None
	        pow_30: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 1.0)
	        mul_162: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_30, 2.0);  pow_30 = None
	        mul_163: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_4, mul_162);  div_4 = mul_162 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_51: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_159, mul_163);  mul_159 = mul_163 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_51, add_49);  add_51 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_49, primals_57);  primals_57 = None
	        permute_181: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_49, [1, 0]);  add_49 = None
	        mm_69: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_181, view_35);  permute_181 = view_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_65: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_68, [sym_size_int_3, 6, 64]);  mm_68 = None
	        alias_313: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_65);  view_65 = None
	        permute_182: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_313, [1, 0, 2]);  alias_313 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_314: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_182);  permute_182 = None
	        permute_183: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_314, [1, 0, 2]);  alias_314 = None
	        alias_315: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_183);  permute_183 = None
	        unsqueeze_44: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_315, 0);  alias_315 = None
	        alias_316: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_177);  alias_177 = None
	        alias_317: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_316);  alias_316 = None
	        _efficient_attention_backward_3 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_44, unsqueeze_24, unsqueeze_25, unsqueeze_26, None, alias_317, convert_element_type_16, convert_element_type_17, sym_size_int_4, sym_size_int_4, getitem_74, 0.0, getitem_75, getitem_76, 1, False);  unsqueeze_44 = unsqueeze_24 = unsqueeze_25 = unsqueeze_26 = alias_317 = convert_element_type_16 = convert_element_type_17 = getitem_74 = getitem_75 = getitem_76 = None
	        getitem_167: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[0]
	        getitem_168: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[1]
	        getitem_169: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[2];  _efficient_attention_backward_3 = None
	        squeeze_21: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_169, 0);  getitem_169 = None
	        squeeze_22: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_168, 0);  getitem_168 = None
	        squeeze_23: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_167, 0);  getitem_167 = None
	        alias_318: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_21);  squeeze_21 = None
	        permute_184: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_318, [1, 0, 2]);  alias_318 = None
	        alias_319: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_22);  squeeze_22 = None
	        permute_185: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_319, [1, 0, 2]);  alias_319 = None
	        alias_320: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_23);  squeeze_23 = None
	        permute_186: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_320, [1, 0, 2]);  alias_320 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_321: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_184);  permute_184 = None
	        permute_187: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_321, [1, 0, 2]);  alias_321 = None
	        view_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_187, [sym_size_int_3, 384]);  permute_187 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_322: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_185);  permute_185 = None
	        permute_188: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_322, [1, 0, 2]);  alias_322 = None
	        view_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_188, [sym_size_int_3, 384]);  permute_188 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_323: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_186);  permute_186 = None
	        permute_189: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_323, [1, 0, 2]);  alias_323 = None
	        view_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_189, [sym_size_int_3, 384]);  permute_189 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_29: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_33 = torch.ops.aten.split.Tensor(full_29, 384, 1)
	        getitem_171: "f32[s6, 384][1152, 1]cuda:0" = split_33[0];  split_33 = None
	        alias_324: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_171);  getitem_171 = None
	        alias_325: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_68);  view_68 = None
	        copy_7: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_324, alias_325);  alias_324 = alias_325 = None
	        slice_scatter_7: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_29, copy_7, 1, 0, 384);  full_29 = copy_7 = None
	        alias_328: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_67);  view_67 = None
	        split_36 = torch.ops.aten.split.Tensor(slice_scatter_7, 384, 1)
	        getitem_181: "f32[s6, 384][1152, 1]cuda:0" = split_36[1];  split_36 = None
	        alias_329: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_181);  getitem_181 = None
	        copy_8: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_329, alias_328);  alias_329 = alias_328 = None
	        slice_scatter_8: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_7, copy_8, 1, 384, 768);  slice_scatter_7 = copy_8 = None
	        alias_332: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_66);  view_66 = None
	        split_39 = torch.ops.aten.split.Tensor(slice_scatter_8, 384, 1)
	        getitem_191: "f32[s6, 384][1152, 1]cuda:0" = split_39[2];  split_39 = None
	        alias_333: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_191);  getitem_191 = None
	        copy_9: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_333, alias_332);  alias_333 = alias_332 = None
	        slice_scatter_9: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_8, copy_9, 1, 768, 1152);  slice_scatter_8 = copy_9 = None
	        mm_70: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_9, primals_56);  primals_56 = None
	        permute_191: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_9, [1, 0]);  slice_scatter_9 = None
	        mm_71: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_191, mul_77);  permute_191 = mul_77 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_20, torch.float32);  gt_20 = None
	        mul_164: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_31, 1.4285714285714286);  convert_element_type_31 = None
	        mul_165: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_70, mul_164);  mm_70 = mul_164 = None
	        clone_7: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_165, memory_format = torch.contiguous_format);  mul_165 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_166: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_7, mul_74);  mul_74 = None
	        mul_167: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_7, primals_55);  clone_7 = primals_55 = None
	        sum_11: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_166, [0], True);  mul_166 = None
	        unsqueeze_45: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_11, 0);  sum_11 = None
	        view_69: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_45, [384]);  unsqueeze_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_168: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_167, add_27)
	        mul_169: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_167, rsqrt_14);  mul_167 = rsqrt_14 = None
	        sum_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_168, [1], True);  mul_168 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_53: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_52, mul_169);  add_52 = mul_169 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_335: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_164);  alias_164 = None
	        alias_336: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_335);  alias_335 = None
	        alias_337: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_336);  alias_336 = None
	        pow_31: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_337, 3);  alias_337 = None
	        mul_170: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_12, -0.5);  sum_12 = None
	        mul_171: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_170, pow_31);  mul_170 = pow_31 = None
	        expand_5: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_171, [-1, 384]);  mul_171 = None
	        div_5: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_5, 384);  expand_5 = None
	        pow_32: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 1.0);  add_27 = None
	        mul_172: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_32, 2.0);  pow_32 = None
	        mul_173: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_5, mul_172);  div_5 = mul_172 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_54: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_53, mul_173);  add_53 = mul_173 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_32: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_19, torch.float32);  gt_19 = None
	        mul_174: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_32, 1.4285714285714286);  convert_element_type_32 = None
	        mul_175: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_54, mul_174);  mul_174 = None
	        clone_8: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_175, memory_format = torch.contiguous_format);  mul_175 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_72: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_8, primals_54);  primals_54 = None
	        permute_192: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_8, [1, 0]);  clone_8 = None
	        mm_73: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_192, mul_71);  permute_192 = mul_71 = None
	        convert_element_type_33: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_18, torch.float32);  gt_18 = None
	        mul_176: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_33, 1.4285714285714286);  convert_element_type_33 = None
	        mul_177: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_72, mul_176);  mm_72 = mul_176 = None
	        clone_9: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_177, memory_format = torch.contiguous_format);  mul_177 = None
	        sigmoid_10: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_28)
	        full_30: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_2: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_30, sigmoid_10);  full_30 = None
	        mul_178: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_28, sub_2);  mm_28 = sub_2 = None
	        add_55: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_178, 1);  mul_178 = None
	        mul_179: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_10, add_55);  sigmoid_10 = add_55 = None
	        mul_180: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_9, mul_179);  clone_9 = mul_179 = None
	        mm_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_180, primals_53);  primals_53 = None
	        permute_194: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_180, [1, 0]);  mul_180 = None
	        mm_75: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_194, mul_68);  permute_194 = mul_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_181: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_74, mul_67);  mul_67 = None
	        mul_182: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_74, primals_52);  mm_74 = primals_52 = None
	        sum_13: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_181, [0], True);  mul_181 = None
	        unsqueeze_46: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_13, 0);  sum_13 = None
	        view_70: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_46, [384]);  unsqueeze_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_183: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_182, add_25)
	        mul_184: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_182, rsqrt_13);  mul_182 = rsqrt_13 = None
	        sum_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_183, [1], True);  mul_183 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_56: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_54, mul_184);  add_54 = mul_184 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_338: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_161);  alias_161 = None
	        alias_339: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_338);  alias_338 = None
	        alias_340: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_339);  alias_339 = None
	        pow_33: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_340, 3);  alias_340 = None
	        mul_185: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_14, -0.5);  sum_14 = None
	        mul_186: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_185, pow_33);  mul_185 = pow_33 = None
	        expand_6: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_186, [-1, 384]);  mul_186 = None
	        div_6: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_6, 384);  expand_6 = None
	        pow_34: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_25, 1.0);  add_25 = None
	        mul_187: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_34, 2.0);  pow_34 = None
	        mul_188: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_6, mul_187);  div_6 = mul_187 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_56, mul_188);  add_56 = mul_188 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_76: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_57, primals_51);  primals_51 = None
	        permute_195: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_57, [1, 0])
	        mm_77: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_195, view_31);  permute_195 = view_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_71: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_76, [sym_size_int_3, 6, 64]);  mm_76 = None
	        alias_341: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_71);  view_71 = None
	        permute_196: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_341, [1, 0, 2]);  alias_341 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_342: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_196);  permute_196 = None
	        permute_197: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_342, [1, 0, 2]);  alias_342 = None
	        alias_343: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_197);  permute_197 = None
	        unsqueeze_47: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_343, 0);  alias_343 = None
	        alias_344: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_156);  alias_156 = None
	        alias_345: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_344);  alias_344 = None
	        _efficient_attention_backward_4 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_47, unsqueeze_21, unsqueeze_22, unsqueeze_23, None, alias_345, convert_element_type_14, convert_element_type_15, sym_size_int_4, sym_size_int_1, getitem_65, 0.0, getitem_66, getitem_67, 0, False);  unsqueeze_47 = unsqueeze_21 = unsqueeze_22 = unsqueeze_23 = alias_345 = convert_element_type_14 = convert_element_type_15 = getitem_65 = getitem_66 = getitem_67 = None
	        getitem_198: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_4[0]
	        getitem_199: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_4[1]
	        getitem_200: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_4[2];  _efficient_attention_backward_4 = None
	        squeeze_24: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_200, 0);  getitem_200 = None
	        squeeze_25: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_199, 0);  getitem_199 = None
	        squeeze_26: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_198, 0);  getitem_198 = None
	        alias_346: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_24);  squeeze_24 = None
	        permute_198: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_346, [1, 0, 2]);  alias_346 = None
	        alias_347: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_25);  squeeze_25 = None
	        permute_199: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_347, [1, 0, 2]);  alias_347 = None
	        alias_348: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_26);  squeeze_26 = None
	        permute_200: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_348, [1, 0, 2]);  alias_348 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_349: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_198);  permute_198 = None
	        permute_201: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_349, [1, 0, 2]);  alias_349 = None
	        view_72: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_201, [sym_size_int, 384]);  permute_201 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_350: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_199);  permute_199 = None
	        permute_202: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_350, [1, 0, 2]);  alias_350 = None
	        view_73: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_202, [sym_size_int, 384]);  permute_202 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_351: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_200);  permute_200 = None
	        permute_203: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_351, [1, 0, 2]);  alias_351 = None
	        view_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_203, [sym_size_int_3, 384]);  permute_203 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        full_31: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 768], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_42 = torch.ops.aten.split.Tensor(full_31, 384, 1)
	        getitem_202: "f32[s1, 384][768, 1]cuda:0" = split_42[0];  split_42 = None
	        alias_352: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_202);  getitem_202 = None
	        alias_353: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_73);  view_73 = None
	        copy_10: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_352, alias_353);  alias_352 = alias_353 = None
	        slice_scatter_10: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_31, copy_10, 1, 0, 384);  full_31 = copy_10 = None
	        alias_356: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_72);  view_72 = None
	        split_45 = torch.ops.aten.split.Tensor(slice_scatter_10, 384, 1)
	        getitem_209: "f32[s1, 384][768, 1]cuda:0" = split_45[1];  split_45 = None
	        alias_357: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_209);  getitem_209 = None
	        copy_11: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_357, alias_356);  alias_357 = alias_356 = None
	        slice_scatter_11: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, copy_11, 1, 384, 768);  slice_scatter_10 = copy_11 = None
	        mm_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_11, primals_50);  primals_50 = None
	        permute_205: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_11, [1, 0]);  slice_scatter_11 = None
	        mm_79: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_205, add_15);  permute_205 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        add_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_50, mm_78);  add_50 = mm_78 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_80: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_74, primals_49);  primals_49 = None
	        permute_206: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_74, [1, 0]);  view_74 = None
	        mm_81: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_206, mul_66);  permute_206 = mul_66 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_34: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_17, torch.float32);  gt_17 = None
	        mul_189: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_34, 1.4285714285714286);  convert_element_type_34 = None
	        mul_190: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_80, mul_189);  mm_80 = mul_189 = None
	        clone_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_190, memory_format = torch.contiguous_format);  mul_190 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_191: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_10, mul_63);  mul_63 = None
	        mul_192: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_10, primals_48);  clone_10 = primals_48 = None
	        sum_15: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_191, [0], True);  mul_191 = None
	        unsqueeze_48: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_15, 0);  sum_15 = None
	        view_75: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_48, [384]);  unsqueeze_48 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_193: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_192, add_21)
	        mul_194: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_192, rsqrt_12);  mul_192 = rsqrt_12 = None
	        sum_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_193, [1], True);  mul_193 = None
	        alias_359: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_143);  alias_143 = None
	        alias_360: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_359);  alias_359 = None
	        alias_361: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_360);  alias_360 = None
	        pow_35: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_361, 3);  alias_361 = None
	        mul_195: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_16, -0.5);  sum_16 = None
	        mul_196: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_195, pow_35);  mul_195 = pow_35 = None
	        expand_7: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_196, [-1, 384]);  mul_196 = None
	        div_7: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_7, 384);  expand_7 = None
	        pow_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 1.0)
	        mul_197: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_36, 2.0);  pow_36 = None
	        mul_198: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_7, mul_197);  div_7 = mul_197 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_59: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_194, mul_198);  mul_194 = mul_198 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_59, add_57);  add_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_82: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_57, primals_47);  primals_47 = None
	        permute_207: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_57, [1, 0]);  add_57 = None
	        mm_83: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_207, view_27);  permute_207 = view_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_76: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_82, [sym_size_int_3, 6, 64]);  mm_82 = None
	        alias_362: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_76);  view_76 = None
	        permute_208: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_362, [1, 0, 2]);  alias_362 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_363: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_208);  permute_208 = None
	        permute_209: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_363, [1, 0, 2]);  alias_363 = None
	        alias_364: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_209);  permute_209 = None
	        unsqueeze_49: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_364, 0);  alias_364 = None
	        alias_365: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_138);  alias_138 = None
	        alias_366: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_365);  alias_365 = None
	        _efficient_attention_backward_5 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_49, unsqueeze_18, unsqueeze_19, unsqueeze_20, None, alias_366, convert_element_type_12, convert_element_type_13, sym_size_int_4, sym_size_int_4, getitem_57, 0.0, getitem_58, getitem_59, 1, False);  unsqueeze_49 = unsqueeze_18 = unsqueeze_19 = unsqueeze_20 = alias_366 = convert_element_type_12 = convert_element_type_13 = getitem_57 = getitem_58 = getitem_59 = None
	        getitem_214: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_5[0]
	        getitem_215: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_5[1]
	        getitem_216: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_5[2];  _efficient_attention_backward_5 = None
	        squeeze_27: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_216, 0);  getitem_216 = None
	        squeeze_28: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_215, 0);  getitem_215 = None
	        squeeze_29: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_214, 0);  getitem_214 = None
	        alias_367: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_27);  squeeze_27 = None
	        permute_210: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_367, [1, 0, 2]);  alias_367 = None
	        alias_368: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_28);  squeeze_28 = None
	        permute_211: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_368, [1, 0, 2]);  alias_368 = None
	        alias_369: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_29);  squeeze_29 = None
	        permute_212: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_369, [1, 0, 2]);  alias_369 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_370: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_210);  permute_210 = None
	        permute_213: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_370, [1, 0, 2]);  alias_370 = None
	        view_77: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_213, [sym_size_int_3, 384]);  permute_213 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_371: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_211);  permute_211 = None
	        permute_214: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_371, [1, 0, 2]);  alias_371 = None
	        view_78: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_214, [sym_size_int_3, 384]);  permute_214 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_372: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_212);  permute_212 = None
	        permute_215: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_372, [1, 0, 2]);  alias_372 = None
	        view_79: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_215, [sym_size_int_3, 384]);  permute_215 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_32: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_48 = torch.ops.aten.split.Tensor(full_32, 384, 1)
	        getitem_218: "f32[s6, 384][1152, 1]cuda:0" = split_48[0];  split_48 = None
	        alias_373: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_218);  getitem_218 = None
	        alias_374: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_79);  view_79 = None
	        copy_12: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_373, alias_374);  alias_373 = alias_374 = None
	        slice_scatter_12: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_32, copy_12, 1, 0, 384);  full_32 = copy_12 = None
	        alias_377: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_78);  view_78 = None
	        split_51 = torch.ops.aten.split.Tensor(slice_scatter_12, 384, 1)
	        getitem_228: "f32[s6, 384][1152, 1]cuda:0" = split_51[1];  split_51 = None
	        alias_378: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_228);  getitem_228 = None
	        copy_13: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_378, alias_377);  alias_378 = alias_377 = None
	        slice_scatter_13: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_12, copy_13, 1, 384, 768);  slice_scatter_12 = copy_13 = None
	        alias_381: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_77);  view_77 = None
	        split_54 = torch.ops.aten.split.Tensor(slice_scatter_13, 384, 1)
	        getitem_238: "f32[s6, 384][1152, 1]cuda:0" = split_54[2];  split_54 = None
	        alias_382: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_238);  getitem_238 = None
	        copy_14: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_382, alias_381);  alias_382 = alias_381 = None
	        slice_scatter_14: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_13, copy_14, 1, 768, 1152);  slice_scatter_13 = copy_14 = None
	        mm_84: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_14, primals_46);  primals_46 = None
	        permute_217: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_14, [1, 0]);  slice_scatter_14 = None
	        mm_85: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_217, mul_62);  permute_217 = mul_62 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_16, torch.float32);  gt_16 = None
	        mul_199: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_35, 1.4285714285714286);  convert_element_type_35 = None
	        mul_200: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_84, mul_199);  mm_84 = mul_199 = None
	        clone_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_200, memory_format = torch.contiguous_format);  mul_200 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_201: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_11, mul_59);  mul_59 = None
	        mul_202: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_11, primals_45);  clone_11 = primals_45 = None
	        sum_17: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_201, [0], True);  mul_201 = None
	        unsqueeze_50: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_17, 0);  sum_17 = None
	        view_80: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_50, [384]);  unsqueeze_50 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_203: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_202, add_21)
	        mul_204: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_202, rsqrt_11);  mul_202 = rsqrt_11 = None
	        sum_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_203, [1], True);  mul_203 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_61: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_60, mul_204);  add_60 = mul_204 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_384: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_125);  alias_125 = None
	        alias_385: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_384);  alias_384 = None
	        alias_386: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_385);  alias_385 = None
	        pow_37: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_386, 3);  alias_386 = None
	        mul_205: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_18, -0.5);  sum_18 = None
	        mul_206: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_205, pow_37);  mul_205 = pow_37 = None
	        expand_8: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_206, [-1, 384]);  mul_206 = None
	        div_8: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_8, 384);  expand_8 = None
	        pow_38: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 1.0);  add_21 = None
	        mul_207: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_38, 2.0);  pow_38 = None
	        mul_208: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_8, mul_207);  div_8 = mul_207 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_61, mul_208);  add_61 = mul_208 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_15, torch.float32);  gt_15 = None
	        mul_209: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_36, 1.4285714285714286);  convert_element_type_36 = None
	        mul_210: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_62, mul_209);  mul_209 = None
	        clone_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_210, memory_format = torch.contiguous_format);  mul_210 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_86: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_12, primals_44);  primals_44 = None
	        permute_218: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_12, [1, 0]);  clone_12 = None
	        mm_87: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_218, mul_56);  permute_218 = mul_56 = None
	        convert_element_type_37: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_14, torch.float32);  gt_14 = None
	        mul_211: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_37, 1.4285714285714286);  convert_element_type_37 = None
	        mul_212: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_86, mul_211);  mm_86 = mul_211 = None
	        clone_13: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_212, memory_format = torch.contiguous_format);  mul_212 = None
	        sigmoid_11: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_21)
	        full_33: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_3: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_33, sigmoid_11);  full_33 = None
	        mul_213: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_21, sub_3);  mm_21 = sub_3 = None
	        add_63: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_213, 1);  mul_213 = None
	        mul_214: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_11, add_63);  sigmoid_11 = add_63 = None
	        mul_215: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_13, mul_214);  clone_13 = mul_214 = None
	        mm_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_215, primals_43);  primals_43 = None
	        permute_220: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_215, [1, 0]);  mul_215 = None
	        mm_89: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_220, mul_53);  permute_220 = mul_53 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_216: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_88, mul_52);  mul_52 = None
	        mul_217: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_88, primals_42);  mm_88 = primals_42 = None
	        sum_19: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_216, [0], True);  mul_216 = None
	        unsqueeze_51: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_19, 0);  sum_19 = None
	        view_81: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_51, [384]);  unsqueeze_51 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_218: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_217, add_19)
	        mul_219: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_217, rsqrt_10);  mul_217 = rsqrt_10 = None
	        sum_20: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_218, [1], True);  mul_218 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_64: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_62, mul_219);  add_62 = mul_219 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_387: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_122);  alias_122 = None
	        alias_388: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_387);  alias_387 = None
	        alias_389: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_388);  alias_388 = None
	        pow_39: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_389, 3);  alias_389 = None
	        mul_220: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_20, -0.5);  sum_20 = None
	        mul_221: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_220, pow_39);  mul_220 = pow_39 = None
	        expand_9: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_221, [-1, 384]);  mul_221 = None
	        div_9: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_9, 384);  expand_9 = None
	        pow_40: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_19, 1.0);  add_19 = None
	        mul_222: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_40, 2.0);  pow_40 = None
	        mul_223: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_9, mul_222);  div_9 = mul_222 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_65: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_64, mul_223);  add_64 = mul_223 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_65, primals_41);  primals_41 = None
	        permute_221: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_65, [1, 0])
	        mm_91: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_221, view_23);  permute_221 = view_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_82: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_90, [sym_size_int_3, 6, 64]);  mm_90 = None
	        alias_390: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_82);  view_82 = None
	        permute_222: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_390, [1, 0, 2]);  alias_390 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_391: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_222);  permute_222 = None
	        permute_223: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_391, [1, 0, 2]);  alias_391 = None
	        alias_392: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_223);  permute_223 = None
	        unsqueeze_52: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_392, 0);  alias_392 = None
	        alias_393: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_117);  alias_117 = None
	        alias_394: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_393);  alias_393 = None
	        _efficient_attention_backward_6 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_52, unsqueeze_15, unsqueeze_16, unsqueeze_17, None, alias_394, convert_element_type_10, convert_element_type_11, sym_size_int_4, sym_size_int_1, getitem_48, 0.0, getitem_49, getitem_50, 0, False);  unsqueeze_52 = unsqueeze_15 = unsqueeze_16 = unsqueeze_17 = alias_394 = convert_element_type_10 = convert_element_type_11 = getitem_48 = getitem_49 = getitem_50 = None
	        getitem_245: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_6[0]
	        getitem_246: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_6[1]
	        getitem_247: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_6[2];  _efficient_attention_backward_6 = None
	        squeeze_30: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_247, 0);  getitem_247 = None
	        squeeze_31: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_246, 0);  getitem_246 = None
	        squeeze_32: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_245, 0);  getitem_245 = None
	        alias_395: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_30);  squeeze_30 = None
	        permute_224: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_395, [1, 0, 2]);  alias_395 = None
	        alias_396: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_31);  squeeze_31 = None
	        permute_225: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_396, [1, 0, 2]);  alias_396 = None
	        alias_397: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_32);  squeeze_32 = None
	        permute_226: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_397, [1, 0, 2]);  alias_397 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_398: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_224);  permute_224 = None
	        permute_227: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_398, [1, 0, 2]);  alias_398 = None
	        view_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_227, [sym_size_int, 384]);  permute_227 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_399: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_225);  permute_225 = None
	        permute_228: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_399, [1, 0, 2]);  alias_399 = None
	        view_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_228, [sym_size_int, 384]);  permute_228 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_400: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_226);  permute_226 = None
	        permute_229: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_400, [1, 0, 2]);  alias_400 = None
	        view_85: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_229, [sym_size_int_3, 384]);  permute_229 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        full_34: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 768], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_57 = torch.ops.aten.split.Tensor(full_34, 384, 1)
	        getitem_249: "f32[s1, 384][768, 1]cuda:0" = split_57[0];  split_57 = None
	        alias_401: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_249);  getitem_249 = None
	        alias_402: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_84);  view_84 = None
	        copy_15: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_401, alias_402);  alias_401 = alias_402 = None
	        slice_scatter_15: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_34, copy_15, 1, 0, 384);  full_34 = copy_15 = None
	        alias_405: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_83);  view_83 = None
	        split_60 = torch.ops.aten.split.Tensor(slice_scatter_15, 384, 1)
	        getitem_256: "f32[s1, 384][768, 1]cuda:0" = split_60[1];  split_60 = None
	        alias_406: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.alias.default(getitem_256);  getitem_256 = None
	        copy_16: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(alias_406, alias_405);  alias_406 = alias_405 = None
	        slice_scatter_16: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_15, copy_16, 1, 384, 768);  slice_scatter_15 = copy_16 = None
	        mm_92: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_16, primals_40);  primals_40 = None
	        permute_231: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_16, [1, 0]);  slice_scatter_16 = None
	        mm_93: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_231, add_15);  permute_231 = add_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        add_66: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_58, mm_92);  add_58 = mm_92 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_94: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_85, primals_39);  primals_39 = None
	        permute_232: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_85, [1, 0]);  view_85 = None
	        mm_95: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_232, mul_51);  permute_232 = mul_51 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_38: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_13, torch.float32);  gt_13 = None
	        mul_224: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_38, 1.4285714285714286);  convert_element_type_38 = None
	        mul_225: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_94, mul_224);  mm_94 = mul_224 = None
	        clone_14: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_225, memory_format = torch.contiguous_format);  mul_225 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_226: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_14, mul_48);  mul_48 = None
	        mul_227: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_14, primals_38);  clone_14 = primals_38 = None
	        sum_21: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_226, [0], True);  mul_226 = None
	        unsqueeze_53: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_21, 0);  sum_21 = None
	        view_86: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_53, [384]);  unsqueeze_53 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_228: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_227, primals_30)
	        mul_229: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_227, rsqrt_9);  mul_227 = rsqrt_9 = None
	        sum_22: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_228, [1], True);  mul_228 = None
	        alias_408: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_104);  alias_104 = None
	        alias_409: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_408);  alias_408 = None
	        alias_410: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_409);  alias_409 = None
	        pow_41: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_410, 3);  alias_410 = None
	        mul_230: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_22, -0.5);  sum_22 = None
	        mul_231: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_230, pow_41);  mul_230 = pow_41 = None
	        expand_10: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_231, [-1, 384]);  mul_231 = None
	        div_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_10, 384);  expand_10 = None
	        pow_42: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 1.0)
	        mul_232: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_42, 2.0);  pow_42 = None
	        mul_233: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_10, mul_232);  div_10 = mul_232 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_229, mul_233);  mul_229 = mul_233 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_67, add_65);  add_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_96: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_65, primals_37);  primals_37 = None
	        permute_233: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_65, [1, 0]);  add_65 = None
	        mm_97: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_233, view_19);  permute_233 = view_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_87: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_96, [sym_size_int_3, 6, 64]);  mm_96 = None
	        alias_411: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_87);  view_87 = None
	        permute_234: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_411, [1, 0, 2]);  alias_411 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_412: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_234);  permute_234 = None
	        permute_235: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_412, [1, 0, 2]);  alias_412 = None
	        alias_413: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_235);  permute_235 = None
	        unsqueeze_54: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_413, 0);  alias_413 = None
	        alias_414: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_99);  alias_99 = None
	        alias_415: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_414);  alias_414 = None
	        _efficient_attention_backward_7 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_54, unsqueeze_12, unsqueeze_13, unsqueeze_14, None, alias_415, convert_element_type_8, convert_element_type_9, sym_size_int_4, sym_size_int_4, getitem_40, 0.0, getitem_41, getitem_42, 1, False);  unsqueeze_54 = unsqueeze_12 = unsqueeze_13 = unsqueeze_14 = alias_415 = convert_element_type_8 = convert_element_type_9 = sym_size_int_4 = getitem_40 = getitem_41 = getitem_42 = None
	        getitem_261: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_7[0]
	        getitem_262: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_7[1]
	        getitem_263: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_7[2];  _efficient_attention_backward_7 = None
	        squeeze_33: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_263, 0);  getitem_263 = None
	        squeeze_34: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_262, 0);  getitem_262 = None
	        squeeze_35: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_261, 0);  getitem_261 = None
	        alias_416: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_33);  squeeze_33 = None
	        permute_236: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_416, [1, 0, 2]);  alias_416 = None
	        alias_417: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_34);  squeeze_34 = None
	        permute_237: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_417, [1, 0, 2]);  alias_417 = None
	        alias_418: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_35);  squeeze_35 = None
	        permute_238: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_418, [1, 0, 2]);  alias_418 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_419: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_236);  permute_236 = None
	        permute_239: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_419, [1, 0, 2]);  alias_419 = None
	        view_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_239, [sym_size_int_3, 384]);  permute_239 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_420: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_237);  permute_237 = None
	        permute_240: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_420, [1, 0, 2]);  alias_420 = None
	        view_89: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_240, [sym_size_int_3, 384]);  permute_240 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_421: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_238);  permute_238 = None
	        permute_241: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_421, [1, 0, 2]);  alias_421 = None
	        view_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_241, [sym_size_int_3, 384]);  permute_241 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_35: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_3 = None
	        split_63 = torch.ops.aten.split.Tensor(full_35, 384, 1)
	        getitem_265: "f32[s6, 384][1152, 1]cuda:0" = split_63[0];  split_63 = None
	        alias_422: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_265);  getitem_265 = None
	        alias_423: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_90);  view_90 = None
	        copy_17: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_422, alias_423);  alias_422 = alias_423 = None
	        slice_scatter_17: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_35, copy_17, 1, 0, 384);  full_35 = copy_17 = None
	        alias_426: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_89);  view_89 = None
	        split_66 = torch.ops.aten.split.Tensor(slice_scatter_17, 384, 1)
	        getitem_275: "f32[s6, 384][1152, 1]cuda:0" = split_66[1];  split_66 = None
	        alias_427: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_275);  getitem_275 = None
	        copy_18: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_427, alias_426);  alias_427 = alias_426 = None
	        slice_scatter_18: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_17, copy_18, 1, 384, 768);  slice_scatter_17 = copy_18 = None
	        alias_430: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_88);  view_88 = None
	        split_69 = torch.ops.aten.split.Tensor(slice_scatter_18, 384, 1)
	        getitem_285: "f32[s6, 384][1152, 1]cuda:0" = split_69[2];  split_69 = None
	        alias_431: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_285);  getitem_285 = None
	        copy_19: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_431, alias_430);  alias_431 = alias_430 = None
	        slice_scatter_19: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_18, copy_19, 1, 768, 1152);  slice_scatter_18 = copy_19 = None
	        mm_98: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_19, primals_36);  primals_36 = None
	        permute_243: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_19, [1, 0]);  slice_scatter_19 = None
	        mm_99: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_243, mul_47);  permute_243 = mul_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_12, torch.float32);  gt_12 = None
	        mul_234: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_39, 1.4285714285714286);  convert_element_type_39 = None
	        mul_235: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_98, mul_234);  mm_98 = mul_234 = None
	        clone_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_235, memory_format = torch.contiguous_format);  mul_235 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_236: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_15, mul_44);  mul_44 = None
	        mul_237: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_15, primals_35);  clone_15 = primals_35 = None
	        sum_23: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_236, [0], True);  mul_236 = None
	        unsqueeze_55: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_23, 0);  sum_23 = None
	        view_91: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_55, [384]);  unsqueeze_55 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_238: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_237, primals_30)
	        mul_239: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_237, rsqrt_8);  mul_237 = rsqrt_8 = None
	        sum_24: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_238, [1], True);  mul_238 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_69: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_68, mul_239);  add_68 = mul_239 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_433: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_86);  alias_86 = None
	        alias_434: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_433);  alias_433 = None
	        alias_435: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_434);  alias_434 = None
	        pow_43: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_435, 3);  alias_435 = None
	        mul_240: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_24, -0.5);  sum_24 = None
	        mul_241: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_240, pow_43);  mul_240 = pow_43 = None
	        expand_11: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_241, [-1, 384]);  mul_241 = None
	        div_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_11, 384);  expand_11 = None
	        pow_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 1.0);  primals_30 = None
	        mul_242: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_44, 2.0);  pow_44 = None
	        mul_243: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_11, mul_242);  div_11 = mul_242 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_70: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_69, mul_243);  add_69 = mul_243 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_40: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_11, torch.float32);  gt_11 = None
	        mul_244: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_40, 1.4285714285714286);  convert_element_type_40 = None
	        mul_245: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_66, mul_244);  mul_244 = None
	        clone_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_245, memory_format = torch.contiguous_format);  mul_245 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_100: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_16, primals_29);  primals_29 = None
	        permute_244: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_16, [1, 0]);  clone_16 = None
	        mm_101: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_244, mul_41);  permute_244 = mul_41 = None
	        convert_element_type_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_10, torch.float32);  gt_10 = None
	        mul_246: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_41, 1.4285714285714286);  convert_element_type_41 = None
	        mul_247: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_100, mul_246);  mm_100 = mul_246 = None
	        clone_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_247, memory_format = torch.contiguous_format);  mul_247 = None
	        sigmoid_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        full_36: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_36, sigmoid_12);  full_36 = None
	        mul_248: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sub_4);  mm_14 = sub_4 = None
	        add_71: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_248, 1);  mul_248 = None
	        mul_249: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_12, add_71);  sigmoid_12 = add_71 = None
	        mul_250: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_17, mul_249);  clone_17 = mul_249 = None
	        mm_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_250, primals_28);  primals_28 = None
	        permute_246: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_250, [1, 0]);  mul_250 = None
	        mm_103: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_246, mul_38);  permute_246 = mul_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_251: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_102, mul_37);  mul_37 = None
	        mul_252: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_102, primals_27);  mm_102 = primals_27 = None
	        sum_25: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_251, [0], True);  mul_251 = None
	        unsqueeze_56: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_25, 0);  sum_25 = None
	        view_92: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_56, [384]);  unsqueeze_56 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_253: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_252, add_13)
	        mul_254: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_252, rsqrt_7);  mul_252 = rsqrt_7 = None
	        sum_26: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_253, [1], True);  mul_253 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_72: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_66, mul_254);  add_66 = mul_254 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_436: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_83);  alias_83 = None
	        alias_437: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_436);  alias_436 = None
	        alias_438: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_437);  alias_437 = None
	        pow_45: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_438, 3);  alias_438 = None
	        mul_255: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_26, -0.5);  sum_26 = None
	        mul_256: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_255, pow_45);  mul_255 = pow_45 = None
	        expand_12: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_256, [-1, 384]);  mul_256 = None
	        div_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_12, 384);  expand_12 = None
	        pow_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 1.0);  add_13 = None
	        mul_257: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_46, 2.0);  pow_46 = None
	        mul_258: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_12, mul_257);  div_12 = mul_257 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_73: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_72, mul_258);  add_72 = mul_258 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_104: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_73, primals_26);  primals_26 = None
	        permute_247: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_73, [1, 0])
	        mm_105: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_247, view_15);  permute_247 = view_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_93: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_104, [sym_size_int, 6, 64]);  mm_104 = None
	        alias_439: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_93);  view_93 = None
	        permute_248: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_439, [1, 0, 2]);  alias_439 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_440: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_248);  permute_248 = None
	        permute_249: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_440, [1, 0, 2]);  alias_440 = None
	        alias_441: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_249);  permute_249 = None
	        unsqueeze_57: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_441, 0);  alias_441 = None
	        alias_442: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_78);  alias_78 = None
	        alias_443: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_442);  alias_442 = None
	        _efficient_attention_backward_8 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_57, unsqueeze_9, unsqueeze_10, unsqueeze_11, None, alias_443, convert_element_type_6, convert_element_type_7, sym_size_int_1, sym_size_int_1, getitem_31, 0.0, getitem_32, getitem_33, 0, False);  unsqueeze_57 = unsqueeze_9 = unsqueeze_10 = unsqueeze_11 = alias_443 = convert_element_type_6 = convert_element_type_7 = getitem_31 = getitem_32 = getitem_33 = None
	        getitem_292: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_8[0]
	        getitem_293: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_8[1]
	        getitem_294: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_8[2];  _efficient_attention_backward_8 = None
	        squeeze_36: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_294, 0);  getitem_294 = None
	        squeeze_37: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_293, 0);  getitem_293 = None
	        squeeze_38: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_292, 0);  getitem_292 = None
	        alias_444: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_36);  squeeze_36 = None
	        permute_250: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_444, [1, 0, 2]);  alias_444 = None
	        alias_445: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_37);  squeeze_37 = None
	        permute_251: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_445, [1, 0, 2]);  alias_445 = None
	        alias_446: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_38);  squeeze_38 = None
	        permute_252: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_446, [1, 0, 2]);  alias_446 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_447: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_250);  permute_250 = None
	        permute_253: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_447, [1, 0, 2]);  alias_447 = None
	        view_94: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_253, [sym_size_int, 384]);  permute_253 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_448: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_251);  permute_251 = None
	        permute_254: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_448, [1, 0, 2]);  alias_448 = None
	        view_95: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_254, [sym_size_int, 384]);  permute_254 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_449: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_252);  permute_252 = None
	        permute_255: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_449, [1, 0, 2]);  alias_449 = None
	        view_96: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_255, [sym_size_int, 384]);  permute_255 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_37: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_72 = torch.ops.aten.split.Tensor(full_37, 384, 1)
	        getitem_296: "f32[s1, 384][1152, 1]cuda:0" = split_72[0];  split_72 = None
	        alias_450: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_296);  getitem_296 = None
	        alias_451: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_96);  view_96 = None
	        copy_20: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_450, alias_451);  alias_450 = alias_451 = None
	        slice_scatter_20: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_37, copy_20, 1, 0, 384);  full_37 = copy_20 = None
	        alias_454: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_95);  view_95 = None
	        split_75 = torch.ops.aten.split.Tensor(slice_scatter_20, 384, 1)
	        getitem_306: "f32[s1, 384][1152, 1]cuda:0" = split_75[1];  split_75 = None
	        alias_455: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_306);  getitem_306 = None
	        copy_21: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_455, alias_454);  alias_455 = alias_454 = None
	        slice_scatter_21: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_20, copy_21, 1, 384, 768);  slice_scatter_20 = copy_21 = None
	        alias_458: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_94);  view_94 = None
	        split_78 = torch.ops.aten.split.Tensor(slice_scatter_21, 384, 1)
	        getitem_316: "f32[s1, 384][1152, 1]cuda:0" = split_78[2];  split_78 = None
	        alias_459: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_316);  getitem_316 = None
	        copy_22: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_459, alias_458);  alias_459 = alias_458 = None
	        slice_scatter_22: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_21, copy_22, 1, 768, 1152);  slice_scatter_21 = copy_22 = None
	        mm_106: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_22, primals_25);  primals_25 = None
	        permute_257: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_22, [1, 0]);  slice_scatter_22 = None
	        mm_107: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_257, mul_36);  permute_257 = mul_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_9, torch.float32);  gt_9 = None
	        mul_259: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_42, 1.4285714285714286);  convert_element_type_42 = None
	        mul_260: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_106, mul_259);  mm_106 = mul_259 = None
	        clone_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_260, memory_format = torch.contiguous_format);  mul_260 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_261: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_18, mul_33);  mul_33 = None
	        mul_262: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_18, primals_24);  clone_18 = primals_24 = None
	        sum_27: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_261, [0], True);  mul_261 = None
	        unsqueeze_58: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_27, 0);  sum_27 = None
	        view_97: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_58, [384]);  unsqueeze_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_263: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_262, add_11)
	        mul_264: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_262, rsqrt_6);  mul_262 = rsqrt_6 = None
	        sum_28: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_263, [1], True);  mul_263 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_74: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_73, mul_264);  add_73 = mul_264 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_461: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_65);  alias_65 = None
	        alias_462: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_461);  alias_461 = None
	        alias_463: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_462);  alias_462 = None
	        pow_47: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_463, 3);  alias_463 = None
	        mul_265: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_28, -0.5);  sum_28 = None
	        mul_266: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_265, pow_47);  mul_265 = pow_47 = None
	        expand_13: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_266, [-1, 384]);  mul_266 = None
	        div_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_13, 384);  expand_13 = None
	        pow_48: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 1.0);  add_11 = None
	        mul_267: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_48, 2.0);  pow_48 = None
	        mul_268: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_13, mul_267);  div_13 = mul_267 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_75: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_74, mul_268);  add_74 = mul_268 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_8, torch.float32);  gt_8 = None
	        mul_269: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_43, 1.4285714285714286);  convert_element_type_43 = None
	        mul_270: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_75, mul_269);  mul_269 = None
	        clone_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_270, memory_format = torch.contiguous_format);  mul_270 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_108: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_19, primals_23);  primals_23 = None
	        permute_258: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_19, [1, 0]);  clone_19 = None
	        mm_109: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_258, mul_30);  permute_258 = mul_30 = None
	        convert_element_type_44: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_7, torch.float32);  gt_7 = None
	        mul_271: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_44, 1.4285714285714286);  convert_element_type_44 = None
	        mul_272: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_108, mul_271);  mm_108 = mul_271 = None
	        clone_20: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_272, memory_format = torch.contiguous_format);  mul_272 = None
	        sigmoid_13: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        full_38: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_38, sigmoid_13);  full_38 = None
	        mul_273: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sub_5);  mm_10 = sub_5 = None
	        add_76: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_273, 1);  mul_273 = None
	        mul_274: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_13, add_76);  sigmoid_13 = add_76 = None
	        mul_275: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_20, mul_274);  clone_20 = mul_274 = None
	        mm_110: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_275, primals_22);  primals_22 = None
	        permute_260: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_275, [1, 0]);  mul_275 = None
	        mm_111: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_260, mul_27);  permute_260 = mul_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_276: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_110, mul_26);  mul_26 = None
	        mul_277: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_110, primals_21);  mm_110 = primals_21 = None
	        sum_29: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_276, [0], True);  mul_276 = None
	        unsqueeze_59: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_29, 0);  sum_29 = None
	        view_98: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_59, [384]);  unsqueeze_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_278: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_277, add_9)
	        mul_279: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_277, rsqrt_5);  mul_277 = rsqrt_5 = None
	        sum_30: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_278, [1], True);  mul_278 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_77: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_75, mul_279);  add_75 = mul_279 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_464: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_62);  alias_62 = None
	        alias_465: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_464);  alias_464 = None
	        alias_466: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_465);  alias_465 = None
	        pow_49: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_466, 3);  alias_466 = None
	        mul_280: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_30, -0.5);  sum_30 = None
	        mul_281: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_280, pow_49);  mul_280 = pow_49 = None
	        expand_14: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_281, [-1, 384]);  mul_281 = None
	        div_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_14, 384);  expand_14 = None
	        pow_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 1.0);  add_9 = None
	        mul_282: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_50, 2.0);  pow_50 = None
	        mul_283: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_14, mul_282);  div_14 = mul_282 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_77, mul_283);  add_77 = mul_283 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_78, primals_20);  primals_20 = None
	        permute_261: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_78, [1, 0])
	        mm_113: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_261, view_11);  permute_261 = view_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_99: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_112, [sym_size_int, 6, 64]);  mm_112 = None
	        alias_467: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_99);  view_99 = None
	        permute_262: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_467, [1, 0, 2]);  alias_467 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_468: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_262);  permute_262 = None
	        permute_263: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_468, [1, 0, 2]);  alias_468 = None
	        alias_469: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_263);  permute_263 = None
	        unsqueeze_60: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_469, 0);  alias_469 = None
	        alias_470: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_57);  alias_57 = None
	        alias_471: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_470);  alias_470 = None
	        _efficient_attention_backward_9 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_60, unsqueeze_6, unsqueeze_7, unsqueeze_8, None, alias_471, convert_element_type_4, convert_element_type_5, sym_size_int_1, sym_size_int_1, getitem_22, 0.0, getitem_23, getitem_24, 0, False);  unsqueeze_60 = unsqueeze_6 = unsqueeze_7 = unsqueeze_8 = alias_471 = convert_element_type_4 = convert_element_type_5 = getitem_22 = getitem_23 = getitem_24 = None
	        getitem_323: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_9[0]
	        getitem_324: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_9[1]
	        getitem_325: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_9[2];  _efficient_attention_backward_9 = None
	        squeeze_39: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_325, 0);  getitem_325 = None
	        squeeze_40: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_324, 0);  getitem_324 = None
	        squeeze_41: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_323, 0);  getitem_323 = None
	        alias_472: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_39);  squeeze_39 = None
	        permute_264: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_472, [1, 0, 2]);  alias_472 = None
	        alias_473: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_40);  squeeze_40 = None
	        permute_265: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_473, [1, 0, 2]);  alias_473 = None
	        alias_474: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_41);  squeeze_41 = None
	        permute_266: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_474, [1, 0, 2]);  alias_474 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_475: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_264);  permute_264 = None
	        permute_267: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_475, [1, 0, 2]);  alias_475 = None
	        view_100: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_267, [sym_size_int, 384]);  permute_267 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_476: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_265);  permute_265 = None
	        permute_268: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_476, [1, 0, 2]);  alias_476 = None
	        view_101: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_268, [sym_size_int, 384]);  permute_268 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_477: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_266);  permute_266 = None
	        permute_269: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_477, [1, 0, 2]);  alias_477 = None
	        view_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_269, [sym_size_int, 384]);  permute_269 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_39: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_81 = torch.ops.aten.split.Tensor(full_39, 384, 1)
	        getitem_327: "f32[s1, 384][1152, 1]cuda:0" = split_81[0];  split_81 = None
	        alias_478: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_327);  getitem_327 = None
	        alias_479: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_102);  view_102 = None
	        copy_23: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_478, alias_479);  alias_478 = alias_479 = None
	        slice_scatter_23: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_39, copy_23, 1, 0, 384);  full_39 = copy_23 = None
	        alias_482: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_101);  view_101 = None
	        split_84 = torch.ops.aten.split.Tensor(slice_scatter_23, 384, 1)
	        getitem_337: "f32[s1, 384][1152, 1]cuda:0" = split_84[1];  split_84 = None
	        alias_483: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_337);  getitem_337 = None
	        copy_24: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_483, alias_482);  alias_483 = alias_482 = None
	        slice_scatter_24: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_23, copy_24, 1, 384, 768);  slice_scatter_23 = copy_24 = None
	        alias_486: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_100);  view_100 = None
	        split_87 = torch.ops.aten.split.Tensor(slice_scatter_24, 384, 1)
	        getitem_347: "f32[s1, 384][1152, 1]cuda:0" = split_87[2];  split_87 = None
	        alias_487: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_347);  getitem_347 = None
	        copy_25: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_487, alias_486);  alias_487 = alias_486 = None
	        slice_scatter_25: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_24, copy_25, 1, 768, 1152);  slice_scatter_24 = copy_25 = None
	        mm_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_25, primals_19);  primals_19 = None
	        permute_271: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_25, [1, 0]);  slice_scatter_25 = None
	        mm_115: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_271, mul_25);  permute_271 = mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_45: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_6, torch.float32);  gt_6 = None
	        mul_284: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_45, 1.4285714285714286);  convert_element_type_45 = None
	        mul_285: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_114, mul_284);  mm_114 = mul_284 = None
	        clone_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_285, memory_format = torch.contiguous_format);  mul_285 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_286: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_21, mul_22);  mul_22 = None
	        mul_287: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_21, primals_18);  clone_21 = primals_18 = None
	        sum_31: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_286, [0], True);  mul_286 = None
	        unsqueeze_61: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_31, 0);  sum_31 = None
	        view_103: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_61, [384]);  unsqueeze_61 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_288: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_287, add_7)
	        mul_289: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_287, rsqrt_4);  mul_287 = rsqrt_4 = None
	        sum_32: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_288, [1], True);  mul_288 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_79: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_78, mul_289);  add_78 = mul_289 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_489: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_44);  alias_44 = None
	        alias_490: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_489);  alias_489 = None
	        alias_491: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_490);  alias_490 = None
	        pow_51: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_491, 3);  alias_491 = None
	        mul_290: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_32, -0.5);  sum_32 = None
	        mul_291: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_290, pow_51);  mul_290 = pow_51 = None
	        expand_15: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_291, [-1, 384]);  mul_291 = None
	        div_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_15, 384);  expand_15 = None
	        pow_52: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 1.0);  add_7 = None
	        mul_292: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_52, 2.0);  pow_52 = None
	        mul_293: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_15, mul_292);  div_15 = mul_292 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_80: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_79, mul_293);  add_79 = mul_293 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_5, torch.float32);  gt_5 = None
	        mul_294: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_46, 1.4285714285714286);  convert_element_type_46 = None
	        mul_295: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_80, mul_294);  mul_294 = None
	        clone_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_295, memory_format = torch.contiguous_format);  mul_295 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_116: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_22, primals_17);  primals_17 = None
	        permute_272: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_22, [1, 0]);  clone_22 = None
	        mm_117: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_272, mul_19);  permute_272 = mul_19 = None
	        convert_element_type_47: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_4, torch.float32);  gt_4 = None
	        mul_296: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_47, 1.4285714285714286);  convert_element_type_47 = None
	        mul_297: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_116, mul_296);  mm_116 = mul_296 = None
	        clone_23: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_297, memory_format = torch.contiguous_format);  mul_297 = None
	        sigmoid_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        full_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_40, sigmoid_14);  full_40 = None
	        mul_298: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sub_6);  mm_6 = sub_6 = None
	        add_81: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_298, 1);  mul_298 = None
	        mul_299: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_14, add_81);  sigmoid_14 = add_81 = None
	        mul_300: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_23, mul_299);  clone_23 = mul_299 = None
	        mm_118: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_300, primals_16);  primals_16 = None
	        permute_274: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_300, [1, 0]);  mul_300 = None
	        mm_119: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_274, mul_16);  permute_274 = mul_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_301: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_118, mul_15);  mul_15 = None
	        mul_302: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_118, primals_15);  mm_118 = primals_15 = None
	        sum_33: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_301, [0], True);  mul_301 = None
	        unsqueeze_62: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_33, 0);  sum_33 = None
	        view_104: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_62, [384]);  unsqueeze_62 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_303: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_302, add_5)
	        mul_304: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_302, rsqrt_3);  mul_302 = rsqrt_3 = None
	        sum_34: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_303, [1], True);  mul_303 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_82: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_80, mul_304);  add_80 = mul_304 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_492: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_41);  alias_41 = None
	        alias_493: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_492);  alias_492 = None
	        alias_494: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_493);  alias_493 = None
	        pow_53: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_494, 3);  alias_494 = None
	        mul_305: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_34, -0.5);  sum_34 = None
	        mul_306: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_305, pow_53);  mul_305 = pow_53 = None
	        expand_16: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_306, [-1, 384]);  mul_306 = None
	        div_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_16, 384);  expand_16 = None
	        pow_54: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 1.0);  add_5 = None
	        mul_307: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_54, 2.0);  pow_54 = None
	        mul_308: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_16, mul_307);  div_16 = mul_307 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_82, mul_308);  add_82 = mul_308 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_120: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_83, primals_14);  primals_14 = None
	        permute_275: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_83, [1, 0])
	        mm_121: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_275, view_7);  permute_275 = view_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_105: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_120, [sym_size_int, 6, 64]);  mm_120 = None
	        alias_495: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_105);  view_105 = None
	        permute_276: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_495, [1, 0, 2]);  alias_495 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_496: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_276);  permute_276 = None
	        permute_277: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_496, [1, 0, 2]);  alias_496 = None
	        alias_497: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_277);  permute_277 = None
	        unsqueeze_63: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_497, 0);  alias_497 = None
	        alias_498: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_36);  alias_36 = None
	        alias_499: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_498);  alias_498 = None
	        _efficient_attention_backward_10 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_63, unsqueeze_3, unsqueeze_4, unsqueeze_5, None, alias_499, convert_element_type_2, convert_element_type_3, sym_size_int_1, sym_size_int_1, getitem_13, 0.0, getitem_14, getitem_15, 0, False);  unsqueeze_63 = unsqueeze_3 = unsqueeze_4 = unsqueeze_5 = alias_499 = convert_element_type_2 = convert_element_type_3 = getitem_13 = getitem_14 = getitem_15 = None
	        getitem_354: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_10[0]
	        getitem_355: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_10[1]
	        getitem_356: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_10[2];  _efficient_attention_backward_10 = None
	        squeeze_42: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_356, 0);  getitem_356 = None
	        squeeze_43: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_355, 0);  getitem_355 = None
	        squeeze_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_354, 0);  getitem_354 = None
	        alias_500: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_42);  squeeze_42 = None
	        permute_278: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_500, [1, 0, 2]);  alias_500 = None
	        alias_501: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_43);  squeeze_43 = None
	        permute_279: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_501, [1, 0, 2]);  alias_501 = None
	        alias_502: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_44);  squeeze_44 = None
	        permute_280: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_502, [1, 0, 2]);  alias_502 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_503: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_278);  permute_278 = None
	        permute_281: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_503, [1, 0, 2]);  alias_503 = None
	        view_106: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_281, [sym_size_int, 384]);  permute_281 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_504: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_279);  permute_279 = None
	        permute_282: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_504, [1, 0, 2]);  alias_504 = None
	        view_107: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_282, [sym_size_int, 384]);  permute_282 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_505: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_280);  permute_280 = None
	        permute_283: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_505, [1, 0, 2]);  alias_505 = None
	        view_108: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_283, [sym_size_int, 384]);  permute_283 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_41: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_90 = torch.ops.aten.split.Tensor(full_41, 384, 1)
	        getitem_358: "f32[s1, 384][1152, 1]cuda:0" = split_90[0];  split_90 = None
	        alias_506: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_358);  getitem_358 = None
	        alias_507: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_108);  view_108 = None
	        copy_26: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_506, alias_507);  alias_506 = alias_507 = None
	        slice_scatter_26: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_41, copy_26, 1, 0, 384);  full_41 = copy_26 = None
	        alias_510: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_107);  view_107 = None
	        split_93 = torch.ops.aten.split.Tensor(slice_scatter_26, 384, 1)
	        getitem_368: "f32[s1, 384][1152, 1]cuda:0" = split_93[1];  split_93 = None
	        alias_511: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_368);  getitem_368 = None
	        copy_27: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_511, alias_510);  alias_511 = alias_510 = None
	        slice_scatter_27: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_26, copy_27, 1, 384, 768);  slice_scatter_26 = copy_27 = None
	        alias_514: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_106);  view_106 = None
	        split_96 = torch.ops.aten.split.Tensor(slice_scatter_27, 384, 1)
	        getitem_378: "f32[s1, 384][1152, 1]cuda:0" = split_96[2];  split_96 = None
	        alias_515: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_378);  getitem_378 = None
	        copy_28: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_515, alias_514);  alias_515 = alias_514 = None
	        slice_scatter_28: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_27, copy_28, 1, 768, 1152);  slice_scatter_27 = copy_28 = None
	        mm_122: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_28, primals_13);  primals_13 = None
	        permute_285: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_28, [1, 0]);  slice_scatter_28 = None
	        mm_123: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_285, mul_14);  permute_285 = mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_48: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_3, torch.float32);  gt_3 = None
	        mul_309: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_48, 1.4285714285714286);  convert_element_type_48 = None
	        mul_310: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_122, mul_309);  mm_122 = mul_309 = None
	        clone_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_310, memory_format = torch.contiguous_format);  mul_310 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_311: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_24, mul_11);  mul_11 = None
	        mul_312: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_24, primals_12);  clone_24 = primals_12 = None
	        sum_35: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_311, [0], True);  mul_311 = None
	        unsqueeze_64: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_35, 0);  sum_35 = None
	        view_109: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_64, [384]);  unsqueeze_64 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_313: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_312, add_3)
	        mul_314: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_312, rsqrt_2);  mul_312 = rsqrt_2 = None
	        sum_36: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_313, [1], True);  mul_313 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_83, mul_314);  add_83 = mul_314 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_517: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_23);  alias_23 = None
	        alias_518: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_517);  alias_517 = None
	        alias_519: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_518);  alias_518 = None
	        pow_55: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_519, 3);  alias_519 = None
	        mul_315: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_36, -0.5);  sum_36 = None
	        mul_316: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_315, pow_55);  mul_315 = pow_55 = None
	        expand_17: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_316, [-1, 384]);  mul_316 = None
	        div_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_17, 384);  expand_17 = None
	        pow_56: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 1.0);  add_3 = None
	        mul_317: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_56, 2.0);  pow_56 = None
	        mul_318: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_17, mul_317);  div_17 = mul_317 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_85: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_84, mul_318);  add_84 = mul_318 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_49: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
	        mul_319: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_49, 1.4285714285714286);  convert_element_type_49 = None
	        mul_320: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_85, mul_319);  mul_319 = None
	        clone_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_320, memory_format = torch.contiguous_format);  mul_320 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_124: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_25, primals_11);  primals_11 = None
	        permute_286: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_25, [1, 0]);  clone_25 = None
	        mm_125: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_286, mul_8);  permute_286 = mul_8 = None
	        convert_element_type_50: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
	        mul_321: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_50, 1.4285714285714286);  convert_element_type_50 = None
	        mul_322: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_124, mul_321);  mm_124 = mul_321 = None
	        clone_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_322, memory_format = torch.contiguous_format);  mul_322 = None
	        sigmoid_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        full_42: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_42, sigmoid_15);  full_42 = None
	        mul_323: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub_7);  mm_2 = sub_7 = None
	        add_86: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_323, 1);  mul_323 = None
	        mul_324: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_15, add_86);  sigmoid_15 = add_86 = None
	        mul_325: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_26, mul_324);  clone_26 = mul_324 = None
	        mm_126: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_325, primals_10);  primals_10 = None
	        permute_288: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_325, [1, 0]);  mul_325 = None
	        mm_127: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_288, mul_5);  permute_288 = mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_326: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_126, mul_4);  mul_4 = None
	        mul_327: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_126, primals_9);  mm_126 = primals_9 = None
	        sum_37: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_326, [0], True);  mul_326 = None
	        unsqueeze_65: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_37, 0);  sum_37 = None
	        view_110: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_65, [384]);  unsqueeze_65 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_328: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_327, add_1)
	        mul_329: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_327, rsqrt_1);  mul_327 = rsqrt_1 = None
	        sum_38: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_328, [1], True);  mul_328 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_87: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_85, mul_329);  add_85 = mul_329 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_520: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
	        alias_521: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_520);  alias_520 = None
	        alias_522: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_521);  alias_521 = None
	        pow_57: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_522, 3);  alias_522 = None
	        mul_330: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_38, -0.5);  sum_38 = None
	        mul_331: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_330, pow_57);  mul_330 = pow_57 = None
	        expand_18: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_331, [-1, 384]);  mul_331 = None
	        div_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_18, 384);  expand_18 = None
	        pow_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
	        mul_332: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_58, 2.0);  pow_58 = None
	        mul_333: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_18, mul_332);  div_18 = mul_332 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_88: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_87, mul_333);  add_87 = mul_333 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_128: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_88, primals_8);  primals_8 = None
	        permute_289: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_88, [1, 0])
	        mm_129: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_289, view_3);  permute_289 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_111: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_128, [sym_size_int, 6, 64]);  mm_128 = None
	        alias_523: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_111);  view_111 = None
	        permute_290: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_523, [1, 0, 2]);  alias_523 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_524: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_290);  permute_290 = None
	        permute_291: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_524, [1, 0, 2]);  alias_524 = None
	        alias_525: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_291);  permute_291 = None
	        unsqueeze_66: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_525, 0);  alias_525 = None
	        alias_526: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
	        alias_527: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_526);  alias_526 = None
	        _efficient_attention_backward_11 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_66, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_527, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_66 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_527 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_385: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_11[0]
	        getitem_386: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_11[1]
	        getitem_387: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_11[2];  _efficient_attention_backward_11 = None
	        squeeze_45: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_387, 0);  getitem_387 = None
	        squeeze_46: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_386, 0);  getitem_386 = None
	        squeeze_47: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_385, 0);  getitem_385 = None
	        alias_528: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_45);  squeeze_45 = None
	        permute_292: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_528, [1, 0, 2]);  alias_528 = None
	        alias_529: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_46);  squeeze_46 = None
	        permute_293: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_529, [1, 0, 2]);  alias_529 = None
	        alias_530: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_47);  squeeze_47 = None
	        permute_294: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_530, [1, 0, 2]);  alias_530 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_531: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_292);  permute_292 = None
	        permute_295: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_531, [1, 0, 2]);  alias_531 = None
	        view_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_295, [sym_size_int, 384]);  permute_295 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_532: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_293);  permute_293 = None
	        permute_296: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_532, [1, 0, 2]);  alias_532 = None
	        view_113: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_296, [sym_size_int, 384]);  permute_296 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_533: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_294);  permute_294 = None
	        permute_297: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_533, [1, 0, 2]);  alias_533 = None
	        view_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_297, [sym_size_int, 384]);  permute_297 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_43: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        split_99 = torch.ops.aten.split.Tensor(full_43, 384, 1)
	        getitem_389: "f32[s1, 384][1152, 1]cuda:0" = split_99[0];  split_99 = None
	        alias_534: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_389);  getitem_389 = None
	        alias_535: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_114);  view_114 = None
	        copy_29: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_534, alias_535);  alias_534 = alias_535 = None
	        slice_scatter_29: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_43, copy_29, 1, 0, 384);  full_43 = copy_29 = None
	        alias_538: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_113);  view_113 = None
	        split_102 = torch.ops.aten.split.Tensor(slice_scatter_29, 384, 1)
	        getitem_399: "f32[s1, 384][1152, 1]cuda:0" = split_102[1];  split_102 = None
	        alias_539: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_399);  getitem_399 = None
	        copy_30: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_539, alias_538);  alias_539 = alias_538 = None
	        slice_scatter_30: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_29, copy_30, 1, 384, 768);  slice_scatter_29 = copy_30 = None
	        alias_542: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_112);  view_112 = None
	        split_105 = torch.ops.aten.split.Tensor(slice_scatter_30, 384, 1)
	        getitem_409: "f32[s1, 384][1152, 1]cuda:0" = split_105[2];  split_105 = None
	        alias_543: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_409);  getitem_409 = None
	        copy_31: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_543, alias_542);  alias_543 = alias_542 = None
	        slice_scatter_31: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_30, copy_31, 1, 768, 1152);  slice_scatter_30 = copy_31 = None
	        mm_130: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_31, primals_7);  primals_7 = None
	        permute_299: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_31, [1, 0]);  slice_scatter_31 = None
	        mm_131: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_299, mul_3);  permute_299 = mul_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_334: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_51, 1.4285714285714286);  convert_element_type_51 = None
	        mul_335: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_130, mul_334);  mm_130 = mul_334 = None
	        clone_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_335, memory_format = torch.contiguous_format);  mul_335 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_336: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_27, mul);  mul = None
	        mul_337: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_27, primals_6);  clone_27 = primals_6 = None
	        sum_39: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_336, [0], True);  mul_336 = None
	        unsqueeze_67: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_39, 0);  sum_39 = None
	        view_115: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_67, [384]);  unsqueeze_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_338: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_337, primals_1)
	        mul_339: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_337, rsqrt);  mul_337 = rsqrt = None
	        sum_40: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_338, [1], True);  mul_338 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_89: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_88, mul_339);  add_88 = mul_339 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_545: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
	        alias_546: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_545);  alias_545 = None
	        alias_547: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_546);  alias_546 = None
	        pow_59: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_547, 3);  alias_547 = None
	        mul_340: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_40, -0.5);  sum_40 = None
	        mul_341: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_340, pow_59);  mul_340 = pow_59 = None
	        expand_19: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_341, [-1, 384]);  mul_341 = None
	        div_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_19, 384);  expand_19 = None
	        pow_60: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
	        mul_342: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_60, 2.0);  pow_60 = None
	        mul_343: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_19, mul_342);  div_19 = mul_342 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_90: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_89, mul_343);  add_89 = mul_343 = None
	        return pytree.tree_unflatten([add_39, primals_31, primals_32, primals_33, add_90, primals_2, primals_3, primals_4, None, view_115, mm_131, mm_129, view_110, mm_127, mm_125, view_109, mm_123, mm_121, view_104, mm_119, mm_117, view_103, mm_115, mm_113, view_98, mm_111, mm_109, view_97, mm_107, mm_105, view_92, mm_103, mm_101, add_70, primals_31, primals_32, primals_33, None, view_91, mm_99, mm_97, view_86, mm_95, mm_93, mm_91, view_81, mm_89, mm_87, view_80, mm_85, mm_83, view_75, mm_81, mm_79, mm_77, view_70, mm_75, mm_73, view_69, mm_71, mm_69, view_64, mm_67, mm_65, mm_63, view_59, mm_61, mm_59, view_58, mm_57, mm_55, view_53, mm_53, mm_51, mm_49, view_48, mm_47, mm_45], self._out_spec)
	        
V0303 09:09:53.778914 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "43bf19209feacee6276caf46a73f74b8"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0", primals_30: "f32[s6, 384][384, 1]cuda:0", primals_31: "i64[257][1]cuda:0", primals_32: "f32[s8, 0][1, 1]cuda:0", primals_33: "f32[s9, 0][1, 1]cuda:0", primals_34: "Sym(s5)", primals_35: "f32[384][1]cuda:0", primals_36: "f32[1152, 384][384, 1]cuda:0", primals_37: "f32[384, 384][384, 1]cuda:0", primals_38: "f32[384][1]cuda:0", primals_39: "f32[384, 384][384, 1]cuda:0", primals_40: "f32[768, 384][384, 1]cuda:0", primals_41: "f32[384, 384][384, 1]cuda:0", primals_42: "f32[384][1]cuda:0", primals_43: "f32[1024, 384][384, 1]cuda:0", primals_44: "f32[384, 1024][1024, 1]cuda:0", primals_45: "f32[384][1]cuda:0", primals_46: "f32[1152, 384][384, 1]cuda:0", primals_47: "f32[384, 384][384, 1]cuda:0", primals_48: "f32[384][1]cuda:0", primals_49: "f32[384, 384][384, 1]cuda:0", primals_50: "f32[768, 384][384, 1]cuda:0", primals_51: "f32[384, 384][384, 1]cuda:0", primals_52: "f32[384][1]cuda:0", primals_53: "f32[1024, 384][384, 1]cuda:0", primals_54: "f32[384, 1024][1024, 1]cuda:0", primals_55: "f32[384][1]cuda:0", primals_56: "f32[1152, 384][384, 1]cuda:0", primals_57: "f32[384, 384][384, 1]cuda:0", primals_58: "f32[384][1]cuda:0", primals_59: "f32[384, 384][384, 1]cuda:0", primals_60: "f32[768, 384][384, 1]cuda:0", primals_61: "f32[384, 384][384, 1]cuda:0", primals_62: "f32[384][1]cuda:0", primals_63: "f32[1024, 384][384, 1]cuda:0", primals_64: "f32[384, 1024][1024, 1]cuda:0", primals_65: "f32[384][1]cuda:0", primals_66: "f32[1152, 384][384, 1]cuda:0", primals_67: "f32[384, 384][384, 1]cuda:0", primals_68: "f32[384][1]cuda:0", primals_69: "f32[384, 384][384, 1]cuda:0", primals_70: "f32[768, 384][384, 1]cuda:0", primals_71: "f32[384, 384][384, 1]cuda:0", primals_72: "f32[384][1]cuda:0", primals_73: "f32[1024, 384][384, 1]cuda:0", primals_74: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[28][1]cuda:0" = torch.ops.prims.inductor_seeds.default(28, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_27, 0.3);  inductor_random_default_27 = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_1: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_26, 0.3);  inductor_random_default_26 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_2: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2)
	        inductor_random_default_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_25, 0.3);  inductor_random_default_25 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12);  mul_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_3: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 3)
	        inductor_random_default_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_3, 'rand');  inductor_lookup_seed_default_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_24, 0.3);  inductor_random_default_24 = None
	        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
	        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
	        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0, 2]);  view_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_16, 0);  permute_16 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_18, 0);  permute_18 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
	        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
	        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
	        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
	        mm_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_7, permute_21);  view_7 = permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
	        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15);  mul_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_4: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 4)
	        inductor_random_default_23: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_4, 'rand');  inductor_lookup_seed_default_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_23, 0.3);  inductor_random_default_23 = None
	        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_5: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 5)
	        inductor_random_default_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_5, 'rand');  inductor_lookup_seed_default_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_22, 0.3);  inductor_random_default_22 = None
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18);  mul_22 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_6: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 6)
	        inductor_random_default_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_6, 'rand');  inductor_lookup_seed_default_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_21, 0.3);  inductor_random_default_21 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
	        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
	        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_8, [1, 0, 2]);  view_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_9, [1, 0, 2]);  view_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_10, [1, 0, 2]);  view_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_25, [1, 0, 2]);  permute_25 = None
	        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_26, [1, 0, 2]);  permute_26 = None
	        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_27, [1, 0, 2]);  permute_27 = None
	        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_28, 0);  permute_28 = None
	        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_29, 0);  permute_29 = None
	        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_30, 0);  permute_30 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
	        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
	        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
	        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
	        mm_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_11, permute_33);  view_11 = permute_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
	        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21);  mul_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_7: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 7)
	        inductor_random_default_20: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_7, 'rand');  inductor_lookup_seed_default_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_20, 0.3);  inductor_random_default_20 = None
	        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_8: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 8)
	        inductor_random_default_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_8, 'rand');  inductor_lookup_seed_default_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_19, 0.3);  inductor_random_default_19 = None
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24);  mul_33 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_9: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 9)
	        inductor_random_default_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_9, 'rand');  inductor_lookup_seed_default_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_18, 0.3);  inductor_random_default_18 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
	        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
	        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_12, [1, 0, 2]);  view_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_13, [1, 0, 2]);  view_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_14, [1, 0, 2]);  view_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_37, [1, 0, 2]);  permute_37 = None
	        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_38, [1, 0, 2]);  permute_38 = None
	        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_39, [1, 0, 2]);  permute_39 = None
	        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_40, 0);  permute_40 = None
	        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_41, 0);  permute_41 = None
	        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_42, 0);  permute_42 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
	        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
	        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
	        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
	        mm_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_15, permute_45);  view_15 = permute_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
	        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27);  mul_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_10: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 10)
	        inductor_random_default_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_10, 'rand');  inductor_lookup_seed_default_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_17, 0.3);  inductor_random_default_17 = None
	        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_11: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 11)
	        inductor_random_default_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_11, 'rand');  inductor_lookup_seed_default_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_16, 0.3);  inductor_random_default_16 = None
	        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_9: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_9, [1], True);  pow_9 = None
	        add_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_8, 1e-06);  mean_8 = None
	        rsqrt_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_16);  add_16 = None
	        mul_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_8)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_45: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_44, primals_35);  mul_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int_3: "Sym(s6)" = torch.ops.aten.sym_size.int(primals_30, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_12: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 12)
	        inductor_random_default_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_12, 'rand');  inductor_lookup_seed_default_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_12: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_15, 0.3);  inductor_random_default_15 = None
	        mul_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_12, mul_45);  mul_45 = None
	        mul_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_46, 1.4285714285714286);  mul_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_48: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_36, [1, 0])
	        mm_16: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_47, permute_48);  permute_48 = None
	        split_4 = torch.ops.aten.split.Tensor(mm_16, 384, 1);  mm_16 = None
	        getitem_36: "f32[s6, 384][1152, 1]cuda:0" = split_4[0]
	        getitem_37: "f32[s6, 384][1152, 1]cuda:0" = split_4[1]
	        getitem_38: "f32[s6, 384][1152, 1]cuda:0" = split_4[2];  split_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_16: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_36, [sym_size_int_3, 6, 64]);  getitem_36 = None
	        permute_49: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_16, [1, 0, 2]);  view_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_17: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_37, [sym_size_int_3, 6, 64]);  getitem_37 = None
	        permute_50: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_17, [1, 0, 2]);  view_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_18: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_38, [sym_size_int_3, 6, 64]);  getitem_38 = None
	        permute_51: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0, 2]);  view_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_52: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_49, [1, 0, 2]);  permute_49 = None
	        permute_53: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_50, [1, 0, 2]);  permute_50 = None
	        permute_54: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_51, [1, 0, 2]);  permute_51 = None
	        convert_element_type_8: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_12: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_52, 0);  permute_52 = None
	        unsqueeze_13: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_53, 0);  permute_53 = None
	        unsqueeze_14: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_54, 0);  permute_54 = None
	        sym_size_int_4: "Sym(s9)" = torch.ops.aten.sym_size.int(primals_33, 0)
	        _efficient_attention_forward_4 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_12, unsqueeze_13, unsqueeze_14, None, convert_element_type_8, convert_element_type_8, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_39: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_4[0]
	        getitem_40: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_4[1]
	        getitem_41: "i64[][]cuda:0" = _efficient_attention_forward_4[2]
	        getitem_42: "i64[][]cuda:0" = _efficient_attention_forward_4[3];  _efficient_attention_forward_4 = None
	        squeeze_4: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_39, 0)
	        permute_55: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_4, [1, 0, 2]);  squeeze_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_56: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_55, [1, 0, 2]);  permute_55 = None
	        view_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_56, [sym_size_int_3, 384]);  permute_56 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_57: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_37, [1, 0])
	        mm_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_19, permute_57);  view_19 = permute_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_30, mm_17);  mm_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_9: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_10, [1], True);  pow_10 = None
	        add_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_9, 1e-06);  mean_9 = None
	        rsqrt_9: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_18);  add_18 = None
	        mul_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_49: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_48, primals_38);  mul_48 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_13: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 13)
	        inductor_random_default_14: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_13, 'rand');  inductor_lookup_seed_default_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_13: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_14, 0.3);  inductor_random_default_14 = None
	        mul_50: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_13, mul_49);  mul_49 = None
	        mul_51: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_50, 1.4285714285714286);  mul_50 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_58: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_39, [1, 0])
	        mm_18: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_51, permute_58);  permute_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_59: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_40, [1, 0])
	        mm_19: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_59);  permute_59 = None
	        split_5 = torch.ops.aten.split.Tensor(mm_19, 384, 1);  mm_19 = None
	        getitem_45: "f32[s1, 384][768, 1]cuda:0" = split_5[0]
	        getitem_46: "f32[s1, 384][768, 1]cuda:0" = split_5[1];  split_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_20: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_18, [sym_size_int_3, 6, 64]);  mm_18 = None
	        permute_60: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0, 2]);  view_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_21: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_45, [sym_size_int, 6, 64]);  getitem_45 = None
	        permute_61: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_21, [1, 0, 2]);  view_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_22: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_46, [sym_size_int, 6, 64]);  getitem_46 = None
	        permute_62: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_22, [1, 0, 2]);  view_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_63: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_60, [1, 0, 2]);  permute_60 = None
	        permute_64: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_61, [1, 0, 2]);  permute_61 = None
	        permute_65: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_62, [1, 0, 2]);  permute_62 = None
	        convert_element_type_10: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_11: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_15: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_63, 0);  permute_63 = None
	        unsqueeze_16: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_64, 0);  permute_64 = None
	        unsqueeze_17: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_65, 0);  permute_65 = None
	        _efficient_attention_forward_5 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_15, unsqueeze_16, unsqueeze_17, None, convert_element_type_10, convert_element_type_11, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_47: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_5[0]
	        getitem_48: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_5[1]
	        getitem_49: "i64[][]cuda:0" = _efficient_attention_forward_5[2]
	        getitem_50: "i64[][]cuda:0" = _efficient_attention_forward_5[3];  _efficient_attention_forward_5 = None
	        squeeze_5: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_47, 0)
	        permute_66: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_5, [1, 0, 2]);  squeeze_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_67: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_66, [1, 0, 2]);  permute_66 = None
	        view_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_67, [sym_size_int_3, 384]);  permute_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_68: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_41, [1, 0])
	        mm_20: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_23, permute_68);  view_23 = permute_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_17, mm_20);  add_17 = mm_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_19, 2)
	        mean_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_11, [1], True);  pow_11 = None
	        add_20: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_10, 1e-06);  mean_10 = None
	        rsqrt_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_20);  add_20 = None
	        mul_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_19, rsqrt_10)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_53: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, primals_42);  mul_52 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_69: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_43, [1, 0])
	        mm_21: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_53, permute_69);  permute_69 = None
	        sigmoid_4: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_21)
	        mul_54: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_21, sigmoid_4);  sigmoid_4 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_14: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 14)
	        inductor_random_default_13: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_14, 'rand');  inductor_lookup_seed_default_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_14: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_13, 0.3);  inductor_random_default_13 = None
	        mul_55: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_14, mul_54);  mul_54 = None
	        mul_56: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, 1.4285714285714286);  mul_55 = None
	        permute_70: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_44, [1, 0])
	        mm_22: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_56, permute_70);  permute_70 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_15: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 15)
	        inductor_random_default_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_15, 'rand');  inductor_lookup_seed_default_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_15: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_12, 0.3);  inductor_random_default_12 = None
	        mul_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_15, mm_22);  mm_22 = None
	        mul_58: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_57, 1.4285714285714286);  mul_57 = None
	        add_21: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, mul_58);  mul_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_11: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_12, [1], True);  pow_12 = None
	        add_22: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None
	        rsqrt_11: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
	        mul_59: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_11)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_59, primals_45);  mul_59 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_16: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 16)
	        inductor_random_default_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_16, 'rand');  inductor_lookup_seed_default_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_16: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_11, 0.3);  inductor_random_default_11 = None
	        mul_61: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_16, mul_60);  mul_60 = None
	        mul_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_61, 1.4285714285714286);  mul_61 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_71: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_46, [1, 0])
	        mm_23: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_62, permute_71);  permute_71 = None
	        split_6 = torch.ops.aten.split.Tensor(mm_23, 384, 1);  mm_23 = None
	        getitem_53: "f32[s6, 384][1152, 1]cuda:0" = split_6[0]
	        getitem_54: "f32[s6, 384][1152, 1]cuda:0" = split_6[1]
	        getitem_55: "f32[s6, 384][1152, 1]cuda:0" = split_6[2];  split_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_24: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_53, [sym_size_int_3, 6, 64]);  getitem_53 = None
	        permute_72: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0, 2]);  view_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_25: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_54, [sym_size_int_3, 6, 64]);  getitem_54 = None
	        permute_73: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_25, [1, 0, 2]);  view_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_26: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_55, [sym_size_int_3, 6, 64]);  getitem_55 = None
	        permute_74: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0, 2]);  view_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_75: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_72, [1, 0, 2]);  permute_72 = None
	        permute_76: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_73, [1, 0, 2]);  permute_73 = None
	        permute_77: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_74, [1, 0, 2]);  permute_74 = None
	        convert_element_type_12: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_18: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_75, 0);  permute_75 = None
	        unsqueeze_19: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_76, 0);  permute_76 = None
	        unsqueeze_20: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_77, 0);  permute_77 = None
	        _efficient_attention_forward_6 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_18, unsqueeze_19, unsqueeze_20, None, convert_element_type_12, convert_element_type_12, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_56: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_6[0]
	        getitem_57: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_6[1]
	        getitem_58: "i64[][]cuda:0" = _efficient_attention_forward_6[2]
	        getitem_59: "i64[][]cuda:0" = _efficient_attention_forward_6[3];  _efficient_attention_forward_6 = None
	        squeeze_6: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_56, 0)
	        permute_78: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_6, [1, 0, 2]);  squeeze_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_79: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_78, [1, 0, 2]);  permute_78 = None
	        view_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_79, [sym_size_int_3, 384]);  permute_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_80: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_47, [1, 0])
	        mm_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_27, permute_80);  view_27 = permute_80 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_21, mm_24);  mm_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_13: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_13, [1], True);  pow_13 = None
	        add_24: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None
	        rsqrt_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_24);  add_24 = None
	        mul_63: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_12)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_64: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_63, primals_48)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_17: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 17)
	        inductor_random_default_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_17, 'rand');  inductor_lookup_seed_default_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_17: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_10, 0.3);  inductor_random_default_10 = None
	        mul_65: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_17, mul_64);  mul_64 = None
	        mul_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, 1.4285714285714286);  mul_65 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_81: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_49, [1, 0])
	        mm_25: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_66, permute_81);  permute_81 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_82: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_50, [1, 0])
	        mm_26: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_82);  permute_82 = None
	        split_7 = torch.ops.aten.split.Tensor(mm_26, 384, 1);  mm_26 = None
	        getitem_62: "f32[s1, 384][768, 1]cuda:0" = split_7[0]
	        getitem_63: "f32[s1, 384][768, 1]cuda:0" = split_7[1];  split_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_28: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_25, [sym_size_int_3, 6, 64]);  mm_25 = None
	        permute_83: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_28, [1, 0, 2]);  view_28 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_29: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_62, [sym_size_int, 6, 64]);  getitem_62 = None
	        permute_84: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_29, [1, 0, 2]);  view_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_30: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_63, [sym_size_int, 6, 64]);  getitem_63 = None
	        permute_85: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0, 2]);  view_30 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_86: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_83, [1, 0, 2]);  permute_83 = None
	        permute_87: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_84, [1, 0, 2]);  permute_84 = None
	        permute_88: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_85, [1, 0, 2]);  permute_85 = None
	        convert_element_type_14: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_15: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_21: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_86, 0);  permute_86 = None
	        unsqueeze_22: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_87, 0);  permute_87 = None
	        unsqueeze_23: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_88, 0);  permute_88 = None
	        _efficient_attention_forward_7 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_21, unsqueeze_22, unsqueeze_23, None, convert_element_type_14, convert_element_type_15, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_64: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_7[0]
	        getitem_65: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_7[1]
	        getitem_66: "i64[][]cuda:0" = _efficient_attention_forward_7[2]
	        getitem_67: "i64[][]cuda:0" = _efficient_attention_forward_7[3];  _efficient_attention_forward_7 = None
	        squeeze_7: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_64, 0)
	        permute_89: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_7, [1, 0, 2]);  squeeze_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_90: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_89, [1, 0, 2]);  permute_89 = None
	        view_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_90, [sym_size_int_3, 384]);  permute_90 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_91: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_51, [1, 0])
	        mm_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_31, permute_91);  view_31 = permute_91 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_25: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, mm_27);  add_23 = mm_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_14: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_25, 2)
	        mean_13: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_14, [1], True);  pow_14 = None
	        add_26: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None
	        rsqrt_13: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_26);  add_26 = None
	        mul_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, rsqrt_13)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_67, primals_52);  mul_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_92: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_53, [1, 0])
	        mm_28: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_68, permute_92);  permute_92 = None
	        sigmoid_5: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_28)
	        mul_69: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_28, sigmoid_5);  sigmoid_5 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_18: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 18)
	        inductor_random_default_9: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_18, 'rand');  inductor_lookup_seed_default_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_18: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_9, 0.3);  inductor_random_default_9 = None
	        mul_70: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_18, mul_69);  mul_69 = None
	        mul_71: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_70, 1.4285714285714286);  mul_70 = None
	        permute_93: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_54, [1, 0])
	        mm_29: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_71, permute_93);  permute_93 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_19: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 19)
	        inductor_random_default_8: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_19, 'rand');  inductor_lookup_seed_default_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_19: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_8, 0.3);  inductor_random_default_8 = None
	        mul_72: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_19, mm_29);  mm_29 = None
	        mul_73: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_72, 1.4285714285714286);  mul_72 = None
	        add_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_25, mul_73);  mul_73 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_15, [1], True);  pow_15 = None
	        add_28: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None
	        rsqrt_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_28);  add_28 = None
	        mul_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_14)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_75: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_74, primals_55);  mul_74 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_20: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 20)
	        inductor_random_default_7: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_20, 'rand');  inductor_lookup_seed_default_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_20: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_7, 0.3);  inductor_random_default_7 = None
	        mul_76: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_20, mul_75);  mul_75 = None
	        mul_77: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_76, 1.4285714285714286);  mul_76 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_94: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_56, [1, 0])
	        mm_30: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_77, permute_94);  permute_94 = None
	        split_8 = torch.ops.aten.split.Tensor(mm_30, 384, 1);  mm_30 = None
	        getitem_70: "f32[s6, 384][1152, 1]cuda:0" = split_8[0]
	        getitem_71: "f32[s6, 384][1152, 1]cuda:0" = split_8[1]
	        getitem_72: "f32[s6, 384][1152, 1]cuda:0" = split_8[2];  split_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_32: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_70, [sym_size_int_3, 6, 64]);  getitem_70 = None
	        permute_95: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0, 2]);  view_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_33: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_71, [sym_size_int_3, 6, 64]);  getitem_71 = None
	        permute_96: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_33, [1, 0, 2]);  view_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_34: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_72, [sym_size_int_3, 6, 64]);  getitem_72 = None
	        permute_97: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_34, [1, 0, 2]);  view_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_98: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_95, [1, 0, 2]);  permute_95 = None
	        permute_99: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_96, [1, 0, 2]);  permute_96 = None
	        permute_100: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_97, [1, 0, 2]);  permute_97 = None
	        convert_element_type_16: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_24: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_98, 0);  permute_98 = None
	        unsqueeze_25: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_99, 0);  permute_99 = None
	        unsqueeze_26: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_100, 0);  permute_100 = None
	        _efficient_attention_forward_8 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_24, unsqueeze_25, unsqueeze_26, None, convert_element_type_16, convert_element_type_16, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_73: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_8[0]
	        getitem_74: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_8[1]
	        getitem_75: "i64[][]cuda:0" = _efficient_attention_forward_8[2]
	        getitem_76: "i64[][]cuda:0" = _efficient_attention_forward_8[3];  _efficient_attention_forward_8 = None
	        squeeze_8: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_73, 0)
	        permute_101: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_8, [1, 0, 2]);  squeeze_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_102: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_101, [1, 0, 2]);  permute_101 = None
	        view_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_102, [sym_size_int_3, 384]);  permute_102 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_103: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_57, [1, 0])
	        mm_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_35, permute_103);  view_35 = permute_103 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_29: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, mm_31);  mm_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_16: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_15: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_16, [1], True);  pow_16 = None
	        add_30: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None
	        rsqrt_15: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_30);  add_30 = None
	        mul_78: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_15)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_79: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_78, primals_58)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_21: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 21)
	        inductor_random_default_6: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_21, 'rand');  inductor_lookup_seed_default_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_21: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_6, 0.3);  inductor_random_default_6 = None
	        mul_80: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_21, mul_79);  mul_79 = None
	        mul_81: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, 1.4285714285714286);  mul_80 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_104: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_59, [1, 0])
	        mm_32: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_81, permute_104);  permute_104 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_105: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_60, [1, 0])
	        mm_33: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_105);  permute_105 = None
	        split_9 = torch.ops.aten.split.Tensor(mm_33, 384, 1);  mm_33 = None
	        getitem_79: "f32[s1, 384][768, 1]cuda:0" = split_9[0]
	        getitem_80: "f32[s1, 384][768, 1]cuda:0" = split_9[1];  split_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_36: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_32, [sym_size_int_3, 6, 64]);  mm_32 = None
	        permute_106: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_36, [1, 0, 2]);  view_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_37: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_79, [sym_size_int, 6, 64]);  getitem_79 = None
	        permute_107: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_37, [1, 0, 2]);  view_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_38: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_80, [sym_size_int, 6, 64]);  getitem_80 = None
	        permute_108: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_38, [1, 0, 2]);  view_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_109: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_106, [1, 0, 2]);  permute_106 = None
	        permute_110: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_107, [1, 0, 2]);  permute_107 = None
	        permute_111: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_108, [1, 0, 2]);  permute_108 = None
	        convert_element_type_18: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_19: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_27: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_109, 0);  permute_109 = None
	        unsqueeze_28: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_110, 0);  permute_110 = None
	        unsqueeze_29: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_111, 0);  permute_111 = None
	        _efficient_attention_forward_9 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_27, unsqueeze_28, unsqueeze_29, None, convert_element_type_18, convert_element_type_19, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_81: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_9[0]
	        getitem_82: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_9[1]
	        getitem_83: "i64[][]cuda:0" = _efficient_attention_forward_9[2]
	        getitem_84: "i64[][]cuda:0" = _efficient_attention_forward_9[3];  _efficient_attention_forward_9 = None
	        squeeze_9: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_81, 0)
	        permute_112: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_9, [1, 0, 2]);  squeeze_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_113: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_112, [1, 0, 2]);  permute_112 = None
	        view_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_113, [sym_size_int_3, 384]);  permute_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_114: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_61, [1, 0])
	        mm_34: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_39, permute_114);  view_39 = permute_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_29, mm_34);  add_29 = mm_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_31, 2)
	        mean_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_17, [1], True);  pow_17 = None
	        add_32: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None
	        rsqrt_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_32);  add_32 = None
	        mul_82: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_31, rsqrt_16)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_83: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_82, primals_62);  mul_82 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_115: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_63, [1, 0])
	        mm_35: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_83, permute_115);  permute_115 = None
	        sigmoid_6: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_35)
	        mul_84: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_35, sigmoid_6);  sigmoid_6 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_22: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 22)
	        inductor_random_default_5: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_22, 'rand');  inductor_lookup_seed_default_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_22: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_5, 0.3);  inductor_random_default_5 = None
	        mul_85: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_22, mul_84);  mul_84 = None
	        mul_86: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, 1.4285714285714286);  mul_85 = None
	        permute_116: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_64, [1, 0])
	        mm_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_86, permute_116);  permute_116 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_23: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 23)
	        inductor_random_default_4: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_23, 'rand');  inductor_lookup_seed_default_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_23: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_4, 0.3);  inductor_random_default_4 = None
	        mul_87: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_23, mm_36);  mm_36 = None
	        mul_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, 1.4285714285714286);  mul_87 = None
	        add_33: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_31, mul_88);  mul_88 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_18: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_17: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_18, [1], True);  pow_18 = None
	        add_34: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None
	        rsqrt_17: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_34);  add_34 = None
	        mul_89: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_17)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_89, primals_65);  mul_89 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_24: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 24)
	        inductor_random_default_3: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_24, 'rand');  inductor_lookup_seed_default_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_24: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_3, 0.3);  inductor_random_default_3 = None
	        mul_91: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_24, mul_90);  mul_90 = None
	        mul_92: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_91, 1.4285714285714286);  mul_91 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_117: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_66, [1, 0])
	        mm_37: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_92, permute_117);  permute_117 = None
	        split_10 = torch.ops.aten.split.Tensor(mm_37, 384, 1);  mm_37 = None
	        getitem_87: "f32[s6, 384][1152, 1]cuda:0" = split_10[0]
	        getitem_88: "f32[s6, 384][1152, 1]cuda:0" = split_10[1]
	        getitem_89: "f32[s6, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_40: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_87, [sym_size_int_3, 6, 64]);  getitem_87 = None
	        permute_118: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_40, [1, 0, 2]);  view_40 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_41: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_88, [sym_size_int_3, 6, 64]);  getitem_88 = None
	        permute_119: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_41, [1, 0, 2]);  view_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_42: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_89, [sym_size_int_3, 6, 64]);  getitem_89 = None
	        permute_120: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_42, [1, 0, 2]);  view_42 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_121: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_118, [1, 0, 2]);  permute_118 = None
	        permute_122: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_119, [1, 0, 2]);  permute_119 = None
	        permute_123: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_120, [1, 0, 2]);  permute_120 = None
	        convert_element_type_20: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_30: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_121, 0);  permute_121 = None
	        unsqueeze_31: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_122, 0);  permute_122 = None
	        unsqueeze_32: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_123, 0);  permute_123 = None
	        _efficient_attention_forward_10 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_30, unsqueeze_31, unsqueeze_32, None, convert_element_type_20, convert_element_type_20, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_90: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_10[0]
	        getitem_91: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_10[1]
	        getitem_92: "i64[][]cuda:0" = _efficient_attention_forward_10[2]
	        getitem_93: "i64[][]cuda:0" = _efficient_attention_forward_10[3];  _efficient_attention_forward_10 = None
	        squeeze_10: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_90, 0)
	        permute_124: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_10, [1, 0, 2]);  squeeze_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_125: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_124, [1, 0, 2]);  permute_124 = None
	        view_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_125, [sym_size_int_3, 384]);  permute_125 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_126: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_67, [1, 0])
	        mm_38: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_43, permute_126);  view_43 = permute_126 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_33, mm_38);  mm_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_19, [1], True);  pow_19 = None
	        add_36: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None
	        rsqrt_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_36);  add_36 = None
	        mul_93: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_18)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_94: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_93, primals_68)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_25: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 25)
	        inductor_random_default_2: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_25, 'rand');  inductor_lookup_seed_default_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_25: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_95: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_25, mul_94);  mul_94 = None
	        mul_96: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_95, 1.4285714285714286);  mul_95 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_127: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_69, [1, 0])
	        mm_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_96, permute_127);  permute_127 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_128: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_70, [1, 0])
	        mm_40: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_128);  permute_128 = None
	        split_11 = torch.ops.aten.split.Tensor(mm_40, 384, 1);  mm_40 = None
	        getitem_96: "f32[s1, 384][768, 1]cuda:0" = split_11[0]
	        getitem_97: "f32[s1, 384][768, 1]cuda:0" = split_11[1];  split_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_44: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_39, [sym_size_int_3, 6, 64]);  mm_39 = None
	        permute_129: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_44, [1, 0, 2]);  view_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_45: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_96, [sym_size_int, 6, 64]);  getitem_96 = None
	        permute_130: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_45, [1, 0, 2]);  view_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_46: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_97, [sym_size_int, 6, 64]);  getitem_97 = None
	        permute_131: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_46, [1, 0, 2]);  view_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_132: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_129, [1, 0, 2]);  permute_129 = None
	        permute_133: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_130, [1, 0, 2]);  permute_130 = None
	        permute_134: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_131, [1, 0, 2]);  permute_131 = None
	        convert_element_type_22: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_23: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_33: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_132, 0);  permute_132 = None
	        unsqueeze_34: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_133, 0);  permute_133 = None
	        unsqueeze_35: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_134, 0);  permute_134 = None
	        _efficient_attention_forward_11 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_33, unsqueeze_34, unsqueeze_35, None, convert_element_type_22, convert_element_type_23, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_98: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_11[0]
	        getitem_99: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_11[1]
	        getitem_100: "i64[][]cuda:0" = _efficient_attention_forward_11[2]
	        getitem_101: "i64[][]cuda:0" = _efficient_attention_forward_11[3];  _efficient_attention_forward_11 = None
	        squeeze_11: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0)
	        permute_135: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_11, [1, 0, 2]);  squeeze_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_136: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_135, [1, 0, 2]);  permute_135 = None
	        view_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_136, [sym_size_int_3, 384]);  permute_136 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_137: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_71, [1, 0])
	        mm_41: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_47, permute_137);  view_47 = permute_137 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:78 in forward, code: attn_out = attn_out + self.cross_attention(
	        add_37: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_35, mm_41);  add_35 = mm_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_20: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_37, 2)
	        mean_19: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_20, [1], True);  pow_20 = None
	        add_38: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None
	        rsqrt_19: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_38);  add_38 = None
	        mul_97: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_37, rsqrt_19)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_98: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_97, primals_72);  mul_97 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_138: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_73, [1, 0])
	        mm_42: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_98, permute_138);  permute_138 = None
	        sigmoid_7: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_42)
	        mul_99: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, sigmoid_7);  sigmoid_7 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_26: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 26)
	        inductor_random_default_1: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_26, 'rand');  inductor_lookup_seed_default_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_26: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_100: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_26, mul_99);  mul_99 = None
	        mul_101: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_100, 1.4285714285714286);  mul_100 = None
	        permute_139: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_74, [1, 0])
	        mm_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_101, permute_139);  permute_139 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_27: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 27);  inductor_seeds_default = None
	        inductor_random_default: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_27, 'rand');  inductor_lookup_seed_default_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_27: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_102: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_27, mm_43);  mm_43 = None
	        mul_103: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, 1.4285714285714286);  mul_102 = None
	        add_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_37, mul_103);  mul_103 = None
	        return (add_39, primals_31, primals_32, primals_33, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, add_3, rsqrt_2, gt_3, mul_14, convert_element_type_2, unsqueeze_3, unsqueeze_4, unsqueeze_5, getitem_12, getitem_13, getitem_14, getitem_15, add_5, rsqrt_3, mul_16, mm_6, gt_4, mul_19, gt_5, add_7, rsqrt_4, gt_6, mul_25, convert_element_type_4, unsqueeze_6, unsqueeze_7, unsqueeze_8, getitem_21, getitem_22, getitem_23, getitem_24, add_9, rsqrt_5, mul_27, mm_10, gt_7, mul_30, gt_8, add_11, rsqrt_6, gt_9, mul_36, convert_element_type_6, unsqueeze_9, unsqueeze_10, unsqueeze_11, getitem_30, getitem_31, getitem_32, getitem_33, add_13, rsqrt_7, mul_38, mm_14, gt_10, mul_41, gt_11, add_15, rsqrt_8, gt_12, mul_47, convert_element_type_8, unsqueeze_12, unsqueeze_13, unsqueeze_14, getitem_39, getitem_40, getitem_41, getitem_42, rsqrt_9, gt_13, mul_51, convert_element_type_10, convert_element_type_11, unsqueeze_15, unsqueeze_16, unsqueeze_17, getitem_47, getitem_48, getitem_49, getitem_50, add_19, rsqrt_10, mul_53, mm_21, gt_14, mul_56, gt_15, add_21, rsqrt_11, gt_16, mul_62, convert_element_type_12, unsqueeze_18, unsqueeze_19, unsqueeze_20, getitem_56, getitem_57, getitem_58, getitem_59, rsqrt_12, mul_63, gt_17, mul_66, convert_element_type_14, convert_element_type_15, unsqueeze_21, unsqueeze_22, unsqueeze_23, getitem_64, getitem_65, getitem_66, getitem_67, add_25, rsqrt_13, mul_68, mm_28, gt_18, mul_71, gt_19, add_27, rsqrt_14, gt_20, mul_77, convert_element_type_16, unsqueeze_24, unsqueeze_25, unsqueeze_26, getitem_73, getitem_74, getitem_75, getitem_76, rsqrt_15, mul_78, gt_21, mul_81, convert_element_type_18, convert_element_type_19, unsqueeze_27, unsqueeze_28, unsqueeze_29, getitem_81, getitem_82, getitem_83, getitem_84, add_31, rsqrt_16, mul_83, mm_35, gt_22, mul_86, gt_23, add_33, rsqrt_17, gt_24, mul_92, convert_element_type_20, unsqueeze_30, unsqueeze_31, unsqueeze_32, getitem_90, getitem_91, getitem_92, getitem_93, rsqrt_18, mul_93, gt_25, mul_96, convert_element_type_22, convert_element_type_23, unsqueeze_33, unsqueeze_34, unsqueeze_35, getitem_98, getitem_99, getitem_100, getitem_101, add_37, rsqrt_19, mul_98, mm_42, gt_26, mul_101, gt_27, sym_size_int, sym_size_int_1, sym_size_int_3, sym_size_int_4)
	        
V0303 09:09:53.838165 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "287ee8f94b7f587cd28f3380be9efb91"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", sym_size_int_3: "Sym(s6)", sym_size_int_4: "Sym(s9)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0", primals_30: "f32[s6, 384][384, 1]cuda:0", primals_31: "i64[257][1]cuda:0", primals_32: "f32[s8, 0][1, 1]cuda:0", primals_33: "f32[s9, 0][1, 1]cuda:0", primals_35: "f32[384][1]cuda:0", primals_36: "f32[1152, 384][384, 1]cuda:0", primals_37: "f32[384, 384][384, 1]cuda:0", primals_38: "f32[384][1]cuda:0", primals_39: "f32[384, 384][384, 1]cuda:0", primals_40: "f32[768, 384][384, 1]cuda:0", primals_41: "f32[384, 384][384, 1]cuda:0", primals_42: "f32[384][1]cuda:0", primals_43: "f32[1024, 384][384, 1]cuda:0", primals_44: "f32[384, 1024][1024, 1]cuda:0", primals_45: "f32[384][1]cuda:0", primals_46: "f32[1152, 384][384, 1]cuda:0", primals_47: "f32[384, 384][384, 1]cuda:0", primals_48: "f32[384][1]cuda:0", primals_49: "f32[384, 384][384, 1]cuda:0", primals_50: "f32[768, 384][384, 1]cuda:0", primals_51: "f32[384, 384][384, 1]cuda:0", primals_52: "f32[384][1]cuda:0", primals_53: "f32[1024, 384][384, 1]cuda:0", primals_54: "f32[384, 1024][1024, 1]cuda:0", primals_55: "f32[384][1]cuda:0", primals_56: "f32[1152, 384][384, 1]cuda:0", primals_57: "f32[384, 384][384, 1]cuda:0", primals_58: "f32[384][1]cuda:0", primals_59: "f32[384, 384][384, 1]cuda:0", primals_60: "f32[768, 384][384, 1]cuda:0", primals_61: "f32[384, 384][384, 1]cuda:0", primals_62: "f32[384][1]cuda:0", primals_63: "f32[1024, 384][384, 1]cuda:0", primals_64: "f32[384, 1024][1024, 1]cuda:0", primals_65: "f32[384][1]cuda:0", primals_66: "f32[1152, 384][384, 1]cuda:0", primals_67: "f32[384, 384][384, 1]cuda:0", primals_68: "f32[384][1]cuda:0", primals_69: "f32[384, 384][384, 1]cuda:0", primals_70: "f32[768, 384][384, 1]cuda:0", primals_71: "f32[384, 384][384, 1]cuda:0", primals_72: "f32[384][1]cuda:0", primals_73: "f32[1024, 384][384, 1]cuda:0", primals_74: "f32[384, 1024][1024, 1]cuda:0", rsqrt: "f32[s1, 1][1, 1]cuda:0", gt: "b8[s1, 384][384, 1]cuda:0", mul_3: "f32[s1, 384][384, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_5: "i64[][]cuda:0", getitem_6: "i64[][]cuda:0", mm_1: "f32[s1, 384][384, 1]cuda:0", rsqrt_1: "f32[s1, 1][1, 1]cuda:0", mul_5: "f32[s1, 384][384, 1]cuda:0", mm_2: "f32[s1, 1024][1024, 1]cuda:0", gt_1: "b8[s1, 1024][1024, 1]cuda:0", mul_8: "f32[s1, 1024][1024, 1]cuda:0", gt_2: "b8[s1, 384][384, 1]cuda:0", add_3: "f32[s1, 384][384, 1]cuda:0", rsqrt_2: "f32[s1, 1][1, 1]cuda:0", gt_3: "b8[s1, 384][384, 1]cuda:0", mul_14: "f32[s1, 384][384, 1]cuda:0", convert_element_type_2: "i32[257][1]cuda:0", unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_14: "i64[][]cuda:0", getitem_15: "i64[][]cuda:0", add_5: "f32[s1, 384][384, 1]cuda:0", rsqrt_3: "f32[s1, 1][1, 1]cuda:0", mul_16: "f32[s1, 384][384, 1]cuda:0", mm_6: "f32[s1, 1024][1024, 1]cuda:0", gt_4: "b8[s1, 1024][1024, 1]cuda:0", mul_19: "f32[s1, 1024][1024, 1]cuda:0", gt_5: "b8[s1, 384][384, 1]cuda:0", add_7: "f32[s1, 384][384, 1]cuda:0", rsqrt_4: "f32[s1, 1][1, 1]cuda:0", gt_6: "b8[s1, 384][384, 1]cuda:0", mul_25: "f32[s1, 384][384, 1]cuda:0", convert_element_type_4: "i32[257][1]cuda:0", unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_23: "i64[][]cuda:0", getitem_24: "i64[][]cuda:0", add_9: "f32[s1, 384][384, 1]cuda:0", rsqrt_5: "f32[s1, 1][1, 1]cuda:0", mul_27: "f32[s1, 384][384, 1]cuda:0", mm_10: "f32[s1, 1024][1024, 1]cuda:0", gt_7: "b8[s1, 1024][1024, 1]cuda:0", mul_30: "f32[s1, 1024][1024, 1]cuda:0", gt_8: "b8[s1, 384][384, 1]cuda:0", add_11: "f32[s1, 384][384, 1]cuda:0", rsqrt_6: "f32[s1, 1][1, 1]cuda:0", gt_9: "b8[s1, 384][384, 1]cuda:0", mul_36: "f32[s1, 384][384, 1]cuda:0", convert_element_type_6: "i32[257][1]cuda:0", unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_32: "i64[][]cuda:0", getitem_33: "i64[][]cuda:0", add_13: "f32[s1, 384][384, 1]cuda:0", rsqrt_7: "f32[s1, 1][1, 1]cuda:0", mul_38: "f32[s1, 384][384, 1]cuda:0", mm_14: "f32[s1, 1024][1024, 1]cuda:0", gt_10: "b8[s1, 1024][1024, 1]cuda:0", mul_41: "f32[s1, 1024][1024, 1]cuda:0", gt_11: "b8[s1, 384][384, 1]cuda:0", add_15: "f32[s1, 384][384, 1]cuda:0", rsqrt_8: "f32[s6, 1][1, 1]cuda:0", gt_12: "b8[s6, 384][384, 1]cuda:0", mul_47: "f32[s6, 384][384, 1]cuda:0", convert_element_type_8: "i32[257][1]cuda:0", unsqueeze_12: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_13: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_14: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", getitem_39: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_40: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_41: "i64[][]cuda:0", getitem_42: "i64[][]cuda:0", rsqrt_9: "f32[s6, 1][1, 1]cuda:0", gt_13: "b8[s6, 384][384, 1]cuda:0", mul_51: "f32[s6, 384][384, 1]cuda:0", convert_element_type_10: "i32[257][1]cuda:0", convert_element_type_11: "i32[257][1]cuda:0", unsqueeze_15: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", unsqueeze_16: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", unsqueeze_17: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", getitem_47: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_48: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_49: "i64[][]cuda:0", getitem_50: "i64[][]cuda:0", add_19: "f32[s6, 384][384, 1]cuda:0", rsqrt_10: "f32[s6, 1][1, 1]cuda:0", mul_53: "f32[s6, 384][384, 1]cuda:0", mm_21: "f32[s6, 1024][1024, 1]cuda:0", gt_14: "b8[s6, 1024][1024, 1]cuda:0", mul_56: "f32[s6, 1024][1024, 1]cuda:0", gt_15: "b8[s6, 384][384, 1]cuda:0", add_21: "f32[s6, 384][384, 1]cuda:0", rsqrt_11: "f32[s6, 1][1, 1]cuda:0", gt_16: "b8[s6, 384][384, 1]cuda:0", mul_62: "f32[s6, 384][384, 1]cuda:0", convert_element_type_12: "i32[257][1]cuda:0", unsqueeze_18: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_19: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_20: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", getitem_56: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_57: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_58: "i64[][]cuda:0", getitem_59: "i64[][]cuda:0", rsqrt_12: "f32[s6, 1][1, 1]cuda:0", mul_63: "f32[s6, 384][384, 1]cuda:0", gt_17: "b8[s6, 384][384, 1]cuda:0", mul_66: "f32[s6, 384][384, 1]cuda:0", convert_element_type_14: "i32[257][1]cuda:0", convert_element_type_15: "i32[257][1]cuda:0", unsqueeze_21: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", unsqueeze_22: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", unsqueeze_23: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", getitem_64: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_65: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_66: "i64[][]cuda:0", getitem_67: "i64[][]cuda:0", add_25: "f32[s6, 384][384, 1]cuda:0", rsqrt_13: "f32[s6, 1][1, 1]cuda:0", mul_68: "f32[s6, 384][384, 1]cuda:0", mm_28: "f32[s6, 1024][1024, 1]cuda:0", gt_18: "b8[s6, 1024][1024, 1]cuda:0", mul_71: "f32[s6, 1024][1024, 1]cuda:0", gt_19: "b8[s6, 384][384, 1]cuda:0", add_27: "f32[s6, 384][384, 1]cuda:0", rsqrt_14: "f32[s6, 1][1, 1]cuda:0", gt_20: "b8[s6, 384][384, 1]cuda:0", mul_77: "f32[s6, 384][384, 1]cuda:0", convert_element_type_16: "i32[257][1]cuda:0", unsqueeze_24: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_25: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_26: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", getitem_73: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_74: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_75: "i64[][]cuda:0", getitem_76: "i64[][]cuda:0", rsqrt_15: "f32[s6, 1][1, 1]cuda:0", mul_78: "f32[s6, 384][384, 1]cuda:0", gt_21: "b8[s6, 384][384, 1]cuda:0", mul_81: "f32[s6, 384][384, 1]cuda:0", convert_element_type_18: "i32[257][1]cuda:0", convert_element_type_19: "i32[257][1]cuda:0", unsqueeze_27: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", unsqueeze_28: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", unsqueeze_29: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", getitem_81: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_82: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_83: "i64[][]cuda:0", getitem_84: "i64[][]cuda:0", add_31: "f32[s6, 384][384, 1]cuda:0", rsqrt_16: "f32[s6, 1][1, 1]cuda:0", mul_83: "f32[s6, 384][384, 1]cuda:0", mm_35: "f32[s6, 1024][1024, 1]cuda:0", gt_22: "b8[s6, 1024][1024, 1]cuda:0", mul_86: "f32[s6, 1024][1024, 1]cuda:0", gt_23: "b8[s6, 384][384, 1]cuda:0", add_33: "f32[s6, 384][384, 1]cuda:0", rsqrt_17: "f32[s6, 1][1, 1]cuda:0", gt_24: "b8[s6, 384][384, 1]cuda:0", mul_92: "f32[s6, 384][384, 1]cuda:0", convert_element_type_20: "i32[257][1]cuda:0", unsqueeze_30: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_31: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", unsqueeze_32: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0", getitem_90: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_91: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_92: "i64[][]cuda:0", getitem_93: "i64[][]cuda:0", rsqrt_18: "f32[s6, 1][1, 1]cuda:0", mul_93: "f32[s6, 384][384, 1]cuda:0", gt_25: "b8[s6, 384][384, 1]cuda:0", mul_96: "f32[s6, 384][384, 1]cuda:0", convert_element_type_22: "i32[257][1]cuda:0", convert_element_type_23: "i32[257][1]cuda:0", unsqueeze_33: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", unsqueeze_34: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", unsqueeze_35: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0", getitem_98: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0", getitem_99: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0", getitem_100: "i64[][]cuda:0", getitem_101: "i64[][]cuda:0", add_37: "f32[s6, 384][384, 1]cuda:0", rsqrt_19: "f32[s6, 1][1, 1]cuda:0", mul_98: "f32[s6, 384][384, 1]cuda:0", mm_42: "f32[s6, 1024][1024, 1]cuda:0", gt_26: "b8[s6, 1024][1024, 1]cuda:0", mul_101: "f32[s6, 1024][1024, 1]cuda:0", gt_27: "b8[s6, 384][384, 1]cuda:0", tangents_1: "f32[s6, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s8, 0][1, 1]cuda:0", tangents_4: "f32[s9, 0][1, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_27, torch.float32);  gt_27 = None
	        mul_104: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_24, 1.4285714285714286);  convert_element_type_24 = None
	        mul_105: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_104);  mul_104 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_44: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_105, primals_74);  primals_74 = None
	        permute_140: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_105, [1, 0]);  mul_105 = None
	        mm_45: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_140, mul_101);  permute_140 = mul_101 = None
	        convert_element_type_25: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_26, torch.float32);  gt_26 = None
	        mul_106: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_25, 1.4285714285714286);  convert_element_type_25 = None
	        mul_107: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_44, mul_106);  mm_44 = mul_106 = None
	        sigmoid_8: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_42)
	        full_24: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_24, sigmoid_8)
	        mul_108: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, sub);  mm_42 = sub = None
	        add_40: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_108, 1);  mul_108 = None
	        mul_109: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_8, add_40);  sigmoid_8 = add_40 = None
	        mul_110: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_107, mul_109);  mul_107 = mul_109 = None
	        mm_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_110, primals_73);  primals_73 = None
	        permute_142: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_110, [1, 0]);  mul_110 = None
	        mm_47: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_142, mul_98);  permute_142 = mul_98 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_97: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_37, rsqrt_19)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_111: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, mul_97);  mul_97 = None
	        mul_112: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, primals_72);  mm_46 = primals_72 = None
	        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_111, [0], True);  mul_111 = None
	        unsqueeze_36: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view_48: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_36, [384]);  unsqueeze_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_113: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, add_37)
	        mul_114: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, rsqrt_19);  mul_112 = None
	        sum_2: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_113, [1], True);  mul_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_41: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_114);  tangents_1 = mul_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_21: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_19, 3);  rsqrt_19 = None
	        mul_115: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
	        mul_116: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, pow_21);  mul_115 = pow_21 = None
	        expand: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_116, [-1, 384]);  mul_116 = None
	        div: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
	        pow_22: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_37, 1.0);  add_37 = None
	        mul_117: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_22, 2.0);  pow_22 = None
	        mul_118: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_117);  div = mul_117 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_42: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_41, mul_118);  add_41 = mul_118 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_42, primals_71);  primals_71 = None
	        permute_143: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_42, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_11: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0)
	        permute_135: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_11, [1, 0, 2]);  squeeze_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_136: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_135, [1, 0, 2]);  permute_135 = None
	        view_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_136, [sym_size_int_3, 384]);  permute_136 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_49: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_143, view_47);  view_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_49: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_48, [sym_size_int_3, 6, 64]);  mm_48 = None
	        permute_144: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_49, [1, 0, 2]);  view_49 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_145: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_144, [1, 0, 2]);  permute_144 = None
	        unsqueeze_37: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_145, 0);  permute_145 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_37, unsqueeze_33, unsqueeze_34, unsqueeze_35, None, getitem_98, convert_element_type_22, convert_element_type_23, sym_size_int_4, sym_size_int_1, getitem_99, 0.0, getitem_100, getitem_101, 0, False);  unsqueeze_37 = unsqueeze_33 = unsqueeze_34 = unsqueeze_35 = getitem_98 = convert_element_type_22 = convert_element_type_23 = getitem_99 = getitem_100 = getitem_101 = None
	        getitem_104: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_105: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_106: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_106, 0);  getitem_106 = None
	        squeeze_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_105, 0);  getitem_105 = None
	        squeeze_14: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_104, 0);  getitem_104 = None
	        permute_146: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_12, [1, 0, 2]);  squeeze_12 = None
	        permute_147: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_13, [1, 0, 2]);  squeeze_13 = None
	        permute_148: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_14, [1, 0, 2]);  squeeze_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_149: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_146, [1, 0, 2]);  permute_146 = None
	        view_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_149, [sym_size_int, 384]);  permute_149 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_150: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_147, [1, 0, 2]);  permute_147 = None
	        view_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_150, [sym_size_int, 384]);  permute_150 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_151: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_148, [1, 0, 2]);  permute_148 = None
	        view_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_151, [sym_size_int_3, 384]);  permute_151 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        full_25: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 768], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_12 = torch.ops.aten.split.Tensor(full_25, 384, 1)
	        getitem_108: "f32[s1, 384][768, 1]cuda:0" = split_12[0];  split_12 = None
	        copy: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_108, view_51);  view_51 = None
	        slice_scatter: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_25, copy, 1, 0, 384);  copy = None
	        split_15 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_115: "f32[s1, 384][768, 1]cuda:0" = split_15[1];  split_15 = None
	        copy_1: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_115, view_50);  getitem_115 = view_50 = None
	        slice_scatter_1: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        mm_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_1, primals_70);  primals_70 = None
	        permute_153: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_1, [1, 0]);  slice_scatter_1 = None
	        mm_51: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_153, add_15);  permute_153 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_52, primals_69);  primals_69 = None
	        permute_154: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_52, [1, 0]);  view_52 = None
	        mm_53: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_154, mul_96);  permute_154 = mul_96 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_26: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_25, torch.float32);  gt_25 = None
	        mul_119: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_26, 1.4285714285714286);  convert_element_type_26 = None
	        mul_120: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_52, mul_119);  mm_52 = mul_119 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_121: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_120, mul_93);  mul_93 = None
	        mul_122: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_120, primals_68);  mul_120 = primals_68 = None
	        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_121, [0], True);  mul_121 = None
	        unsqueeze_38: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
	        view_53: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_38, [384]);  unsqueeze_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_123: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_122, add_33)
	        mul_124: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_122, rsqrt_18);  mul_122 = None
	        sum_4: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_123, [1], True);  mul_123 = None
	        pow_23: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_18, 3);  rsqrt_18 = None
	        mul_125: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
	        mul_126: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_125, pow_23);  mul_125 = pow_23 = None
	        expand_1: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_126, [-1, 384]);  mul_126 = None
	        div_1: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
	        pow_24: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 1.0)
	        mul_127: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_24, 2.0);  pow_24 = None
	        mul_128: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_127);  div_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_124, mul_128);  mul_124 = mul_128 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_43, add_42);  add_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_54: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_42, primals_67);  add_42 = primals_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_10: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_90, 0)
	        permute_124: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_10, [1, 0, 2]);  squeeze_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_125: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_124, [1, 0, 2]);  permute_124 = None
	        view_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_125, [sym_size_int_3, 384]);  permute_125 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_55: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_143, view_43);  permute_143 = view_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_54: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_54, [sym_size_int_3, 6, 64]);  mm_54 = None
	        permute_156: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_54, [1, 0, 2]);  view_54 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_157: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_156, [1, 0, 2]);  permute_156 = None
	        unsqueeze_39: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_157, 0);  permute_157 = None
	        _efficient_attention_backward_1 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_39, unsqueeze_30, unsqueeze_31, unsqueeze_32, None, getitem_90, convert_element_type_20, convert_element_type_20, sym_size_int_4, sym_size_int_4, getitem_91, 0.0, getitem_92, getitem_93, 1, False);  unsqueeze_39 = unsqueeze_30 = unsqueeze_31 = unsqueeze_32 = getitem_90 = convert_element_type_20 = getitem_91 = getitem_92 = getitem_93 = None
	        getitem_120: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[0]
	        getitem_121: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[1]
	        getitem_122: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[2];  _efficient_attention_backward_1 = None
	        squeeze_15: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_122, 0);  getitem_122 = None
	        squeeze_16: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_121, 0);  getitem_121 = None
	        squeeze_17: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_120, 0);  getitem_120 = None
	        permute_158: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_15, [1, 0, 2]);  squeeze_15 = None
	        permute_159: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_16, [1, 0, 2]);  squeeze_16 = None
	        permute_160: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_17, [1, 0, 2]);  squeeze_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_161: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_158, [1, 0, 2]);  permute_158 = None
	        view_55: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_161, [sym_size_int_3, 384]);  permute_161 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_162: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_159, [1, 0, 2]);  permute_159 = None
	        view_56: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_162, [sym_size_int_3, 384]);  permute_162 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_163: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_160, [1, 0, 2]);  permute_160 = None
	        view_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_163, [sym_size_int_3, 384]);  permute_163 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_26: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_18 = torch.ops.aten.split.Tensor(full_26, 384, 1)
	        getitem_124: "f32[s6, 384][1152, 1]cuda:0" = split_18[0];  split_18 = None
	        copy_2: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_124, view_57);  view_57 = None
	        slice_scatter_2: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_26, copy_2, 1, 0, 384);  copy_2 = None
	        split_21 = torch.ops.aten.split.Tensor(slice_scatter_2, 384, 1)
	        getitem_134: "f32[s6, 384][1152, 1]cuda:0" = split_21[1];  split_21 = None
	        copy_3: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_134, view_56);  getitem_134 = view_56 = None
	        slice_scatter_3: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_2, copy_3, 1, 384, 768);  slice_scatter_2 = copy_3 = None
	        split_24 = torch.ops.aten.split.Tensor(slice_scatter_3, 384, 1)
	        getitem_144: "f32[s6, 384][1152, 1]cuda:0" = split_24[2];  split_24 = None
	        copy_4: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_144, view_55);  getitem_144 = view_55 = None
	        slice_scatter_4: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, copy_4, 1, 768, 1152);  slice_scatter_3 = copy_4 = None
	        mm_56: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_4, primals_66);  primals_66 = None
	        permute_165: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_4, [1, 0]);  slice_scatter_4 = None
	        mm_57: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_165, mul_92);  permute_165 = mul_92 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_24, torch.float32);  gt_24 = None
	        mul_129: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_27, 1.4285714285714286);  convert_element_type_27 = None
	        mul_130: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_56, mul_129);  mm_56 = mul_129 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_89: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_17)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_131: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_130, mul_89);  mul_89 = None
	        mul_132: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_130, primals_65);  mul_130 = primals_65 = None
	        sum_5: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_131, [0], True);  mul_131 = None
	        unsqueeze_40: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_5, 0);  sum_5 = None
	        view_58: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_40, [384]);  unsqueeze_40 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_133: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_132, add_33);  add_33 = None
	        mul_134: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_132, rsqrt_17);  mul_132 = None
	        sum_6: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_133, [1], True);  mul_133 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_45: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_44, mul_134);  add_44 = mul_134 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_25: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_17, 3);  rsqrt_17 = None
	        mul_135: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_6, -0.5);  sum_6 = None
	        mul_136: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_135, pow_25);  mul_135 = pow_25 = None
	        expand_2: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_136, [-1, 384]);  mul_136 = None
	        div_2: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_2, 384);  expand_2 = None
	        mul_138: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_2, mul_127);  div_2 = mul_127 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_45, mul_138);  add_45 = mul_138 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_28: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_23, torch.float32);  gt_23 = None
	        mul_139: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_28, 1.4285714285714286);  convert_element_type_28 = None
	        mul_140: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_46, mul_139);  mul_139 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_58: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_140, primals_64);  primals_64 = None
	        permute_166: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_140, [1, 0]);  mul_140 = None
	        mm_59: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_166, mul_86);  permute_166 = mul_86 = None
	        convert_element_type_29: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_22, torch.float32);  gt_22 = None
	        mul_141: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_29, 1.4285714285714286);  convert_element_type_29 = None
	        mul_142: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_58, mul_141);  mm_58 = mul_141 = None
	        sigmoid_9: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_35)
	        sub_1: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_24, sigmoid_9)
	        mul_143: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_35, sub_1);  mm_35 = sub_1 = None
	        add_47: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_143, 1);  mul_143 = None
	        mul_144: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_9, add_47);  sigmoid_9 = add_47 = None
	        mul_145: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_142, mul_144);  mul_142 = mul_144 = None
	        mm_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_145, primals_63);  primals_63 = None
	        permute_168: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_145, [1, 0]);  mul_145 = None
	        mm_61: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_168, mul_83);  permute_168 = mul_83 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_82: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_31, rsqrt_16)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_146: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_60, mul_82);  mul_82 = None
	        mul_147: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_60, primals_62);  mm_60 = primals_62 = None
	        sum_7: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_146, [0], True);  mul_146 = None
	        unsqueeze_41: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_7, 0);  sum_7 = None
	        view_59: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_41, [384]);  unsqueeze_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_148: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, add_31)
	        mul_149: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_147, rsqrt_16);  mul_147 = None
	        sum_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_148, [1], True);  mul_148 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_46, mul_149);  add_46 = mul_149 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_27: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_16, 3);  rsqrt_16 = None
	        mul_150: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_8, -0.5);  sum_8 = None
	        mul_151: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_150, pow_27);  mul_150 = pow_27 = None
	        expand_3: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_151, [-1, 384]);  mul_151 = None
	        div_3: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_3, 384);  expand_3 = None
	        pow_28: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_31, 1.0);  add_31 = None
	        mul_152: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_28, 2.0);  pow_28 = None
	        mul_153: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_3, mul_152);  div_3 = mul_152 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_49: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_48, mul_153);  add_48 = mul_153 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_49, primals_61);  primals_61 = None
	        permute_169: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_49, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_9: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_81, 0)
	        permute_112: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_9, [1, 0, 2]);  squeeze_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_113: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_112, [1, 0, 2]);  permute_112 = None
	        view_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_113, [sym_size_int_3, 384]);  permute_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_63: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_169, view_39);  view_39 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_60: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_62, [sym_size_int_3, 6, 64]);  mm_62 = None
	        permute_170: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_60, [1, 0, 2]);  view_60 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_171: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_170, [1, 0, 2]);  permute_170 = None
	        unsqueeze_42: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_171, 0);  permute_171 = None
	        _efficient_attention_backward_2 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_42, unsqueeze_27, unsqueeze_28, unsqueeze_29, None, getitem_81, convert_element_type_18, convert_element_type_19, sym_size_int_4, sym_size_int_1, getitem_82, 0.0, getitem_83, getitem_84, 0, False);  unsqueeze_42 = unsqueeze_27 = unsqueeze_28 = unsqueeze_29 = getitem_81 = convert_element_type_18 = convert_element_type_19 = getitem_82 = getitem_83 = getitem_84 = None
	        getitem_151: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[0]
	        getitem_152: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[1]
	        getitem_153: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[2];  _efficient_attention_backward_2 = None
	        squeeze_18: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_153, 0);  getitem_153 = None
	        squeeze_19: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_152, 0);  getitem_152 = None
	        squeeze_20: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_151, 0);  getitem_151 = None
	        permute_172: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_18, [1, 0, 2]);  squeeze_18 = None
	        permute_173: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_19, [1, 0, 2]);  squeeze_19 = None
	        permute_174: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_20, [1, 0, 2]);  squeeze_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_175: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_172, [1, 0, 2]);  permute_172 = None
	        view_61: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_175, [sym_size_int, 384]);  permute_175 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_176: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_173, [1, 0, 2]);  permute_173 = None
	        view_62: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_176, [sym_size_int, 384]);  permute_176 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_177: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_174, [1, 0, 2]);  permute_174 = None
	        view_63: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_177, [sym_size_int_3, 384]);  permute_177 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        copy_5: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_108, view_62);  view_62 = None
	        slice_scatter_5: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_25, copy_5, 1, 0, 384);  copy_5 = None
	        split_30 = torch.ops.aten.split.Tensor(slice_scatter_5, 384, 1)
	        getitem_162: "f32[s1, 384][768, 1]cuda:0" = split_30[1];  split_30 = None
	        copy_6: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_162, view_61);  getitem_162 = view_61 = None
	        slice_scatter_6: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_5, copy_6, 1, 384, 768);  slice_scatter_5 = copy_6 = None
	        mm_64: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_6, primals_60);  primals_60 = None
	        permute_179: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_6, [1, 0]);  slice_scatter_6 = None
	        mm_65: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_179, add_15);  permute_179 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        add_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mm_50, mm_64);  mm_50 = mm_64 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_63, primals_59);  primals_59 = None
	        permute_180: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_63, [1, 0]);  view_63 = None
	        mm_67: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_180, mul_81);  permute_180 = mul_81 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_30: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_21, torch.float32);  gt_21 = None
	        mul_154: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_30, 1.4285714285714286);  convert_element_type_30 = None
	        mul_155: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_66, mul_154);  mm_66 = mul_154 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_156: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_155, mul_78);  mul_78 = None
	        mul_157: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_155, primals_58);  mul_155 = primals_58 = None
	        sum_9: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_156, [0], True);  mul_156 = None
	        unsqueeze_43: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_9, 0);  sum_9 = None
	        view_64: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_43, [384]);  unsqueeze_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_158: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_157, add_27)
	        mul_159: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_157, rsqrt_15);  mul_157 = None
	        sum_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_158, [1], True);  mul_158 = None
	        pow_29: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_15, 3);  rsqrt_15 = None
	        mul_160: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_10, -0.5);  sum_10 = None
	        mul_161: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_160, pow_29);  mul_160 = pow_29 = None
	        expand_4: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_161, [-1, 384]);  mul_161 = None
	        div_4: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_4, 384);  expand_4 = None
	        pow_30: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 1.0)
	        mul_162: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_30, 2.0);  pow_30 = None
	        mul_163: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_4, mul_162);  div_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_51: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_159, mul_163);  mul_159 = mul_163 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_51, add_49);  add_51 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_49, primals_57);  add_49 = primals_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_8: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_73, 0)
	        permute_101: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_8, [1, 0, 2]);  squeeze_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_102: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_101, [1, 0, 2]);  permute_101 = None
	        view_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_102, [sym_size_int_3, 384]);  permute_102 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_69: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_169, view_35);  permute_169 = view_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_65: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_68, [sym_size_int_3, 6, 64]);  mm_68 = None
	        permute_182: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_65, [1, 0, 2]);  view_65 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_183: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_182, [1, 0, 2]);  permute_182 = None
	        unsqueeze_44: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_183, 0);  permute_183 = None
	        _efficient_attention_backward_3 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_44, unsqueeze_24, unsqueeze_25, unsqueeze_26, None, getitem_73, convert_element_type_16, convert_element_type_16, sym_size_int_4, sym_size_int_4, getitem_74, 0.0, getitem_75, getitem_76, 1, False);  unsqueeze_44 = unsqueeze_24 = unsqueeze_25 = unsqueeze_26 = getitem_73 = convert_element_type_16 = getitem_74 = getitem_75 = getitem_76 = None
	        getitem_167: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[0]
	        getitem_168: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[1]
	        getitem_169: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[2];  _efficient_attention_backward_3 = None
	        squeeze_21: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_169, 0);  getitem_169 = None
	        squeeze_22: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_168, 0);  getitem_168 = None
	        squeeze_23: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_167, 0);  getitem_167 = None
	        permute_184: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_21, [1, 0, 2]);  squeeze_21 = None
	        permute_185: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_22, [1, 0, 2]);  squeeze_22 = None
	        permute_186: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_23, [1, 0, 2]);  squeeze_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_187: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_184, [1, 0, 2]);  permute_184 = None
	        view_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_187, [sym_size_int_3, 384]);  permute_187 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_188: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_185, [1, 0, 2]);  permute_185 = None
	        view_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_188, [sym_size_int_3, 384]);  permute_188 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_189: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_186, [1, 0, 2]);  permute_186 = None
	        view_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_189, [sym_size_int_3, 384]);  permute_189 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_7: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_124, view_68);  view_68 = None
	        slice_scatter_7: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_26, copy_7, 1, 0, 384);  copy_7 = None
	        split_36 = torch.ops.aten.split.Tensor(slice_scatter_7, 384, 1)
	        getitem_181: "f32[s6, 384][1152, 1]cuda:0" = split_36[1];  split_36 = None
	        copy_8: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_181, view_67);  getitem_181 = view_67 = None
	        slice_scatter_8: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_7, copy_8, 1, 384, 768);  slice_scatter_7 = copy_8 = None
	        split_39 = torch.ops.aten.split.Tensor(slice_scatter_8, 384, 1)
	        getitem_191: "f32[s6, 384][1152, 1]cuda:0" = split_39[2];  split_39 = None
	        copy_9: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_191, view_66);  getitem_191 = view_66 = None
	        slice_scatter_9: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_8, copy_9, 1, 768, 1152);  slice_scatter_8 = copy_9 = None
	        mm_70: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_9, primals_56);  primals_56 = None
	        permute_191: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_9, [1, 0]);  slice_scatter_9 = None
	        mm_71: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_191, mul_77);  permute_191 = mul_77 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_20, torch.float32);  gt_20 = None
	        mul_164: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_31, 1.4285714285714286);  convert_element_type_31 = None
	        mul_165: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_70, mul_164);  mm_70 = mul_164 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_14)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_166: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_165, mul_74);  mul_74 = None
	        mul_167: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_165, primals_55);  mul_165 = primals_55 = None
	        sum_11: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_166, [0], True);  mul_166 = None
	        unsqueeze_45: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_11, 0);  sum_11 = None
	        view_69: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_45, [384]);  unsqueeze_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_168: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_167, add_27);  add_27 = None
	        mul_169: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_167, rsqrt_14);  mul_167 = None
	        sum_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_168, [1], True);  mul_168 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_53: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_52, mul_169);  add_52 = mul_169 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_31: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_14, 3);  rsqrt_14 = None
	        mul_170: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_12, -0.5);  sum_12 = None
	        mul_171: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_170, pow_31);  mul_170 = pow_31 = None
	        expand_5: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_171, [-1, 384]);  mul_171 = None
	        div_5: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_5, 384);  expand_5 = None
	        mul_173: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_5, mul_162);  div_5 = mul_162 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_54: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_53, mul_173);  add_53 = mul_173 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_32: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_19, torch.float32);  gt_19 = None
	        mul_174: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_32, 1.4285714285714286);  convert_element_type_32 = None
	        mul_175: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_54, mul_174);  mul_174 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_72: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_175, primals_54);  primals_54 = None
	        permute_192: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_175, [1, 0]);  mul_175 = None
	        mm_73: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_192, mul_71);  permute_192 = mul_71 = None
	        convert_element_type_33: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_18, torch.float32);  gt_18 = None
	        mul_176: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_33, 1.4285714285714286);  convert_element_type_33 = None
	        mul_177: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_72, mul_176);  mm_72 = mul_176 = None
	        sigmoid_10: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_28)
	        sub_2: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_24, sigmoid_10)
	        mul_178: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_28, sub_2);  mm_28 = sub_2 = None
	        add_55: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_178, 1);  mul_178 = None
	        mul_179: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_10, add_55);  sigmoid_10 = add_55 = None
	        mul_180: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_177, mul_179);  mul_177 = mul_179 = None
	        mm_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_180, primals_53);  primals_53 = None
	        permute_194: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_180, [1, 0]);  mul_180 = None
	        mm_75: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_194, mul_68);  permute_194 = mul_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, rsqrt_13)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_181: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_74, mul_67);  mul_67 = None
	        mul_182: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_74, primals_52);  mm_74 = primals_52 = None
	        sum_13: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_181, [0], True);  mul_181 = None
	        unsqueeze_46: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_13, 0);  sum_13 = None
	        view_70: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_46, [384]);  unsqueeze_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_183: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_182, add_25)
	        mul_184: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_182, rsqrt_13);  mul_182 = None
	        sum_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_183, [1], True);  mul_183 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_56: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_54, mul_184);  add_54 = mul_184 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_33: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_13, 3);  rsqrt_13 = None
	        mul_185: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_14, -0.5);  sum_14 = None
	        mul_186: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_185, pow_33);  mul_185 = pow_33 = None
	        expand_6: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_186, [-1, 384]);  mul_186 = None
	        div_6: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_6, 384);  expand_6 = None
	        pow_34: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_25, 1.0);  add_25 = None
	        mul_187: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_34, 2.0);  pow_34 = None
	        mul_188: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_6, mul_187);  div_6 = mul_187 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_56, mul_188);  add_56 = mul_188 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_76: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_57, primals_51);  primals_51 = None
	        permute_195: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_57, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_7: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_64, 0)
	        permute_89: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_7, [1, 0, 2]);  squeeze_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_90: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_89, [1, 0, 2]);  permute_89 = None
	        view_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_90, [sym_size_int_3, 384]);  permute_90 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_77: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_195, view_31);  view_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_71: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_76, [sym_size_int_3, 6, 64]);  mm_76 = None
	        permute_196: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_71, [1, 0, 2]);  view_71 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_197: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_196, [1, 0, 2]);  permute_196 = None
	        unsqueeze_47: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_197, 0);  permute_197 = None
	        _efficient_attention_backward_4 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_47, unsqueeze_21, unsqueeze_22, unsqueeze_23, None, getitem_64, convert_element_type_14, convert_element_type_15, sym_size_int_4, sym_size_int_1, getitem_65, 0.0, getitem_66, getitem_67, 0, False);  unsqueeze_47 = unsqueeze_21 = unsqueeze_22 = unsqueeze_23 = getitem_64 = convert_element_type_14 = convert_element_type_15 = getitem_65 = getitem_66 = getitem_67 = None
	        getitem_198: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_4[0]
	        getitem_199: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_4[1]
	        getitem_200: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_4[2];  _efficient_attention_backward_4 = None
	        squeeze_24: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_200, 0);  getitem_200 = None
	        squeeze_25: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_199, 0);  getitem_199 = None
	        squeeze_26: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_198, 0);  getitem_198 = None
	        permute_198: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_24, [1, 0, 2]);  squeeze_24 = None
	        permute_199: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_25, [1, 0, 2]);  squeeze_25 = None
	        permute_200: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_26, [1, 0, 2]);  squeeze_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_201: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_198, [1, 0, 2]);  permute_198 = None
	        view_72: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_201, [sym_size_int, 384]);  permute_201 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_202: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_199, [1, 0, 2]);  permute_199 = None
	        view_73: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_202, [sym_size_int, 384]);  permute_202 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_203: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_200, [1, 0, 2]);  permute_200 = None
	        view_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_203, [sym_size_int_3, 384]);  permute_203 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        copy_10: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_108, view_73);  view_73 = None
	        slice_scatter_10: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_25, copy_10, 1, 0, 384);  copy_10 = None
	        split_45 = torch.ops.aten.split.Tensor(slice_scatter_10, 384, 1)
	        getitem_209: "f32[s1, 384][768, 1]cuda:0" = split_45[1];  split_45 = None
	        copy_11: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_209, view_72);  getitem_209 = view_72 = None
	        slice_scatter_11: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, copy_11, 1, 384, 768);  slice_scatter_10 = copy_11 = None
	        mm_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_11, primals_50);  primals_50 = None
	        permute_205: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_11, [1, 0]);  slice_scatter_11 = None
	        mm_79: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_205, add_15);  permute_205 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        add_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_50, mm_78);  add_50 = mm_78 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_80: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_74, primals_49);  primals_49 = None
	        permute_206: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_74, [1, 0]);  view_74 = None
	        mm_81: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_206, mul_66);  permute_206 = mul_66 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_34: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_17, torch.float32);  gt_17 = None
	        mul_189: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_34, 1.4285714285714286);  convert_element_type_34 = None
	        mul_190: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_80, mul_189);  mm_80 = mul_189 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_191: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_190, mul_63);  mul_63 = None
	        mul_192: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_190, primals_48);  mul_190 = primals_48 = None
	        sum_15: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_191, [0], True);  mul_191 = None
	        unsqueeze_48: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_15, 0);  sum_15 = None
	        view_75: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_48, [384]);  unsqueeze_48 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_193: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_192, add_21)
	        mul_194: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_192, rsqrt_12);  mul_192 = None
	        sum_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_193, [1], True);  mul_193 = None
	        pow_35: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_12, 3);  rsqrt_12 = None
	        mul_195: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_16, -0.5);  sum_16 = None
	        mul_196: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_195, pow_35);  mul_195 = pow_35 = None
	        expand_7: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_196, [-1, 384]);  mul_196 = None
	        div_7: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_7, 384);  expand_7 = None
	        pow_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 1.0)
	        mul_197: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_36, 2.0);  pow_36 = None
	        mul_198: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_7, mul_197);  div_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_59: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_194, mul_198);  mul_194 = mul_198 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_59, add_57);  add_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_82: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_57, primals_47);  add_57 = primals_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_6: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_56, 0)
	        permute_78: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_6, [1, 0, 2]);  squeeze_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_79: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_78, [1, 0, 2]);  permute_78 = None
	        view_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_79, [sym_size_int_3, 384]);  permute_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_83: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_195, view_27);  permute_195 = view_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_76: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_82, [sym_size_int_3, 6, 64]);  mm_82 = None
	        permute_208: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_76, [1, 0, 2]);  view_76 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_209: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_208, [1, 0, 2]);  permute_208 = None
	        unsqueeze_49: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_209, 0);  permute_209 = None
	        _efficient_attention_backward_5 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_49, unsqueeze_18, unsqueeze_19, unsqueeze_20, None, getitem_56, convert_element_type_12, convert_element_type_12, sym_size_int_4, sym_size_int_4, getitem_57, 0.0, getitem_58, getitem_59, 1, False);  unsqueeze_49 = unsqueeze_18 = unsqueeze_19 = unsqueeze_20 = getitem_56 = convert_element_type_12 = getitem_57 = getitem_58 = getitem_59 = None
	        getitem_214: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_5[0]
	        getitem_215: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_5[1]
	        getitem_216: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_5[2];  _efficient_attention_backward_5 = None
	        squeeze_27: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_216, 0);  getitem_216 = None
	        squeeze_28: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_215, 0);  getitem_215 = None
	        squeeze_29: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_214, 0);  getitem_214 = None
	        permute_210: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_27, [1, 0, 2]);  squeeze_27 = None
	        permute_211: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_28, [1, 0, 2]);  squeeze_28 = None
	        permute_212: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_29, [1, 0, 2]);  squeeze_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_213: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_210, [1, 0, 2]);  permute_210 = None
	        view_77: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_213, [sym_size_int_3, 384]);  permute_213 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_214: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_211, [1, 0, 2]);  permute_211 = None
	        view_78: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_214, [sym_size_int_3, 384]);  permute_214 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_215: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_212, [1, 0, 2]);  permute_212 = None
	        view_79: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_215, [sym_size_int_3, 384]);  permute_215 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_12: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_124, view_79);  view_79 = None
	        slice_scatter_12: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_26, copy_12, 1, 0, 384);  copy_12 = None
	        split_51 = torch.ops.aten.split.Tensor(slice_scatter_12, 384, 1)
	        getitem_228: "f32[s6, 384][1152, 1]cuda:0" = split_51[1];  split_51 = None
	        copy_13: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_228, view_78);  getitem_228 = view_78 = None
	        slice_scatter_13: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_12, copy_13, 1, 384, 768);  slice_scatter_12 = copy_13 = None
	        split_54 = torch.ops.aten.split.Tensor(slice_scatter_13, 384, 1)
	        getitem_238: "f32[s6, 384][1152, 1]cuda:0" = split_54[2];  split_54 = None
	        copy_14: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_238, view_77);  getitem_238 = view_77 = None
	        slice_scatter_14: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_13, copy_14, 1, 768, 1152);  slice_scatter_13 = copy_14 = None
	        mm_84: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_14, primals_46);  primals_46 = None
	        permute_217: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_14, [1, 0]);  slice_scatter_14 = None
	        mm_85: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_217, mul_62);  permute_217 = mul_62 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_16, torch.float32);  gt_16 = None
	        mul_199: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_35, 1.4285714285714286);  convert_element_type_35 = None
	        mul_200: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_84, mul_199);  mm_84 = mul_199 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_59: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_11)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_201: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_200, mul_59);  mul_59 = None
	        mul_202: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_200, primals_45);  mul_200 = primals_45 = None
	        sum_17: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_201, [0], True);  mul_201 = None
	        unsqueeze_50: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_17, 0);  sum_17 = None
	        view_80: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_50, [384]);  unsqueeze_50 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_203: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_202, add_21);  add_21 = None
	        mul_204: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_202, rsqrt_11);  mul_202 = None
	        sum_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_203, [1], True);  mul_203 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_61: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_60, mul_204);  add_60 = mul_204 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_37: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_11, 3);  rsqrt_11 = None
	        mul_205: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_18, -0.5);  sum_18 = None
	        mul_206: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_205, pow_37);  mul_205 = pow_37 = None
	        expand_8: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_206, [-1, 384]);  mul_206 = None
	        div_8: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_8, 384);  expand_8 = None
	        mul_208: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_8, mul_197);  div_8 = mul_197 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_61, mul_208);  add_61 = mul_208 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_15, torch.float32);  gt_15 = None
	        mul_209: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_36, 1.4285714285714286);  convert_element_type_36 = None
	        mul_210: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_62, mul_209);  mul_209 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_86: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_210, primals_44);  primals_44 = None
	        permute_218: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_210, [1, 0]);  mul_210 = None
	        mm_87: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_218, mul_56);  permute_218 = mul_56 = None
	        convert_element_type_37: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_14, torch.float32);  gt_14 = None
	        mul_211: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_37, 1.4285714285714286);  convert_element_type_37 = None
	        mul_212: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_86, mul_211);  mm_86 = mul_211 = None
	        sigmoid_11: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_21)
	        sub_3: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_24, sigmoid_11);  full_24 = None
	        mul_213: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_21, sub_3);  mm_21 = sub_3 = None
	        add_63: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_213, 1);  mul_213 = None
	        mul_214: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_11, add_63);  sigmoid_11 = add_63 = None
	        mul_215: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_212, mul_214);  mul_212 = mul_214 = None
	        mm_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_215, primals_43);  primals_43 = None
	        permute_220: "f32[1024, s6][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_215, [1, 0]);  mul_215 = None
	        mm_89: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_220, mul_53);  permute_220 = mul_53 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_19, rsqrt_10)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_216: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_88, mul_52);  mul_52 = None
	        mul_217: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_88, primals_42);  mm_88 = primals_42 = None
	        sum_19: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_216, [0], True);  mul_216 = None
	        unsqueeze_51: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_19, 0);  sum_19 = None
	        view_81: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_51, [384]);  unsqueeze_51 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_218: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_217, add_19)
	        mul_219: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_217, rsqrt_10);  mul_217 = None
	        sum_20: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_218, [1], True);  mul_218 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_64: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_62, mul_219);  add_62 = mul_219 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_39: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_10, 3);  rsqrt_10 = None
	        mul_220: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_20, -0.5);  sum_20 = None
	        mul_221: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_220, pow_39);  mul_220 = pow_39 = None
	        expand_9: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_221, [-1, 384]);  mul_221 = None
	        div_9: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_9, 384);  expand_9 = None
	        pow_40: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_19, 1.0);  add_19 = None
	        mul_222: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_40, 2.0);  pow_40 = None
	        mul_223: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_9, mul_222);  div_9 = mul_222 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_65: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_64, mul_223);  add_64 = mul_223 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_65, primals_41);  primals_41 = None
	        permute_221: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(add_65, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_5: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_47, 0)
	        permute_66: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_5, [1, 0, 2]);  squeeze_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_67: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_66, [1, 0, 2]);  permute_66 = None
	        view_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_67, [sym_size_int_3, 384]);  permute_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_91: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_221, view_23);  view_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_82: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_90, [sym_size_int_3, 6, 64]);  mm_90 = None
	        permute_222: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_82, [1, 0, 2]);  view_82 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_223: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_222, [1, 0, 2]);  permute_222 = None
	        unsqueeze_52: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_223, 0);  permute_223 = None
	        _efficient_attention_backward_6 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_52, unsqueeze_15, unsqueeze_16, unsqueeze_17, None, getitem_47, convert_element_type_10, convert_element_type_11, sym_size_int_4, sym_size_int_1, getitem_48, 0.0, getitem_49, getitem_50, 0, False);  unsqueeze_52 = unsqueeze_15 = unsqueeze_16 = unsqueeze_17 = getitem_47 = convert_element_type_10 = convert_element_type_11 = getitem_48 = getitem_49 = getitem_50 = None
	        getitem_245: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_6[0]
	        getitem_246: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_6[1]
	        getitem_247: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_6[2];  _efficient_attention_backward_6 = None
	        squeeze_30: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_247, 0);  getitem_247 = None
	        squeeze_31: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_246, 0);  getitem_246 = None
	        squeeze_32: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_245, 0);  getitem_245 = None
	        permute_224: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_30, [1, 0, 2]);  squeeze_30 = None
	        permute_225: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_31, [1, 0, 2]);  squeeze_31 = None
	        permute_226: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_32, [1, 0, 2]);  squeeze_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_227: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_224, [1, 0, 2]);  permute_224 = None
	        view_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_227, [sym_size_int, 384]);  permute_227 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_228: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_225, [1, 0, 2]);  permute_225 = None
	        view_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_228, [sym_size_int, 384]);  permute_228 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_229: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_226, [1, 0, 2]);  permute_226 = None
	        view_85: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_229, [sym_size_int_3, 384]);  permute_229 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        copy_15: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_108, view_84);  getitem_108 = view_84 = None
	        slice_scatter_15: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_25, copy_15, 1, 0, 384);  full_25 = copy_15 = None
	        split_60 = torch.ops.aten.split.Tensor(slice_scatter_15, 384, 1)
	        getitem_256: "f32[s1, 384][768, 1]cuda:0" = split_60[1];  split_60 = None
	        copy_16: "f32[s1, 384][768, 1]cuda:0" = torch.ops.aten.copy.default(getitem_256, view_83);  getitem_256 = view_83 = None
	        slice_scatter_16: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_15, copy_16, 1, 384, 768);  slice_scatter_15 = copy_16 = None
	        mm_92: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_16, primals_40);  primals_40 = None
	        permute_231: "f32[768, s1][1, 768]cuda:0" = torch.ops.aten.permute.default(slice_scatter_16, [1, 0]);  slice_scatter_16 = None
	        mm_93: "f32[768, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_231, add_15);  permute_231 = add_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        add_66: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_58, mm_92);  add_58 = mm_92 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        mm_94: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_85, primals_39);  primals_39 = None
	        permute_232: "f32[384, s6][1, 384]cuda:0" = torch.ops.aten.permute.default(view_85, [1, 0]);  view_85 = None
	        mm_95: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_232, mul_51);  permute_232 = mul_51 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        convert_element_type_38: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_13, torch.float32);  gt_13 = None
	        mul_224: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_38, 1.4285714285714286);  convert_element_type_38 = None
	        mul_225: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_94, mul_224);  mm_94 = mul_224 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_226: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_225, mul_48);  mul_48 = None
	        mul_227: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_225, primals_38);  mul_225 = primals_38 = None
	        sum_21: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_226, [0], True);  mul_226 = None
	        unsqueeze_53: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_21, 0);  sum_21 = None
	        view_86: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_53, [384]);  unsqueeze_53 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_228: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_227, primals_30)
	        mul_229: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_227, rsqrt_9);  mul_227 = None
	        sum_22: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_228, [1], True);  mul_228 = None
	        pow_41: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_9, 3);  rsqrt_9 = None
	        mul_230: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_22, -0.5);  sum_22 = None
	        mul_231: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_230, pow_41);  mul_230 = pow_41 = None
	        expand_10: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_231, [-1, 384]);  mul_231 = None
	        div_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_10, 384);  expand_10 = None
	        pow_42: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 1.0)
	        mul_232: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_42, 2.0);  pow_42 = None
	        mul_233: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_10, mul_232);  div_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_229, mul_233);  mul_229 = mul_233 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_67, add_65);  add_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_96: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_65, primals_37);  add_65 = primals_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_4: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_39, 0)
	        permute_55: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_4, [1, 0, 2]);  squeeze_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_56: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_55, [1, 0, 2]);  permute_55 = None
	        view_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_56, [sym_size_int_3, 384]);  permute_56 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_97: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_221, view_19);  permute_221 = view_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_87: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_96, [sym_size_int_3, 6, 64]);  mm_96 = None
	        permute_234: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_87, [1, 0, 2]);  view_87 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_235: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_234, [1, 0, 2]);  permute_234 = None
	        unsqueeze_54: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_235, 0);  permute_235 = None
	        _efficient_attention_backward_7 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_54, unsqueeze_12, unsqueeze_13, unsqueeze_14, None, getitem_39, convert_element_type_8, convert_element_type_8, sym_size_int_4, sym_size_int_4, getitem_40, 0.0, getitem_41, getitem_42, 1, False);  unsqueeze_54 = unsqueeze_12 = unsqueeze_13 = unsqueeze_14 = getitem_39 = convert_element_type_8 = sym_size_int_4 = getitem_40 = getitem_41 = getitem_42 = None
	        getitem_261: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_7[0]
	        getitem_262: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_7[1]
	        getitem_263: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_backward_7[2];  _efficient_attention_backward_7 = None
	        squeeze_33: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_263, 0);  getitem_263 = None
	        squeeze_34: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_262, 0);  getitem_262 = None
	        squeeze_35: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_261, 0);  getitem_261 = None
	        permute_236: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_33, [1, 0, 2]);  squeeze_33 = None
	        permute_237: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_34, [1, 0, 2]);  squeeze_34 = None
	        permute_238: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_35, [1, 0, 2]);  squeeze_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_239: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_236, [1, 0, 2]);  permute_236 = None
	        view_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_239, [sym_size_int_3, 384]);  permute_239 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_240: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_237, [1, 0, 2]);  permute_237 = None
	        view_89: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_240, [sym_size_int_3, 384]);  permute_240 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_241: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_238, [1, 0, 2]);  permute_238 = None
	        view_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_241, [sym_size_int_3, 384]);  permute_241 = sym_size_int_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_17: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_124, view_90);  getitem_124 = view_90 = None
	        slice_scatter_17: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_26, copy_17, 1, 0, 384);  full_26 = copy_17 = None
	        split_66 = torch.ops.aten.split.Tensor(slice_scatter_17, 384, 1)
	        getitem_275: "f32[s6, 384][1152, 1]cuda:0" = split_66[1];  split_66 = None
	        copy_18: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_275, view_89);  getitem_275 = view_89 = None
	        slice_scatter_18: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_17, copy_18, 1, 384, 768);  slice_scatter_17 = copy_18 = None
	        split_69 = torch.ops.aten.split.Tensor(slice_scatter_18, 384, 1)
	        getitem_285: "f32[s6, 384][1152, 1]cuda:0" = split_69[2];  split_69 = None
	        copy_19: "f32[s6, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_285, view_88);  getitem_285 = view_88 = None
	        slice_scatter_19: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_18, copy_19, 1, 768, 1152);  slice_scatter_18 = copy_19 = None
	        mm_98: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_19, primals_36);  primals_36 = None
	        permute_243: "f32[1152, s6][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_19, [1, 0]);  slice_scatter_19 = None
	        mm_99: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_243, mul_47);  permute_243 = mul_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_12, torch.float32);  gt_12 = None
	        mul_234: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_39, 1.4285714285714286);  convert_element_type_39 = None
	        mul_235: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_98, mul_234);  mm_98 = mul_234 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_8)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_236: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_235, mul_44);  mul_44 = None
	        mul_237: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_235, primals_35);  mul_235 = primals_35 = None
	        sum_23: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_236, [0], True);  mul_236 = None
	        unsqueeze_55: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_23, 0);  sum_23 = None
	        view_91: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_55, [384]);  unsqueeze_55 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_238: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_237, primals_30);  primals_30 = None
	        mul_239: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_237, rsqrt_8);  mul_237 = None
	        sum_24: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_238, [1], True);  mul_238 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_69: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_68, mul_239);  add_68 = mul_239 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_43: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_8, 3);  rsqrt_8 = None
	        mul_240: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_24, -0.5);  sum_24 = None
	        mul_241: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_240, pow_43);  mul_240 = pow_43 = None
	        expand_11: "f32[s6, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_241, [-1, 384]);  mul_241 = None
	        div_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_11, 384);  expand_11 = None
	        mul_243: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_11, mul_232);  div_11 = mul_232 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_70: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_69, mul_243);  add_69 = mul_243 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_40: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_11, torch.float32);  gt_11 = None
	        mul_244: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_40, 1.4285714285714286);  convert_element_type_40 = None
	        mul_245: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_66, mul_244);  mul_244 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_100: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_245, primals_29);  primals_29 = None
	        permute_244: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_245, [1, 0]);  mul_245 = None
	        mm_101: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_244, mul_41);  permute_244 = mul_41 = None
	        convert_element_type_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_10, torch.float32);  gt_10 = None
	        mul_246: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_41, 1.4285714285714286);  convert_element_type_41 = None
	        mul_247: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_100, mul_246);  mm_100 = mul_246 = None
	        sigmoid_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        full_36: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_36, sigmoid_12)
	        mul_248: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sub_4);  mm_14 = sub_4 = None
	        add_71: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_248, 1);  mul_248 = None
	        mul_249: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_12, add_71);  sigmoid_12 = add_71 = None
	        mul_250: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_247, mul_249);  mul_247 = mul_249 = None
	        mm_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_250, primals_28);  primals_28 = None
	        permute_246: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_250, [1, 0]);  mul_250 = None
	        mm_103: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_246, mul_38);  permute_246 = mul_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_251: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_102, mul_37);  mul_37 = None
	        mul_252: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_102, primals_27);  mm_102 = primals_27 = None
	        sum_25: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_251, [0], True);  mul_251 = None
	        unsqueeze_56: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_25, 0);  sum_25 = None
	        view_92: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_56, [384]);  unsqueeze_56 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_253: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_252, add_13)
	        mul_254: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_252, rsqrt_7);  mul_252 = None
	        sum_26: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_253, [1], True);  mul_253 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_72: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_66, mul_254);  add_66 = mul_254 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_45: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_7, 3);  rsqrt_7 = None
	        mul_255: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_26, -0.5);  sum_26 = None
	        mul_256: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_255, pow_45);  mul_255 = pow_45 = None
	        expand_12: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_256, [-1, 384]);  mul_256 = None
	        div_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_12, 384);  expand_12 = None
	        pow_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 1.0);  add_13 = None
	        mul_257: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_46, 2.0);  pow_46 = None
	        mul_258: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_12, mul_257);  div_12 = mul_257 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_73: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_72, mul_258);  add_72 = mul_258 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_104: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_73, primals_26);  primals_26 = None
	        permute_247: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_73, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_105: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_247, view_15);  permute_247 = view_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_93: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_104, [sym_size_int, 6, 64]);  mm_104 = None
	        permute_248: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_93, [1, 0, 2]);  view_93 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_249: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_248, [1, 0, 2]);  permute_248 = None
	        unsqueeze_57: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_249, 0);  permute_249 = None
	        _efficient_attention_backward_8 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_57, unsqueeze_9, unsqueeze_10, unsqueeze_11, None, getitem_30, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, getitem_31, 0.0, getitem_32, getitem_33, 0, False);  unsqueeze_57 = unsqueeze_9 = unsqueeze_10 = unsqueeze_11 = getitem_30 = convert_element_type_6 = getitem_31 = getitem_32 = getitem_33 = None
	        getitem_292: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_8[0]
	        getitem_293: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_8[1]
	        getitem_294: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_8[2];  _efficient_attention_backward_8 = None
	        squeeze_36: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_294, 0);  getitem_294 = None
	        squeeze_37: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_293, 0);  getitem_293 = None
	        squeeze_38: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_292, 0);  getitem_292 = None
	        permute_250: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_36, [1, 0, 2]);  squeeze_36 = None
	        permute_251: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_37, [1, 0, 2]);  squeeze_37 = None
	        permute_252: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_38, [1, 0, 2]);  squeeze_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_253: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_250, [1, 0, 2]);  permute_250 = None
	        view_94: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_253, [sym_size_int, 384]);  permute_253 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_254: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_251, [1, 0, 2]);  permute_251 = None
	        view_95: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_254, [sym_size_int, 384]);  permute_254 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_255: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_252, [1, 0, 2]);  permute_252 = None
	        view_96: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_255, [sym_size_int, 384]);  permute_255 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_37: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_72 = torch.ops.aten.split.Tensor(full_37, 384, 1)
	        getitem_296: "f32[s1, 384][1152, 1]cuda:0" = split_72[0];  split_72 = None
	        copy_20: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_296, view_96);  view_96 = None
	        slice_scatter_20: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_37, copy_20, 1, 0, 384);  copy_20 = None
	        split_75 = torch.ops.aten.split.Tensor(slice_scatter_20, 384, 1)
	        getitem_306: "f32[s1, 384][1152, 1]cuda:0" = split_75[1];  split_75 = None
	        copy_21: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_306, view_95);  getitem_306 = view_95 = None
	        slice_scatter_21: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_20, copy_21, 1, 384, 768);  slice_scatter_20 = copy_21 = None
	        split_78 = torch.ops.aten.split.Tensor(slice_scatter_21, 384, 1)
	        getitem_316: "f32[s1, 384][1152, 1]cuda:0" = split_78[2];  split_78 = None
	        copy_22: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_316, view_94);  getitem_316 = view_94 = None
	        slice_scatter_22: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_21, copy_22, 1, 768, 1152);  slice_scatter_21 = copy_22 = None
	        mm_106: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_22, primals_25);  primals_25 = None
	        permute_257: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_22, [1, 0]);  slice_scatter_22 = None
	        mm_107: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_257, mul_36);  permute_257 = mul_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_9, torch.float32);  gt_9 = None
	        mul_259: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_42, 1.4285714285714286);  convert_element_type_42 = None
	        mul_260: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_106, mul_259);  mm_106 = mul_259 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_261: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_260, mul_33);  mul_33 = None
	        mul_262: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_260, primals_24);  mul_260 = primals_24 = None
	        sum_27: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_261, [0], True);  mul_261 = None
	        unsqueeze_58: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_27, 0);  sum_27 = None
	        view_97: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_58, [384]);  unsqueeze_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_263: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_262, add_11)
	        mul_264: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_262, rsqrt_6);  mul_262 = None
	        sum_28: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_263, [1], True);  mul_263 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_74: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_73, mul_264);  add_73 = mul_264 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_47: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_6, 3);  rsqrt_6 = None
	        mul_265: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_28, -0.5);  sum_28 = None
	        mul_266: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_265, pow_47);  mul_265 = pow_47 = None
	        expand_13: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_266, [-1, 384]);  mul_266 = None
	        div_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_13, 384);  expand_13 = None
	        pow_48: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 1.0);  add_11 = None
	        mul_267: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_48, 2.0);  pow_48 = None
	        mul_268: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_13, mul_267);  div_13 = mul_267 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_75: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_74, mul_268);  add_74 = mul_268 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_8, torch.float32);  gt_8 = None
	        mul_269: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_43, 1.4285714285714286);  convert_element_type_43 = None
	        mul_270: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_75, mul_269);  mul_269 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_108: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_270, primals_23);  primals_23 = None
	        permute_258: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_270, [1, 0]);  mul_270 = None
	        mm_109: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_258, mul_30);  permute_258 = mul_30 = None
	        convert_element_type_44: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_7, torch.float32);  gt_7 = None
	        mul_271: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_44, 1.4285714285714286);  convert_element_type_44 = None
	        mul_272: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_108, mul_271);  mm_108 = mul_271 = None
	        sigmoid_13: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        sub_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_36, sigmoid_13)
	        mul_273: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sub_5);  mm_10 = sub_5 = None
	        add_76: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_273, 1);  mul_273 = None
	        mul_274: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_13, add_76);  sigmoid_13 = add_76 = None
	        mul_275: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_272, mul_274);  mul_272 = mul_274 = None
	        mm_110: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_275, primals_22);  primals_22 = None
	        permute_260: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_275, [1, 0]);  mul_275 = None
	        mm_111: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_260, mul_27);  permute_260 = mul_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_276: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_110, mul_26);  mul_26 = None
	        mul_277: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_110, primals_21);  mm_110 = primals_21 = None
	        sum_29: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_276, [0], True);  mul_276 = None
	        unsqueeze_59: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_29, 0);  sum_29 = None
	        view_98: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_59, [384]);  unsqueeze_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_278: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_277, add_9)
	        mul_279: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_277, rsqrt_5);  mul_277 = None
	        sum_30: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_278, [1], True);  mul_278 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_77: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_75, mul_279);  add_75 = mul_279 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_49: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_5, 3);  rsqrt_5 = None
	        mul_280: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_30, -0.5);  sum_30 = None
	        mul_281: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_280, pow_49);  mul_280 = pow_49 = None
	        expand_14: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_281, [-1, 384]);  mul_281 = None
	        div_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_14, 384);  expand_14 = None
	        pow_50: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 1.0);  add_9 = None
	        mul_282: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_50, 2.0);  pow_50 = None
	        mul_283: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_14, mul_282);  div_14 = mul_282 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_77, mul_283);  add_77 = mul_283 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_78, primals_20);  primals_20 = None
	        permute_261: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_78, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_113: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_261, view_11);  permute_261 = view_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_99: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_112, [sym_size_int, 6, 64]);  mm_112 = None
	        permute_262: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_99, [1, 0, 2]);  view_99 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_263: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_262, [1, 0, 2]);  permute_262 = None
	        unsqueeze_60: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_263, 0);  permute_263 = None
	        _efficient_attention_backward_9 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_60, unsqueeze_6, unsqueeze_7, unsqueeze_8, None, getitem_21, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, getitem_22, 0.0, getitem_23, getitem_24, 0, False);  unsqueeze_60 = unsqueeze_6 = unsqueeze_7 = unsqueeze_8 = getitem_21 = convert_element_type_4 = getitem_22 = getitem_23 = getitem_24 = None
	        getitem_323: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_9[0]
	        getitem_324: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_9[1]
	        getitem_325: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_9[2];  _efficient_attention_backward_9 = None
	        squeeze_39: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_325, 0);  getitem_325 = None
	        squeeze_40: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_324, 0);  getitem_324 = None
	        squeeze_41: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_323, 0);  getitem_323 = None
	        permute_264: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_39, [1, 0, 2]);  squeeze_39 = None
	        permute_265: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_40, [1, 0, 2]);  squeeze_40 = None
	        permute_266: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_41, [1, 0, 2]);  squeeze_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_267: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_264, [1, 0, 2]);  permute_264 = None
	        view_100: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_267, [sym_size_int, 384]);  permute_267 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_268: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_265, [1, 0, 2]);  permute_265 = None
	        view_101: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_268, [sym_size_int, 384]);  permute_268 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_269: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_266, [1, 0, 2]);  permute_266 = None
	        view_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_269, [sym_size_int, 384]);  permute_269 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_23: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_296, view_102);  view_102 = None
	        slice_scatter_23: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_37, copy_23, 1, 0, 384);  copy_23 = None
	        split_84 = torch.ops.aten.split.Tensor(slice_scatter_23, 384, 1)
	        getitem_337: "f32[s1, 384][1152, 1]cuda:0" = split_84[1];  split_84 = None
	        copy_24: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_337, view_101);  getitem_337 = view_101 = None
	        slice_scatter_24: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_23, copy_24, 1, 384, 768);  slice_scatter_23 = copy_24 = None
	        split_87 = torch.ops.aten.split.Tensor(slice_scatter_24, 384, 1)
	        getitem_347: "f32[s1, 384][1152, 1]cuda:0" = split_87[2];  split_87 = None
	        copy_25: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_347, view_100);  getitem_347 = view_100 = None
	        slice_scatter_25: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_24, copy_25, 1, 768, 1152);  slice_scatter_24 = copy_25 = None
	        mm_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_25, primals_19);  primals_19 = None
	        permute_271: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_25, [1, 0]);  slice_scatter_25 = None
	        mm_115: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_271, mul_25);  permute_271 = mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_45: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_6, torch.float32);  gt_6 = None
	        mul_284: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_45, 1.4285714285714286);  convert_element_type_45 = None
	        mul_285: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_114, mul_284);  mm_114 = mul_284 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_286: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_285, mul_22);  mul_22 = None
	        mul_287: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_285, primals_18);  mul_285 = primals_18 = None
	        sum_31: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_286, [0], True);  mul_286 = None
	        unsqueeze_61: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_31, 0);  sum_31 = None
	        view_103: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_61, [384]);  unsqueeze_61 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_288: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_287, add_7)
	        mul_289: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_287, rsqrt_4);  mul_287 = None
	        sum_32: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_288, [1], True);  mul_288 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_79: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_78, mul_289);  add_78 = mul_289 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_51: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_4, 3);  rsqrt_4 = None
	        mul_290: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_32, -0.5);  sum_32 = None
	        mul_291: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_290, pow_51);  mul_290 = pow_51 = None
	        expand_15: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_291, [-1, 384]);  mul_291 = None
	        div_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_15, 384);  expand_15 = None
	        pow_52: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 1.0);  add_7 = None
	        mul_292: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_52, 2.0);  pow_52 = None
	        mul_293: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_15, mul_292);  div_15 = mul_292 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_80: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_79, mul_293);  add_79 = mul_293 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_5, torch.float32);  gt_5 = None
	        mul_294: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_46, 1.4285714285714286);  convert_element_type_46 = None
	        mul_295: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_80, mul_294);  mul_294 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_116: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_295, primals_17);  primals_17 = None
	        permute_272: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_295, [1, 0]);  mul_295 = None
	        mm_117: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_272, mul_19);  permute_272 = mul_19 = None
	        convert_element_type_47: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_4, torch.float32);  gt_4 = None
	        mul_296: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_47, 1.4285714285714286);  convert_element_type_47 = None
	        mul_297: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_116, mul_296);  mm_116 = mul_296 = None
	        sigmoid_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        sub_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_36, sigmoid_14)
	        mul_298: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sub_6);  mm_6 = sub_6 = None
	        add_81: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_298, 1);  mul_298 = None
	        mul_299: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_14, add_81);  sigmoid_14 = add_81 = None
	        mul_300: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_297, mul_299);  mul_297 = mul_299 = None
	        mm_118: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_300, primals_16);  primals_16 = None
	        permute_274: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_300, [1, 0]);  mul_300 = None
	        mm_119: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_274, mul_16);  permute_274 = mul_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_301: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_118, mul_15);  mul_15 = None
	        mul_302: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_118, primals_15);  mm_118 = primals_15 = None
	        sum_33: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_301, [0], True);  mul_301 = None
	        unsqueeze_62: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_33, 0);  sum_33 = None
	        view_104: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_62, [384]);  unsqueeze_62 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_303: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_302, add_5)
	        mul_304: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_302, rsqrt_3);  mul_302 = None
	        sum_34: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_303, [1], True);  mul_303 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_82: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_80, mul_304);  add_80 = mul_304 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_53: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_3, 3);  rsqrt_3 = None
	        mul_305: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_34, -0.5);  sum_34 = None
	        mul_306: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_305, pow_53);  mul_305 = pow_53 = None
	        expand_16: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_306, [-1, 384]);  mul_306 = None
	        div_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_16, 384);  expand_16 = None
	        pow_54: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 1.0);  add_5 = None
	        mul_307: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_54, 2.0);  pow_54 = None
	        mul_308: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_16, mul_307);  div_16 = mul_307 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_82, mul_308);  add_82 = mul_308 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_120: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_83, primals_14);  primals_14 = None
	        permute_275: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_83, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_121: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_275, view_7);  permute_275 = view_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_105: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_120, [sym_size_int, 6, 64]);  mm_120 = None
	        permute_276: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_105, [1, 0, 2]);  view_105 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_277: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_276, [1, 0, 2]);  permute_276 = None
	        unsqueeze_63: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_277, 0);  permute_277 = None
	        _efficient_attention_backward_10 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_63, unsqueeze_3, unsqueeze_4, unsqueeze_5, None, getitem_12, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, getitem_13, 0.0, getitem_14, getitem_15, 0, False);  unsqueeze_63 = unsqueeze_3 = unsqueeze_4 = unsqueeze_5 = getitem_12 = convert_element_type_2 = getitem_13 = getitem_14 = getitem_15 = None
	        getitem_354: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_10[0]
	        getitem_355: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_10[1]
	        getitem_356: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_10[2];  _efficient_attention_backward_10 = None
	        squeeze_42: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_356, 0);  getitem_356 = None
	        squeeze_43: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_355, 0);  getitem_355 = None
	        squeeze_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_354, 0);  getitem_354 = None
	        permute_278: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_42, [1, 0, 2]);  squeeze_42 = None
	        permute_279: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_43, [1, 0, 2]);  squeeze_43 = None
	        permute_280: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_44, [1, 0, 2]);  squeeze_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_281: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_278, [1, 0, 2]);  permute_278 = None
	        view_106: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_281, [sym_size_int, 384]);  permute_281 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_282: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_279, [1, 0, 2]);  permute_279 = None
	        view_107: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_282, [sym_size_int, 384]);  permute_282 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_283: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_280, [1, 0, 2]);  permute_280 = None
	        view_108: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_283, [sym_size_int, 384]);  permute_283 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_26: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_296, view_108);  view_108 = None
	        slice_scatter_26: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_37, copy_26, 1, 0, 384);  copy_26 = None
	        split_93 = torch.ops.aten.split.Tensor(slice_scatter_26, 384, 1)
	        getitem_368: "f32[s1, 384][1152, 1]cuda:0" = split_93[1];  split_93 = None
	        copy_27: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_368, view_107);  getitem_368 = view_107 = None
	        slice_scatter_27: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_26, copy_27, 1, 384, 768);  slice_scatter_26 = copy_27 = None
	        split_96 = torch.ops.aten.split.Tensor(slice_scatter_27, 384, 1)
	        getitem_378: "f32[s1, 384][1152, 1]cuda:0" = split_96[2];  split_96 = None
	        copy_28: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_378, view_106);  getitem_378 = view_106 = None
	        slice_scatter_28: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_27, copy_28, 1, 768, 1152);  slice_scatter_27 = copy_28 = None
	        mm_122: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_28, primals_13);  primals_13 = None
	        permute_285: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_28, [1, 0]);  slice_scatter_28 = None
	        mm_123: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_285, mul_14);  permute_285 = mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_48: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_3, torch.float32);  gt_3 = None
	        mul_309: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_48, 1.4285714285714286);  convert_element_type_48 = None
	        mul_310: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_122, mul_309);  mm_122 = mul_309 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_311: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_310, mul_11);  mul_11 = None
	        mul_312: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_310, primals_12);  mul_310 = primals_12 = None
	        sum_35: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_311, [0], True);  mul_311 = None
	        unsqueeze_64: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_35, 0);  sum_35 = None
	        view_109: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_64, [384]);  unsqueeze_64 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_313: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_312, add_3)
	        mul_314: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_312, rsqrt_2);  mul_312 = None
	        sum_36: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_313, [1], True);  mul_313 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_83, mul_314);  add_83 = mul_314 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_55: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_2, 3);  rsqrt_2 = None
	        mul_315: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_36, -0.5);  sum_36 = None
	        mul_316: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_315, pow_55);  mul_315 = pow_55 = None
	        expand_17: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_316, [-1, 384]);  mul_316 = None
	        div_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_17, 384);  expand_17 = None
	        pow_56: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 1.0);  add_3 = None
	        mul_317: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_56, 2.0);  pow_56 = None
	        mul_318: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_17, mul_317);  div_17 = mul_317 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_85: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_84, mul_318);  add_84 = mul_318 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_49: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
	        mul_319: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_49, 1.4285714285714286);  convert_element_type_49 = None
	        mul_320: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_85, mul_319);  mul_319 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_124: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_320, primals_11);  primals_11 = None
	        permute_286: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_320, [1, 0]);  mul_320 = None
	        mm_125: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_286, mul_8);  permute_286 = mul_8 = None
	        convert_element_type_50: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
	        mul_321: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_50, 1.4285714285714286);  convert_element_type_50 = None
	        mul_322: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_124, mul_321);  mm_124 = mul_321 = None
	        sigmoid_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        sub_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_36, sigmoid_15);  full_36 = None
	        mul_323: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub_7);  mm_2 = sub_7 = None
	        add_86: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_323, 1);  mul_323 = None
	        mul_324: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_15, add_86);  sigmoid_15 = add_86 = None
	        mul_325: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_322, mul_324);  mul_322 = mul_324 = None
	        mm_126: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_325, primals_10);  primals_10 = None
	        permute_288: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_325, [1, 0]);  mul_325 = None
	        mm_127: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_288, mul_5);  permute_288 = mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_326: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_126, mul_4);  mul_4 = None
	        mul_327: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_126, primals_9);  mm_126 = primals_9 = None
	        sum_37: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_326, [0], True);  mul_326 = None
	        unsqueeze_65: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_37, 0);  sum_37 = None
	        view_110: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_65, [384]);  unsqueeze_65 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_328: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_327, add_1)
	        mul_329: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_327, rsqrt_1);  mul_327 = None
	        sum_38: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_328, [1], True);  mul_328 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_87: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_85, mul_329);  add_85 = mul_329 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_57: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_1, 3);  rsqrt_1 = None
	        mul_330: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_38, -0.5);  sum_38 = None
	        mul_331: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_330, pow_57);  mul_330 = pow_57 = None
	        expand_18: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_331, [-1, 384]);  mul_331 = None
	        div_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_18, 384);  expand_18 = None
	        pow_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
	        mul_332: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_58, 2.0);  pow_58 = None
	        mul_333: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_18, mul_332);  div_18 = mul_332 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_88: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_87, mul_333);  add_87 = mul_333 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_128: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_88, primals_8);  primals_8 = None
	        permute_289: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_88, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_129: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_289, view_3);  permute_289 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_111: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_128, [sym_size_int, 6, 64]);  mm_128 = None
	        permute_290: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_111, [1, 0, 2]);  view_111 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_291: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_290, [1, 0, 2]);  permute_290 = None
	        unsqueeze_66: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_291, 0);  permute_291 = None
	        _efficient_attention_backward_11 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_66, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem_3, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_66 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem_3 = convert_element_type = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_385: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_11[0]
	        getitem_386: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_11[1]
	        getitem_387: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_11[2];  _efficient_attention_backward_11 = None
	        squeeze_45: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_387, 0);  getitem_387 = None
	        squeeze_46: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_386, 0);  getitem_386 = None
	        squeeze_47: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_385, 0);  getitem_385 = None
	        permute_292: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_45, [1, 0, 2]);  squeeze_45 = None
	        permute_293: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_46, [1, 0, 2]);  squeeze_46 = None
	        permute_294: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_47, [1, 0, 2]);  squeeze_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_295: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_292, [1, 0, 2]);  permute_292 = None
	        view_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_295, [sym_size_int, 384]);  permute_295 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_296: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_293, [1, 0, 2]);  permute_293 = None
	        view_113: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_296, [sym_size_int, 384]);  permute_296 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_297: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_294, [1, 0, 2]);  permute_294 = None
	        view_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_297, [sym_size_int, 384]);  permute_297 = sym_size_int = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_29: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_296, view_114);  getitem_296 = view_114 = None
	        slice_scatter_29: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_37, copy_29, 1, 0, 384);  full_37 = copy_29 = None
	        split_102 = torch.ops.aten.split.Tensor(slice_scatter_29, 384, 1)
	        getitem_399: "f32[s1, 384][1152, 1]cuda:0" = split_102[1];  split_102 = None
	        copy_30: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_399, view_113);  getitem_399 = view_113 = None
	        slice_scatter_30: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_29, copy_30, 1, 384, 768);  slice_scatter_29 = copy_30 = None
	        split_105 = torch.ops.aten.split.Tensor(slice_scatter_30, 384, 1)
	        getitem_409: "f32[s1, 384][1152, 1]cuda:0" = split_105[2];  split_105 = None
	        copy_31: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_409, view_112);  getitem_409 = view_112 = None
	        slice_scatter_31: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_30, copy_31, 1, 768, 1152);  slice_scatter_30 = copy_31 = None
	        mm_130: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_31, primals_7);  primals_7 = None
	        permute_299: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_31, [1, 0]);  slice_scatter_31 = None
	        mm_131: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_299, mul_3);  permute_299 = mul_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_334: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_51, 1.4285714285714286);  convert_element_type_51 = None
	        mul_335: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_130, mul_334);  mm_130 = mul_334 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_336: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_335, mul);  mul = None
	        mul_337: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_335, primals_6);  mul_335 = primals_6 = None
	        sum_39: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_336, [0], True);  mul_336 = None
	        unsqueeze_67: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_39, 0);  sum_39 = None
	        view_115: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_67, [384]);  unsqueeze_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_338: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_337, primals_1)
	        mul_339: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_337, rsqrt);  mul_337 = None
	        sum_40: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_338, [1], True);  mul_338 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_89: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_88, mul_339);  add_88 = mul_339 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_59: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
	        mul_340: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_40, -0.5);  sum_40 = None
	        mul_341: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_340, pow_59);  mul_340 = pow_59 = None
	        expand_19: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_341, [-1, 384]);  mul_341 = None
	        div_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_19, 384);  expand_19 = None
	        pow_60: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
	        mul_342: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_60, 2.0);  pow_60 = None
	        mul_343: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_19, mul_342);  div_19 = mul_342 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_90: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_89, mul_343);  add_89 = mul_343 = None
	        return (add_90, primals_2, primals_3, primals_4, None, view_115, mm_131, mm_129, view_110, mm_127, mm_125, view_109, mm_123, mm_121, view_104, mm_119, mm_117, view_103, mm_115, mm_113, view_98, mm_111, mm_109, view_97, mm_107, mm_105, view_92, mm_103, mm_101, add_70, primals_31, primals_32, primals_33, None, view_91, mm_99, mm_97, view_86, mm_95, mm_93, mm_91, view_81, mm_89, mm_87, view_80, mm_85, mm_83, view_75, mm_81, mm_79, mm_77, view_70, mm_75, mm_73, view_69, mm_71, mm_69, view_64, mm_67, mm_65, mm_63, view_59, mm_61, mm_59, view_58, mm_57, mm_55, view_53, mm_53, mm_51, mm_49, view_48, mm_47, mm_45)
	        
V0303 09:09:53.839143 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "491f79c70928a2fc61e2da5af29466bf"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992993838807.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:53.839676 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "cdecbd54e9adb06294ecfb2da6ade76a"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992993839611.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:53.839853 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "2e29865a257df45d1071a9d6b830eef7"}
	{
	"name": "inductor_compile",
	"ts": 1740992993839611.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:53.926640 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "fb177c4a6bfd2eb94ca86858a0e7d342"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74):
	        pow_1 = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt = torch.ops.aten.rsqrt.default(add);  add = None
	        mul = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        mul_1 = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
	        inductor_seeds_default = torch.ops.prims.inductor_seeds.default(28, device(type='cuda', index=0))
	        inductor_lookup_seed_default = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_27 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        gt = torch.ops.aten.gt.Scalar(inductor_random_default_27, 0.3);  inductor_random_default_27 = None
	        mul_2 = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3 = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        permute = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem = split[0]
	        getitem_1 = split[1]
	        getitem_2 = split[2];  split = None
	        view = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1 = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        view_1 = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2 = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        view_2 = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3 = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        permute_4 = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5 = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6 = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1 = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2 = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1 = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3 = _efficient_attention_forward[0]
	        getitem_4 = _efficient_attention_forward[1]
	        getitem_5 = _efficient_attention_forward[2]
	        getitem_6 = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7 = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3 = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        permute_9 = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1 = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        add_1 = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        pow_2 = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1 = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2 = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1 = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4 = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        mul_5 = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        permute_10 = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2 = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6 = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        inductor_lookup_seed_default_1 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_26 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        gt_1 = torch.ops.aten.gt.Scalar(inductor_random_default_26, 0.3);  inductor_random_default_26 = None
	        mul_7 = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8 = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11 = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3 = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        inductor_lookup_seed_default_2 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2)
	        inductor_random_default_25 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        gt_2 = torch.ops.aten.gt.Scalar(inductor_random_default_25, 0.3);  inductor_random_default_25 = None
	        mul_9 = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10 = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3 = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        pow_3 = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2 = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4 = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2 = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        mul_11 = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        mul_12 = torch.ops.aten.mul.Tensor(mul_11, primals_12);  mul_11 = None
	        inductor_lookup_seed_default_3 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 3)
	        inductor_random_default_24 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_3, 'rand');  inductor_lookup_seed_default_3 = None
	        gt_3 = torch.ops.aten.gt.Scalar(inductor_random_default_24, 0.3);  inductor_random_default_24 = None
	        mul_13 = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14 = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        permute_12 = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4 = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9 = split_1[0]
	        getitem_10 = split_1[1]
	        getitem_11 = split_1[2];  split_1 = None
	        view_4 = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        permute_13 = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        view_5 = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        permute_14 = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        view_6 = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        permute_15 = torch.ops.aten.permute.default(view_6, [1, 0, 2]);  view_6 = None
	        permute_16 = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        permute_17 = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        permute_18 = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        convert_element_type_2 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_3 = torch.ops.aten.unsqueeze.default(permute_16, 0);  permute_16 = None
	        unsqueeze_4 = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        unsqueeze_5 = torch.ops.aten.unsqueeze.default(permute_18, 0);  permute_18 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12 = _efficient_attention_forward_1[0]
	        getitem_13 = _efficient_attention_forward_1[1]
	        getitem_14 = _efficient_attention_forward_1[2]
	        getitem_15 = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        squeeze_1 = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19 = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        permute_20 = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7 = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        permute_21 = torch.ops.aten.permute.default(primals_14, [1, 0])
	        mm_5 = torch.ops.aten.mm.default(view_7, permute_21);  view_7 = permute_21 = None
	        add_5 = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
	        pow_4 = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
	        mean_3 = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6 = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3 = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        mul_15 = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        mul_16 = torch.ops.aten.mul.Tensor(mul_15, primals_15);  mul_15 = None
	        permute_22 = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6 = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1 = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17 = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        inductor_lookup_seed_default_4 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 4)
	        inductor_random_default_23 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_4, 'rand');  inductor_lookup_seed_default_4 = None
	        gt_4 = torch.ops.aten.gt.Scalar(inductor_random_default_23, 0.3);  inductor_random_default_23 = None
	        mul_18 = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19 = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23 = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7 = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        inductor_lookup_seed_default_5 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 5)
	        inductor_random_default_22 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_5, 'rand');  inductor_lookup_seed_default_5 = None
	        gt_5 = torch.ops.aten.gt.Scalar(inductor_random_default_22, 0.3);  inductor_random_default_22 = None
	        mul_20 = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21 = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7 = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
	        pow_5 = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4 = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8 = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4 = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        mul_22 = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        mul_23 = torch.ops.aten.mul.Tensor(mul_22, primals_18);  mul_22 = None
	        inductor_lookup_seed_default_6 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 6)
	        inductor_random_default_21 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_6, 'rand');  inductor_lookup_seed_default_6 = None
	        gt_6 = torch.ops.aten.gt.Scalar(inductor_random_default_21, 0.3);  inductor_random_default_21 = None
	        mul_24 = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25 = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        permute_24 = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8 = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18 = split_2[0]
	        getitem_19 = split_2[1]
	        getitem_20 = split_2[2];  split_2 = None
	        view_8 = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        permute_25 = torch.ops.aten.permute.default(view_8, [1, 0, 2]);  view_8 = None
	        view_9 = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        permute_26 = torch.ops.aten.permute.default(view_9, [1, 0, 2]);  view_9 = None
	        view_10 = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        permute_27 = torch.ops.aten.permute.default(view_10, [1, 0, 2]);  view_10 = None
	        permute_28 = torch.ops.aten.permute.default(permute_25, [1, 0, 2]);  permute_25 = None
	        permute_29 = torch.ops.aten.permute.default(permute_26, [1, 0, 2]);  permute_26 = None
	        permute_30 = torch.ops.aten.permute.default(permute_27, [1, 0, 2]);  permute_27 = None
	        convert_element_type_4 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_6 = torch.ops.aten.unsqueeze.default(permute_28, 0);  permute_28 = None
	        unsqueeze_7 = torch.ops.aten.unsqueeze.default(permute_29, 0);  permute_29 = None
	        unsqueeze_8 = torch.ops.aten.unsqueeze.default(permute_30, 0);  permute_30 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21 = _efficient_attention_forward_2[0]
	        getitem_22 = _efficient_attention_forward_2[1]
	        getitem_23 = _efficient_attention_forward_2[2]
	        getitem_24 = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        squeeze_2 = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31 = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        permute_32 = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11 = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        permute_33 = torch.ops.aten.permute.default(primals_20, [1, 0])
	        mm_9 = torch.ops.aten.mm.default(view_11, permute_33);  view_11 = permute_33 = None
	        add_9 = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
	        pow_6 = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
	        mean_5 = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10 = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5 = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        mul_26 = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        mul_27 = torch.ops.aten.mul.Tensor(mul_26, primals_21);  mul_26 = None
	        permute_34 = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10 = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2 = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28 = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        inductor_lookup_seed_default_7 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 7)
	        inductor_random_default_20 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_7, 'rand');  inductor_lookup_seed_default_7 = None
	        gt_7 = torch.ops.aten.gt.Scalar(inductor_random_default_20, 0.3);  inductor_random_default_20 = None
	        mul_29 = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30 = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35 = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11 = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        inductor_lookup_seed_default_8 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 8)
	        inductor_random_default_19 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_8, 'rand');  inductor_lookup_seed_default_8 = None
	        gt_8 = torch.ops.aten.gt.Scalar(inductor_random_default_19, 0.3);  inductor_random_default_19 = None
	        mul_31 = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32 = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11 = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
	        pow_7 = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6 = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12 = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6 = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        mul_33 = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        mul_34 = torch.ops.aten.mul.Tensor(mul_33, primals_24);  mul_33 = None
	        inductor_lookup_seed_default_9 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 9)
	        inductor_random_default_18 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_9, 'rand');  inductor_lookup_seed_default_9 = None
	        gt_9 = torch.ops.aten.gt.Scalar(inductor_random_default_18, 0.3);  inductor_random_default_18 = None
	        mul_35 = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36 = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        permute_36 = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12 = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27 = split_3[0]
	        getitem_28 = split_3[1]
	        getitem_29 = split_3[2];  split_3 = None
	        view_12 = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        permute_37 = torch.ops.aten.permute.default(view_12, [1, 0, 2]);  view_12 = None
	        view_13 = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        permute_38 = torch.ops.aten.permute.default(view_13, [1, 0, 2]);  view_13 = None
	        view_14 = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        permute_39 = torch.ops.aten.permute.default(view_14, [1, 0, 2]);  view_14 = None
	        permute_40 = torch.ops.aten.permute.default(permute_37, [1, 0, 2]);  permute_37 = None
	        permute_41 = torch.ops.aten.permute.default(permute_38, [1, 0, 2]);  permute_38 = None
	        permute_42 = torch.ops.aten.permute.default(permute_39, [1, 0, 2]);  permute_39 = None
	        convert_element_type_6 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_9 = torch.ops.aten.unsqueeze.default(permute_40, 0);  permute_40 = None
	        unsqueeze_10 = torch.ops.aten.unsqueeze.default(permute_41, 0);  permute_41 = None
	        unsqueeze_11 = torch.ops.aten.unsqueeze.default(permute_42, 0);  permute_42 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30 = _efficient_attention_forward_3[0]
	        getitem_31 = _efficient_attention_forward_3[1]
	        getitem_32 = _efficient_attention_forward_3[2]
	        getitem_33 = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        squeeze_3 = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43 = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        permute_44 = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15 = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        permute_45 = torch.ops.aten.permute.default(primals_26, [1, 0])
	        mm_13 = torch.ops.aten.mm.default(view_15, permute_45);  view_15 = permute_45 = None
	        add_13 = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
	        pow_8 = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
	        mean_7 = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14 = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7 = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        mul_37 = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        mul_38 = torch.ops.aten.mul.Tensor(mul_37, primals_27);  mul_37 = None
	        permute_46 = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14 = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3 = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39 = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        inductor_lookup_seed_default_10 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 10)
	        inductor_random_default_17 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_10, 'rand');  inductor_lookup_seed_default_10 = None
	        gt_10 = torch.ops.aten.gt.Scalar(inductor_random_default_17, 0.3);  inductor_random_default_17 = None
	        mul_40 = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41 = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47 = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15 = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        inductor_lookup_seed_default_11 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 11)
	        inductor_random_default_16 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_11, 'rand');  inductor_lookup_seed_default_11 = None
	        gt_11 = torch.ops.aten.gt.Scalar(inductor_random_default_16, 0.3);  inductor_random_default_16 = None
	        mul_42 = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43 = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15 = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
	        pow_9 = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_8 = torch.ops.aten.mean.dim(pow_9, [1], True);  pow_9 = None
	        add_16 = torch.ops.aten.add.Tensor(mean_8, 1e-06);  mean_8 = None
	        rsqrt_8 = torch.ops.aten.rsqrt.default(add_16);  add_16 = None
	        mul_44 = torch.ops.aten.mul.Tensor(primals_30, rsqrt_8)
	        mul_45 = torch.ops.aten.mul.Tensor(mul_44, primals_35);  mul_44 = None
	        sym_size_int_3 = torch.ops.aten.sym_size.int(primals_30, 0)
	        inductor_lookup_seed_default_12 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 12)
	        inductor_random_default_15 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_12, 'rand');  inductor_lookup_seed_default_12 = None
	        gt_12 = torch.ops.aten.gt.Scalar(inductor_random_default_15, 0.3);  inductor_random_default_15 = None
	        mul_46 = torch.ops.aten.mul.Tensor(gt_12, mul_45);  mul_45 = None
	        mul_47 = torch.ops.aten.mul.Tensor(mul_46, 1.4285714285714286);  mul_46 = None
	        permute_48 = torch.ops.aten.permute.default(primals_36, [1, 0])
	        mm_16 = torch.ops.aten.mm.default(mul_47, permute_48);  permute_48 = None
	        split_4 = torch.ops.aten.split.Tensor(mm_16, 384, 1);  mm_16 = None
	        getitem_36 = split_4[0]
	        getitem_37 = split_4[1]
	        getitem_38 = split_4[2];  split_4 = None
	        view_16 = torch.ops.aten.view.default(getitem_36, [sym_size_int_3, 6, 64]);  getitem_36 = None
	        permute_49 = torch.ops.aten.permute.default(view_16, [1, 0, 2]);  view_16 = None
	        view_17 = torch.ops.aten.view.default(getitem_37, [sym_size_int_3, 6, 64]);  getitem_37 = None
	        permute_50 = torch.ops.aten.permute.default(view_17, [1, 0, 2]);  view_17 = None
	        view_18 = torch.ops.aten.view.default(getitem_38, [sym_size_int_3, 6, 64]);  getitem_38 = None
	        permute_51 = torch.ops.aten.permute.default(view_18, [1, 0, 2]);  view_18 = None
	        permute_52 = torch.ops.aten.permute.default(permute_49, [1, 0, 2]);  permute_49 = None
	        permute_53 = torch.ops.aten.permute.default(permute_50, [1, 0, 2]);  permute_50 = None
	        permute_54 = torch.ops.aten.permute.default(permute_51, [1, 0, 2]);  permute_51 = None
	        convert_element_type_8 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_12 = torch.ops.aten.unsqueeze.default(permute_52, 0);  permute_52 = None
	        unsqueeze_13 = torch.ops.aten.unsqueeze.default(permute_53, 0);  permute_53 = None
	        unsqueeze_14 = torch.ops.aten.unsqueeze.default(permute_54, 0);  permute_54 = None
	        sym_size_int_4 = torch.ops.aten.sym_size.int(primals_33, 0)
	        _efficient_attention_forward_4 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_12, unsqueeze_13, unsqueeze_14, None, convert_element_type_8, convert_element_type_8, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_39 = _efficient_attention_forward_4[0]
	        getitem_40 = _efficient_attention_forward_4[1]
	        getitem_41 = _efficient_attention_forward_4[2]
	        getitem_42 = _efficient_attention_forward_4[3];  _efficient_attention_forward_4 = None
	        squeeze_4 = torch.ops.aten.squeeze.dim(getitem_39, 0)
	        permute_55 = torch.ops.aten.permute.default(squeeze_4, [1, 0, 2]);  squeeze_4 = None
	        permute_56 = torch.ops.aten.permute.default(permute_55, [1, 0, 2]);  permute_55 = None
	        view_19 = torch.ops.aten.view.default(permute_56, [sym_size_int_3, 384]);  permute_56 = None
	        permute_57 = torch.ops.aten.permute.default(primals_37, [1, 0])
	        mm_17 = torch.ops.aten.mm.default(view_19, permute_57);  view_19 = permute_57 = None
	        add_17 = torch.ops.aten.add.Tensor(primals_30, mm_17);  mm_17 = None
	        pow_10 = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_9 = torch.ops.aten.mean.dim(pow_10, [1], True);  pow_10 = None
	        add_18 = torch.ops.aten.add.Tensor(mean_9, 1e-06);  mean_9 = None
	        rsqrt_9 = torch.ops.aten.rsqrt.default(add_18);  add_18 = None
	        mul_48 = torch.ops.aten.mul.Tensor(primals_30, rsqrt_9)
	        mul_49 = torch.ops.aten.mul.Tensor(mul_48, primals_38);  mul_48 = None
	        inductor_lookup_seed_default_13 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 13)
	        inductor_random_default_14 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_13, 'rand');  inductor_lookup_seed_default_13 = None
	        gt_13 = torch.ops.aten.gt.Scalar(inductor_random_default_14, 0.3);  inductor_random_default_14 = None
	        mul_50 = torch.ops.aten.mul.Tensor(gt_13, mul_49);  mul_49 = None
	        mul_51 = torch.ops.aten.mul.Tensor(mul_50, 1.4285714285714286);  mul_50 = None
	        permute_58 = torch.ops.aten.permute.default(primals_39, [1, 0])
	        mm_18 = torch.ops.aten.mm.default(mul_51, permute_58);  permute_58 = None
	        permute_59 = torch.ops.aten.permute.default(primals_40, [1, 0])
	        mm_19 = torch.ops.aten.mm.default(add_15, permute_59);  permute_59 = None
	        split_5 = torch.ops.aten.split.Tensor(mm_19, 384, 1);  mm_19 = None
	        getitem_45 = split_5[0]
	        getitem_46 = split_5[1];  split_5 = None
	        view_20 = torch.ops.aten.view.default(mm_18, [sym_size_int_3, 6, 64]);  mm_18 = None
	        permute_60 = torch.ops.aten.permute.default(view_20, [1, 0, 2]);  view_20 = None
	        view_21 = torch.ops.aten.view.default(getitem_45, [sym_size_int, 6, 64]);  getitem_45 = None
	        permute_61 = torch.ops.aten.permute.default(view_21, [1, 0, 2]);  view_21 = None
	        view_22 = torch.ops.aten.view.default(getitem_46, [sym_size_int, 6, 64]);  getitem_46 = None
	        permute_62 = torch.ops.aten.permute.default(view_22, [1, 0, 2]);  view_22 = None
	        permute_63 = torch.ops.aten.permute.default(permute_60, [1, 0, 2]);  permute_60 = None
	        permute_64 = torch.ops.aten.permute.default(permute_61, [1, 0, 2]);  permute_61 = None
	        permute_65 = torch.ops.aten.permute.default(permute_62, [1, 0, 2]);  permute_62 = None
	        convert_element_type_10 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_11 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_15 = torch.ops.aten.unsqueeze.default(permute_63, 0);  permute_63 = None
	        unsqueeze_16 = torch.ops.aten.unsqueeze.default(permute_64, 0);  permute_64 = None
	        unsqueeze_17 = torch.ops.aten.unsqueeze.default(permute_65, 0);  permute_65 = None
	        _efficient_attention_forward_5 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_15, unsqueeze_16, unsqueeze_17, None, convert_element_type_10, convert_element_type_11, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_47 = _efficient_attention_forward_5[0]
	        getitem_48 = _efficient_attention_forward_5[1]
	        getitem_49 = _efficient_attention_forward_5[2]
	        getitem_50 = _efficient_attention_forward_5[3];  _efficient_attention_forward_5 = None
	        squeeze_5 = torch.ops.aten.squeeze.dim(getitem_47, 0)
	        permute_66 = torch.ops.aten.permute.default(squeeze_5, [1, 0, 2]);  squeeze_5 = None
	        permute_67 = torch.ops.aten.permute.default(permute_66, [1, 0, 2]);  permute_66 = None
	        view_23 = torch.ops.aten.view.default(permute_67, [sym_size_int_3, 384]);  permute_67 = None
	        permute_68 = torch.ops.aten.permute.default(primals_41, [1, 0])
	        mm_20 = torch.ops.aten.mm.default(view_23, permute_68);  view_23 = permute_68 = None
	        add_19 = torch.ops.aten.add.Tensor(add_17, mm_20);  add_17 = mm_20 = None
	        pow_11 = torch.ops.aten.pow.Tensor_Scalar(add_19, 2)
	        mean_10 = torch.ops.aten.mean.dim(pow_11, [1], True);  pow_11 = None
	        add_20 = torch.ops.aten.add.Tensor(mean_10, 1e-06);  mean_10 = None
	        rsqrt_10 = torch.ops.aten.rsqrt.default(add_20);  add_20 = None
	        mul_52 = torch.ops.aten.mul.Tensor(add_19, rsqrt_10)
	        mul_53 = torch.ops.aten.mul.Tensor(mul_52, primals_42);  mul_52 = None
	        permute_69 = torch.ops.aten.permute.default(primals_43, [1, 0])
	        mm_21 = torch.ops.aten.mm.default(mul_53, permute_69);  permute_69 = None
	        sigmoid_4 = torch.ops.aten.sigmoid.default(mm_21)
	        mul_54 = torch.ops.aten.mul.Tensor(mm_21, sigmoid_4);  sigmoid_4 = None
	        inductor_lookup_seed_default_14 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 14)
	        inductor_random_default_13 = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_14, 'rand');  inductor_lookup_seed_default_14 = None
	        gt_14 = torch.ops.aten.gt.Scalar(inductor_random_default_13, 0.3);  inductor_random_default_13 = None
	        mul_55 = torch.ops.aten.mul.Tensor(gt_14, mul_54);  mul_54 = None
	        mul_56 = torch.ops.aten.mul.Tensor(mul_55, 1.4285714285714286);  mul_55 = None
	        permute_70 = torch.ops.aten.permute.default(primals_44, [1, 0])
	        mm_22 = torch.ops.aten.mm.default(mul_56, permute_70);  permute_70 = None
	        inductor_lookup_seed_default_15 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 15)
	        inductor_random_default_12 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_15, 'rand');  inductor_lookup_seed_default_15 = None
	        gt_15 = torch.ops.aten.gt.Scalar(inductor_random_default_12, 0.3);  inductor_random_default_12 = None
	        mul_57 = torch.ops.aten.mul.Tensor(gt_15, mm_22);  mm_22 = None
	        mul_58 = torch.ops.aten.mul.Tensor(mul_57, 1.4285714285714286);  mul_57 = None
	        add_21 = torch.ops.aten.add.Tensor(add_19, mul_58);  mul_58 = None
	        pow_12 = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_11 = torch.ops.aten.mean.dim(pow_12, [1], True);  pow_12 = None
	        add_22 = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None
	        rsqrt_11 = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
	        mul_59 = torch.ops.aten.mul.Tensor(add_21, rsqrt_11)
	        mul_60 = torch.ops.aten.mul.Tensor(mul_59, primals_45);  mul_59 = None
	        inductor_lookup_seed_default_16 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 16)
	        inductor_random_default_11 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_16, 'rand');  inductor_lookup_seed_default_16 = None
	        gt_16 = torch.ops.aten.gt.Scalar(inductor_random_default_11, 0.3);  inductor_random_default_11 = None
	        mul_61 = torch.ops.aten.mul.Tensor(gt_16, mul_60);  mul_60 = None
	        mul_62 = torch.ops.aten.mul.Tensor(mul_61, 1.4285714285714286);  mul_61 = None
	        permute_71 = torch.ops.aten.permute.default(primals_46, [1, 0])
	        mm_23 = torch.ops.aten.mm.default(mul_62, permute_71);  permute_71 = None
	        split_6 = torch.ops.aten.split.Tensor(mm_23, 384, 1);  mm_23 = None
	        getitem_53 = split_6[0]
	        getitem_54 = split_6[1]
	        getitem_55 = split_6[2];  split_6 = None
	        view_24 = torch.ops.aten.view.default(getitem_53, [sym_size_int_3, 6, 64]);  getitem_53 = None
	        permute_72 = torch.ops.aten.permute.default(view_24, [1, 0, 2]);  view_24 = None
	        view_25 = torch.ops.aten.view.default(getitem_54, [sym_size_int_3, 6, 64]);  getitem_54 = None
	        permute_73 = torch.ops.aten.permute.default(view_25, [1, 0, 2]);  view_25 = None
	        view_26 = torch.ops.aten.view.default(getitem_55, [sym_size_int_3, 6, 64]);  getitem_55 = None
	        permute_74 = torch.ops.aten.permute.default(view_26, [1, 0, 2]);  view_26 = None
	        permute_75 = torch.ops.aten.permute.default(permute_72, [1, 0, 2]);  permute_72 = None
	        permute_76 = torch.ops.aten.permute.default(permute_73, [1, 0, 2]);  permute_73 = None
	        permute_77 = torch.ops.aten.permute.default(permute_74, [1, 0, 2]);  permute_74 = None
	        convert_element_type_12 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_18 = torch.ops.aten.unsqueeze.default(permute_75, 0);  permute_75 = None
	        unsqueeze_19 = torch.ops.aten.unsqueeze.default(permute_76, 0);  permute_76 = None
	        unsqueeze_20 = torch.ops.aten.unsqueeze.default(permute_77, 0);  permute_77 = None
	        _efficient_attention_forward_6 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_18, unsqueeze_19, unsqueeze_20, None, convert_element_type_12, convert_element_type_12, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_56 = _efficient_attention_forward_6[0]
	        getitem_57 = _efficient_attention_forward_6[1]
	        getitem_58 = _efficient_attention_forward_6[2]
	        getitem_59 = _efficient_attention_forward_6[3];  _efficient_attention_forward_6 = None
	        squeeze_6 = torch.ops.aten.squeeze.dim(getitem_56, 0)
	        permute_78 = torch.ops.aten.permute.default(squeeze_6, [1, 0, 2]);  squeeze_6 = None
	        permute_79 = torch.ops.aten.permute.default(permute_78, [1, 0, 2]);  permute_78 = None
	        view_27 = torch.ops.aten.view.default(permute_79, [sym_size_int_3, 384]);  permute_79 = None
	        permute_80 = torch.ops.aten.permute.default(primals_47, [1, 0])
	        mm_24 = torch.ops.aten.mm.default(view_27, permute_80);  view_27 = permute_80 = None
	        add_23 = torch.ops.aten.add.Tensor(add_21, mm_24);  mm_24 = None
	        pow_13 = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_12 = torch.ops.aten.mean.dim(pow_13, [1], True);  pow_13 = None
	        add_24 = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None
	        rsqrt_12 = torch.ops.aten.rsqrt.default(add_24);  add_24 = None
	        mul_63 = torch.ops.aten.mul.Tensor(add_21, rsqrt_12)
	        mul_64 = torch.ops.aten.mul.Tensor(mul_63, primals_48)
	        inductor_lookup_seed_default_17 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 17)
	        inductor_random_default_10 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_17, 'rand');  inductor_lookup_seed_default_17 = None
	        gt_17 = torch.ops.aten.gt.Scalar(inductor_random_default_10, 0.3);  inductor_random_default_10 = None
	        mul_65 = torch.ops.aten.mul.Tensor(gt_17, mul_64);  mul_64 = None
	        mul_66 = torch.ops.aten.mul.Tensor(mul_65, 1.4285714285714286);  mul_65 = None
	        permute_81 = torch.ops.aten.permute.default(primals_49, [1, 0])
	        mm_25 = torch.ops.aten.mm.default(mul_66, permute_81);  permute_81 = None
	        permute_82 = torch.ops.aten.permute.default(primals_50, [1, 0])
	        mm_26 = torch.ops.aten.mm.default(add_15, permute_82);  permute_82 = None
	        split_7 = torch.ops.aten.split.Tensor(mm_26, 384, 1);  mm_26 = None
	        getitem_62 = split_7[0]
	        getitem_63 = split_7[1];  split_7 = None
	        view_28 = torch.ops.aten.view.default(mm_25, [sym_size_int_3, 6, 64]);  mm_25 = None
	        permute_83 = torch.ops.aten.permute.default(view_28, [1, 0, 2]);  view_28 = None
	        view_29 = torch.ops.aten.view.default(getitem_62, [sym_size_int, 6, 64]);  getitem_62 = None
	        permute_84 = torch.ops.aten.permute.default(view_29, [1, 0, 2]);  view_29 = None
	        view_30 = torch.ops.aten.view.default(getitem_63, [sym_size_int, 6, 64]);  getitem_63 = None
	        permute_85 = torch.ops.aten.permute.default(view_30, [1, 0, 2]);  view_30 = None
	        permute_86 = torch.ops.aten.permute.default(permute_83, [1, 0, 2]);  permute_83 = None
	        permute_87 = torch.ops.aten.permute.default(permute_84, [1, 0, 2]);  permute_84 = None
	        permute_88 = torch.ops.aten.permute.default(permute_85, [1, 0, 2]);  permute_85 = None
	        convert_element_type_14 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_15 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_21 = torch.ops.aten.unsqueeze.default(permute_86, 0);  permute_86 = None
	        unsqueeze_22 = torch.ops.aten.unsqueeze.default(permute_87, 0);  permute_87 = None
	        unsqueeze_23 = torch.ops.aten.unsqueeze.default(permute_88, 0);  permute_88 = None
	        _efficient_attention_forward_7 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_21, unsqueeze_22, unsqueeze_23, None, convert_element_type_14, convert_element_type_15, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_64 = _efficient_attention_forward_7[0]
	        getitem_65 = _efficient_attention_forward_7[1]
	        getitem_66 = _efficient_attention_forward_7[2]
	        getitem_67 = _efficient_attention_forward_7[3];  _efficient_attention_forward_7 = None
	        squeeze_7 = torch.ops.aten.squeeze.dim(getitem_64, 0)
	        permute_89 = torch.ops.aten.permute.default(squeeze_7, [1, 0, 2]);  squeeze_7 = None
	        permute_90 = torch.ops.aten.permute.default(permute_89, [1, 0, 2]);  permute_89 = None
	        view_31 = torch.ops.aten.view.default(permute_90, [sym_size_int_3, 384]);  permute_90 = None
	        permute_91 = torch.ops.aten.permute.default(primals_51, [1, 0])
	        mm_27 = torch.ops.aten.mm.default(view_31, permute_91);  view_31 = permute_91 = None
	        add_25 = torch.ops.aten.add.Tensor(add_23, mm_27);  add_23 = mm_27 = None
	        pow_14 = torch.ops.aten.pow.Tensor_Scalar(add_25, 2)
	        mean_13 = torch.ops.aten.mean.dim(pow_14, [1], True);  pow_14 = None
	        add_26 = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None
	        rsqrt_13 = torch.ops.aten.rsqrt.default(add_26);  add_26 = None
	        mul_67 = torch.ops.aten.mul.Tensor(add_25, rsqrt_13)
	        mul_68 = torch.ops.aten.mul.Tensor(mul_67, primals_52);  mul_67 = None
	        permute_92 = torch.ops.aten.permute.default(primals_53, [1, 0])
	        mm_28 = torch.ops.aten.mm.default(mul_68, permute_92);  permute_92 = None
	        sigmoid_5 = torch.ops.aten.sigmoid.default(mm_28)
	        mul_69 = torch.ops.aten.mul.Tensor(mm_28, sigmoid_5);  sigmoid_5 = None
	        inductor_lookup_seed_default_18 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 18)
	        inductor_random_default_9 = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_18, 'rand');  inductor_lookup_seed_default_18 = None
	        gt_18 = torch.ops.aten.gt.Scalar(inductor_random_default_9, 0.3);  inductor_random_default_9 = None
	        mul_70 = torch.ops.aten.mul.Tensor(gt_18, mul_69);  mul_69 = None
	        mul_71 = torch.ops.aten.mul.Tensor(mul_70, 1.4285714285714286);  mul_70 = None
	        permute_93 = torch.ops.aten.permute.default(primals_54, [1, 0])
	        mm_29 = torch.ops.aten.mm.default(mul_71, permute_93);  permute_93 = None
	        inductor_lookup_seed_default_19 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 19)
	        inductor_random_default_8 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_19, 'rand');  inductor_lookup_seed_default_19 = None
	        gt_19 = torch.ops.aten.gt.Scalar(inductor_random_default_8, 0.3);  inductor_random_default_8 = None
	        mul_72 = torch.ops.aten.mul.Tensor(gt_19, mm_29);  mm_29 = None
	        mul_73 = torch.ops.aten.mul.Tensor(mul_72, 1.4285714285714286);  mul_72 = None
	        add_27 = torch.ops.aten.add.Tensor(add_25, mul_73);  mul_73 = None
	        pow_15 = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_14 = torch.ops.aten.mean.dim(pow_15, [1], True);  pow_15 = None
	        add_28 = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None
	        rsqrt_14 = torch.ops.aten.rsqrt.default(add_28);  add_28 = None
	        mul_74 = torch.ops.aten.mul.Tensor(add_27, rsqrt_14)
	        mul_75 = torch.ops.aten.mul.Tensor(mul_74, primals_55);  mul_74 = None
	        inductor_lookup_seed_default_20 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 20)
	        inductor_random_default_7 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_20, 'rand');  inductor_lookup_seed_default_20 = None
	        gt_20 = torch.ops.aten.gt.Scalar(inductor_random_default_7, 0.3);  inductor_random_default_7 = None
	        mul_76 = torch.ops.aten.mul.Tensor(gt_20, mul_75);  mul_75 = None
	        mul_77 = torch.ops.aten.mul.Tensor(mul_76, 1.4285714285714286);  mul_76 = None
	        permute_94 = torch.ops.aten.permute.default(primals_56, [1, 0])
	        mm_30 = torch.ops.aten.mm.default(mul_77, permute_94);  permute_94 = None
	        split_8 = torch.ops.aten.split.Tensor(mm_30, 384, 1);  mm_30 = None
	        getitem_70 = split_8[0]
	        getitem_71 = split_8[1]
	        getitem_72 = split_8[2];  split_8 = None
	        view_32 = torch.ops.aten.view.default(getitem_70, [sym_size_int_3, 6, 64]);  getitem_70 = None
	        permute_95 = torch.ops.aten.permute.default(view_32, [1, 0, 2]);  view_32 = None
	        view_33 = torch.ops.aten.view.default(getitem_71, [sym_size_int_3, 6, 64]);  getitem_71 = None
	        permute_96 = torch.ops.aten.permute.default(view_33, [1, 0, 2]);  view_33 = None
	        view_34 = torch.ops.aten.view.default(getitem_72, [sym_size_int_3, 6, 64]);  getitem_72 = None
	        permute_97 = torch.ops.aten.permute.default(view_34, [1, 0, 2]);  view_34 = None
	        permute_98 = torch.ops.aten.permute.default(permute_95, [1, 0, 2]);  permute_95 = None
	        permute_99 = torch.ops.aten.permute.default(permute_96, [1, 0, 2]);  permute_96 = None
	        permute_100 = torch.ops.aten.permute.default(permute_97, [1, 0, 2]);  permute_97 = None
	        convert_element_type_16 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_24 = torch.ops.aten.unsqueeze.default(permute_98, 0);  permute_98 = None
	        unsqueeze_25 = torch.ops.aten.unsqueeze.default(permute_99, 0);  permute_99 = None
	        unsqueeze_26 = torch.ops.aten.unsqueeze.default(permute_100, 0);  permute_100 = None
	        _efficient_attention_forward_8 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_24, unsqueeze_25, unsqueeze_26, None, convert_element_type_16, convert_element_type_16, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_73 = _efficient_attention_forward_8[0]
	        getitem_74 = _efficient_attention_forward_8[1]
	        getitem_75 = _efficient_attention_forward_8[2]
	        getitem_76 = _efficient_attention_forward_8[3];  _efficient_attention_forward_8 = None
	        squeeze_8 = torch.ops.aten.squeeze.dim(getitem_73, 0)
	        permute_101 = torch.ops.aten.permute.default(squeeze_8, [1, 0, 2]);  squeeze_8 = None
	        permute_102 = torch.ops.aten.permute.default(permute_101, [1, 0, 2]);  permute_101 = None
	        view_35 = torch.ops.aten.view.default(permute_102, [sym_size_int_3, 384]);  permute_102 = None
	        permute_103 = torch.ops.aten.permute.default(primals_57, [1, 0])
	        mm_31 = torch.ops.aten.mm.default(view_35, permute_103);  view_35 = permute_103 = None
	        add_29 = torch.ops.aten.add.Tensor(add_27, mm_31);  mm_31 = None
	        pow_16 = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_15 = torch.ops.aten.mean.dim(pow_16, [1], True);  pow_16 = None
	        add_30 = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None
	        rsqrt_15 = torch.ops.aten.rsqrt.default(add_30);  add_30 = None
	        mul_78 = torch.ops.aten.mul.Tensor(add_27, rsqrt_15)
	        mul_79 = torch.ops.aten.mul.Tensor(mul_78, primals_58)
	        inductor_lookup_seed_default_21 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 21)
	        inductor_random_default_6 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_21, 'rand');  inductor_lookup_seed_default_21 = None
	        gt_21 = torch.ops.aten.gt.Scalar(inductor_random_default_6, 0.3);  inductor_random_default_6 = None
	        mul_80 = torch.ops.aten.mul.Tensor(gt_21, mul_79);  mul_79 = None
	        mul_81 = torch.ops.aten.mul.Tensor(mul_80, 1.4285714285714286);  mul_80 = None
	        permute_104 = torch.ops.aten.permute.default(primals_59, [1, 0])
	        mm_32 = torch.ops.aten.mm.default(mul_81, permute_104);  permute_104 = None
	        permute_105 = torch.ops.aten.permute.default(primals_60, [1, 0])
	        mm_33 = torch.ops.aten.mm.default(add_15, permute_105);  permute_105 = None
	        split_9 = torch.ops.aten.split.Tensor(mm_33, 384, 1);  mm_33 = None
	        getitem_79 = split_9[0]
	        getitem_80 = split_9[1];  split_9 = None
	        view_36 = torch.ops.aten.view.default(mm_32, [sym_size_int_3, 6, 64]);  mm_32 = None
	        permute_106 = torch.ops.aten.permute.default(view_36, [1, 0, 2]);  view_36 = None
	        view_37 = torch.ops.aten.view.default(getitem_79, [sym_size_int, 6, 64]);  getitem_79 = None
	        permute_107 = torch.ops.aten.permute.default(view_37, [1, 0, 2]);  view_37 = None
	        view_38 = torch.ops.aten.view.default(getitem_80, [sym_size_int, 6, 64]);  getitem_80 = None
	        permute_108 = torch.ops.aten.permute.default(view_38, [1, 0, 2]);  view_38 = None
	        permute_109 = torch.ops.aten.permute.default(permute_106, [1, 0, 2]);  permute_106 = None
	        permute_110 = torch.ops.aten.permute.default(permute_107, [1, 0, 2]);  permute_107 = None
	        permute_111 = torch.ops.aten.permute.default(permute_108, [1, 0, 2]);  permute_108 = None
	        convert_element_type_18 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_19 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_27 = torch.ops.aten.unsqueeze.default(permute_109, 0);  permute_109 = None
	        unsqueeze_28 = torch.ops.aten.unsqueeze.default(permute_110, 0);  permute_110 = None
	        unsqueeze_29 = torch.ops.aten.unsqueeze.default(permute_111, 0);  permute_111 = None
	        _efficient_attention_forward_9 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_27, unsqueeze_28, unsqueeze_29, None, convert_element_type_18, convert_element_type_19, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_81 = _efficient_attention_forward_9[0]
	        getitem_82 = _efficient_attention_forward_9[1]
	        getitem_83 = _efficient_attention_forward_9[2]
	        getitem_84 = _efficient_attention_forward_9[3];  _efficient_attention_forward_9 = None
	        squeeze_9 = torch.ops.aten.squeeze.dim(getitem_81, 0)
	        permute_112 = torch.ops.aten.permute.default(squeeze_9, [1, 0, 2]);  squeeze_9 = None
	        permute_113 = torch.ops.aten.permute.default(permute_112, [1, 0, 2]);  permute_112 = None
	        view_39 = torch.ops.aten.view.default(permute_113, [sym_size_int_3, 384]);  permute_113 = None
	        permute_114 = torch.ops.aten.permute.default(primals_61, [1, 0])
	        mm_34 = torch.ops.aten.mm.default(view_39, permute_114);  view_39 = permute_114 = None
	        add_31 = torch.ops.aten.add.Tensor(add_29, mm_34);  add_29 = mm_34 = None
	        pow_17 = torch.ops.aten.pow.Tensor_Scalar(add_31, 2)
	        mean_16 = torch.ops.aten.mean.dim(pow_17, [1], True);  pow_17 = None
	        add_32 = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None
	        rsqrt_16 = torch.ops.aten.rsqrt.default(add_32);  add_32 = None
	        mul_82 = torch.ops.aten.mul.Tensor(add_31, rsqrt_16)
	        mul_83 = torch.ops.aten.mul.Tensor(mul_82, primals_62);  mul_82 = None
	        permute_115 = torch.ops.aten.permute.default(primals_63, [1, 0])
	        mm_35 = torch.ops.aten.mm.default(mul_83, permute_115);  permute_115 = None
	        sigmoid_6 = torch.ops.aten.sigmoid.default(mm_35)
	        mul_84 = torch.ops.aten.mul.Tensor(mm_35, sigmoid_6);  sigmoid_6 = None
	        inductor_lookup_seed_default_22 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 22)
	        inductor_random_default_5 = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_22, 'rand');  inductor_lookup_seed_default_22 = None
	        gt_22 = torch.ops.aten.gt.Scalar(inductor_random_default_5, 0.3);  inductor_random_default_5 = None
	        mul_85 = torch.ops.aten.mul.Tensor(gt_22, mul_84);  mul_84 = None
	        mul_86 = torch.ops.aten.mul.Tensor(mul_85, 1.4285714285714286);  mul_85 = None
	        permute_116 = torch.ops.aten.permute.default(primals_64, [1, 0])
	        mm_36 = torch.ops.aten.mm.default(mul_86, permute_116);  permute_116 = None
	        inductor_lookup_seed_default_23 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 23)
	        inductor_random_default_4 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_23, 'rand');  inductor_lookup_seed_default_23 = None
	        gt_23 = torch.ops.aten.gt.Scalar(inductor_random_default_4, 0.3);  inductor_random_default_4 = None
	        mul_87 = torch.ops.aten.mul.Tensor(gt_23, mm_36);  mm_36 = None
	        mul_88 = torch.ops.aten.mul.Tensor(mul_87, 1.4285714285714286);  mul_87 = None
	        add_33 = torch.ops.aten.add.Tensor(add_31, mul_88);  mul_88 = None
	        pow_18 = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_17 = torch.ops.aten.mean.dim(pow_18, [1], True);  pow_18 = None
	        add_34 = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None
	        rsqrt_17 = torch.ops.aten.rsqrt.default(add_34);  add_34 = None
	        mul_89 = torch.ops.aten.mul.Tensor(add_33, rsqrt_17)
	        mul_90 = torch.ops.aten.mul.Tensor(mul_89, primals_65);  mul_89 = None
	        inductor_lookup_seed_default_24 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 24)
	        inductor_random_default_3 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_24, 'rand');  inductor_lookup_seed_default_24 = None
	        gt_24 = torch.ops.aten.gt.Scalar(inductor_random_default_3, 0.3);  inductor_random_default_3 = None
	        mul_91 = torch.ops.aten.mul.Tensor(gt_24, mul_90);  mul_90 = None
	        mul_92 = torch.ops.aten.mul.Tensor(mul_91, 1.4285714285714286);  mul_91 = None
	        permute_117 = torch.ops.aten.permute.default(primals_66, [1, 0])
	        mm_37 = torch.ops.aten.mm.default(mul_92, permute_117);  permute_117 = None
	        split_10 = torch.ops.aten.split.Tensor(mm_37, 384, 1);  mm_37 = None
	        getitem_87 = split_10[0]
	        getitem_88 = split_10[1]
	        getitem_89 = split_10[2];  split_10 = None
	        view_40 = torch.ops.aten.view.default(getitem_87, [sym_size_int_3, 6, 64]);  getitem_87 = None
	        permute_118 = torch.ops.aten.permute.default(view_40, [1, 0, 2]);  view_40 = None
	        view_41 = torch.ops.aten.view.default(getitem_88, [sym_size_int_3, 6, 64]);  getitem_88 = None
	        permute_119 = torch.ops.aten.permute.default(view_41, [1, 0, 2]);  view_41 = None
	        view_42 = torch.ops.aten.view.default(getitem_89, [sym_size_int_3, 6, 64]);  getitem_89 = None
	        permute_120 = torch.ops.aten.permute.default(view_42, [1, 0, 2]);  view_42 = None
	        permute_121 = torch.ops.aten.permute.default(permute_118, [1, 0, 2]);  permute_118 = None
	        permute_122 = torch.ops.aten.permute.default(permute_119, [1, 0, 2]);  permute_119 = None
	        permute_123 = torch.ops.aten.permute.default(permute_120, [1, 0, 2]);  permute_120 = None
	        convert_element_type_20 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_30 = torch.ops.aten.unsqueeze.default(permute_121, 0);  permute_121 = None
	        unsqueeze_31 = torch.ops.aten.unsqueeze.default(permute_122, 0);  permute_122 = None
	        unsqueeze_32 = torch.ops.aten.unsqueeze.default(permute_123, 0);  permute_123 = None
	        _efficient_attention_forward_10 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_30, unsqueeze_31, unsqueeze_32, None, convert_element_type_20, convert_element_type_20, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_90 = _efficient_attention_forward_10[0]
	        getitem_91 = _efficient_attention_forward_10[1]
	        getitem_92 = _efficient_attention_forward_10[2]
	        getitem_93 = _efficient_attention_forward_10[3];  _efficient_attention_forward_10 = None
	        squeeze_10 = torch.ops.aten.squeeze.dim(getitem_90, 0)
	        permute_124 = torch.ops.aten.permute.default(squeeze_10, [1, 0, 2]);  squeeze_10 = None
	        permute_125 = torch.ops.aten.permute.default(permute_124, [1, 0, 2]);  permute_124 = None
	        view_43 = torch.ops.aten.view.default(permute_125, [sym_size_int_3, 384]);  permute_125 = None
	        permute_126 = torch.ops.aten.permute.default(primals_67, [1, 0])
	        mm_38 = torch.ops.aten.mm.default(view_43, permute_126);  view_43 = permute_126 = None
	        add_35 = torch.ops.aten.add.Tensor(add_33, mm_38);  mm_38 = None
	        pow_19 = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_18 = torch.ops.aten.mean.dim(pow_19, [1], True);  pow_19 = None
	        add_36 = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None
	        rsqrt_18 = torch.ops.aten.rsqrt.default(add_36);  add_36 = None
	        mul_93 = torch.ops.aten.mul.Tensor(add_33, rsqrt_18)
	        mul_94 = torch.ops.aten.mul.Tensor(mul_93, primals_68)
	        inductor_lookup_seed_default_25 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 25)
	        inductor_random_default_2 = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_25, 'rand');  inductor_lookup_seed_default_25 = None
	        gt_25 = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_95 = torch.ops.aten.mul.Tensor(gt_25, mul_94);  mul_94 = None
	        mul_96 = torch.ops.aten.mul.Tensor(mul_95, 1.4285714285714286);  mul_95 = None
	        permute_127 = torch.ops.aten.permute.default(primals_69, [1, 0])
	        mm_39 = torch.ops.aten.mm.default(mul_96, permute_127);  permute_127 = None
	        permute_128 = torch.ops.aten.permute.default(primals_70, [1, 0])
	        mm_40 = torch.ops.aten.mm.default(add_15, permute_128);  permute_128 = None
	        split_11 = torch.ops.aten.split.Tensor(mm_40, 384, 1);  mm_40 = None
	        getitem_96 = split_11[0]
	        getitem_97 = split_11[1];  split_11 = None
	        view_44 = torch.ops.aten.view.default(mm_39, [sym_size_int_3, 6, 64]);  mm_39 = None
	        permute_129 = torch.ops.aten.permute.default(view_44, [1, 0, 2]);  view_44 = None
	        view_45 = torch.ops.aten.view.default(getitem_96, [sym_size_int, 6, 64]);  getitem_96 = None
	        permute_130 = torch.ops.aten.permute.default(view_45, [1, 0, 2]);  view_45 = None
	        view_46 = torch.ops.aten.view.default(getitem_97, [sym_size_int, 6, 64]);  getitem_97 = None
	        permute_131 = torch.ops.aten.permute.default(view_46, [1, 0, 2]);  view_46 = None
	        permute_132 = torch.ops.aten.permute.default(permute_129, [1, 0, 2]);  permute_129 = None
	        permute_133 = torch.ops.aten.permute.default(permute_130, [1, 0, 2]);  permute_130 = None
	        permute_134 = torch.ops.aten.permute.default(permute_131, [1, 0, 2]);  permute_131 = None
	        convert_element_type_22 = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_23 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_33 = torch.ops.aten.unsqueeze.default(permute_132, 0);  permute_132 = None
	        unsqueeze_34 = torch.ops.aten.unsqueeze.default(permute_133, 0);  permute_133 = None
	        unsqueeze_35 = torch.ops.aten.unsqueeze.default(permute_134, 0);  permute_134 = None
	        _efficient_attention_forward_11 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_33, unsqueeze_34, unsqueeze_35, None, convert_element_type_22, convert_element_type_23, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_98 = _efficient_attention_forward_11[0]
	        getitem_99 = _efficient_attention_forward_11[1]
	        getitem_100 = _efficient_attention_forward_11[2]
	        getitem_101 = _efficient_attention_forward_11[3];  _efficient_attention_forward_11 = None
	        squeeze_11 = torch.ops.aten.squeeze.dim(getitem_98, 0)
	        permute_135 = torch.ops.aten.permute.default(squeeze_11, [1, 0, 2]);  squeeze_11 = None
	        permute_136 = torch.ops.aten.permute.default(permute_135, [1, 0, 2]);  permute_135 = None
	        view_47 = torch.ops.aten.view.default(permute_136, [sym_size_int_3, 384]);  permute_136 = None
	        permute_137 = torch.ops.aten.permute.default(primals_71, [1, 0])
	        mm_41 = torch.ops.aten.mm.default(view_47, permute_137);  view_47 = permute_137 = None
	        add_37 = torch.ops.aten.add.Tensor(add_35, mm_41);  add_35 = mm_41 = None
	        pow_20 = torch.ops.aten.pow.Tensor_Scalar(add_37, 2)
	        mean_19 = torch.ops.aten.mean.dim(pow_20, [1], True);  pow_20 = None
	        add_38 = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None
	        rsqrt_19 = torch.ops.aten.rsqrt.default(add_38);  add_38 = None
	        mul_97 = torch.ops.aten.mul.Tensor(add_37, rsqrt_19)
	        mul_98 = torch.ops.aten.mul.Tensor(mul_97, primals_72);  mul_97 = None
	        permute_138 = torch.ops.aten.permute.default(primals_73, [1, 0])
	        mm_42 = torch.ops.aten.mm.default(mul_98, permute_138);  permute_138 = None
	        sigmoid_7 = torch.ops.aten.sigmoid.default(mm_42)
	        mul_99 = torch.ops.aten.mul.Tensor(mm_42, sigmoid_7);  sigmoid_7 = None
	        inductor_lookup_seed_default_26 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 26)
	        inductor_random_default_1 = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_26, 'rand');  inductor_lookup_seed_default_26 = None
	        gt_26 = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_100 = torch.ops.aten.mul.Tensor(gt_26, mul_99);  mul_99 = None
	        mul_101 = torch.ops.aten.mul.Tensor(mul_100, 1.4285714285714286);  mul_100 = None
	        permute_139 = torch.ops.aten.permute.default(primals_74, [1, 0])
	        mm_43 = torch.ops.aten.mm.default(mul_101, permute_139);  permute_139 = None
	        inductor_lookup_seed_default_27 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 27);  inductor_seeds_default = None
	        inductor_random_default = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_27, 'rand');  inductor_lookup_seed_default_27 = None
	        gt_27 = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_102 = torch.ops.aten.mul.Tensor(gt_27, mm_43);  mm_43 = None
	        mul_103 = torch.ops.aten.mul.Tensor(mul_102, 1.4285714285714286);  mul_102 = None
	        add_39 = torch.ops.aten.add.Tensor(add_37, mul_103);  mul_103 = None
	        return (add_39, primals_31, primals_32, primals_33, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, add_3, rsqrt_2, gt_3, mul_14, convert_element_type_2, unsqueeze_3, unsqueeze_4, unsqueeze_5, getitem_12, getitem_13, getitem_14, getitem_15, add_5, rsqrt_3, mul_16, mm_6, gt_4, mul_19, gt_5, add_7, rsqrt_4, gt_6, mul_25, convert_element_type_4, unsqueeze_6, unsqueeze_7, unsqueeze_8, getitem_21, getitem_22, getitem_23, getitem_24, add_9, rsqrt_5, mul_27, mm_10, gt_7, mul_30, gt_8, add_11, rsqrt_6, gt_9, mul_36, convert_element_type_6, unsqueeze_9, unsqueeze_10, unsqueeze_11, getitem_30, getitem_31, getitem_32, getitem_33, add_13, rsqrt_7, mul_38, mm_14, gt_10, mul_41, gt_11, add_15, rsqrt_8, gt_12, mul_47, convert_element_type_8, unsqueeze_12, unsqueeze_13, unsqueeze_14, getitem_39, getitem_40, getitem_41, getitem_42, rsqrt_9, gt_13, mul_51, convert_element_type_10, convert_element_type_11, unsqueeze_15, unsqueeze_16, unsqueeze_17, getitem_47, getitem_48, getitem_49, getitem_50, add_19, rsqrt_10, mul_53, mm_21, gt_14, mul_56, gt_15, add_21, rsqrt_11, gt_16, mul_62, convert_element_type_12, unsqueeze_18, unsqueeze_19, unsqueeze_20, getitem_56, getitem_57, getitem_58, getitem_59, rsqrt_12, mul_63, gt_17, mul_66, convert_element_type_14, convert_element_type_15, unsqueeze_21, unsqueeze_22, unsqueeze_23, getitem_64, getitem_65, getitem_66, getitem_67, add_25, rsqrt_13, mul_68, mm_28, gt_18, mul_71, gt_19, add_27, rsqrt_14, gt_20, mul_77, convert_element_type_16, unsqueeze_24, unsqueeze_25, unsqueeze_26, getitem_73, getitem_74, getitem_75, getitem_76, rsqrt_15, mul_78, gt_21, mul_81, convert_element_type_18, convert_element_type_19, unsqueeze_27, unsqueeze_28, unsqueeze_29, getitem_81, getitem_82, getitem_83, getitem_84, add_31, rsqrt_16, mul_83, mm_35, gt_22, mul_86, gt_23, add_33, rsqrt_17, gt_24, mul_92, convert_element_type_20, unsqueeze_30, unsqueeze_31, unsqueeze_32, getitem_90, getitem_91, getitem_92, getitem_93, rsqrt_18, mul_93, gt_25, mul_96, convert_element_type_22, convert_element_type_23, unsqueeze_33, unsqueeze_34, unsqueeze_35, getitem_98, getitem_99, getitem_100, getitem_101, add_37, rsqrt_19, mul_98, mm_42, gt_26, mul_101, gt_27, sym_size_int, sym_size_int_1, sym_size_int_3, sym_size_int_4)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s1, 384), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j1)  # primals_5
	    buf4 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (384,), is_leaf=True)  # primals_6
	    buf5 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf5, (1152, 384), is_leaf=True)  # primals_7
	    buf6 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf6, (384, 384), is_leaf=True)  # primals_8
	    buf7 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf7, (384,), is_leaf=True)  # primals_9
	    buf8 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf8, (1024, 384), is_leaf=True)  # primals_10
	    buf9 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf9, (384, 1024), is_leaf=True)  # primals_11
	    buf10 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf10, (384,), is_leaf=True)  # primals_12
	    buf11 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf11, (1152, 384), is_leaf=True)  # primals_13
	    buf12 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf12, (384, 384), is_leaf=True)  # primals_14
	    buf13 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf13, (384,), is_leaf=True)  # primals_15
	    buf14 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf14, (1024, 384), is_leaf=True)  # primals_16
	    buf15 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf15, (384, 1024), is_leaf=True)  # primals_17
	    buf16 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf16, (384,), is_leaf=True)  # primals_18
	    buf17 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf17, (1152, 384), is_leaf=True)  # primals_19
	    buf18 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf18, (384, 384), is_leaf=True)  # primals_20
	    buf19 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf19, (384,), is_leaf=True)  # primals_21
	    buf20 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf20, (1024, 384), is_leaf=True)  # primals_22
	    buf21 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf21, (384, 1024), is_leaf=True)  # primals_23
	    buf22 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf22, (384,), is_leaf=True)  # primals_24
	    buf23 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf23, (1152, 384), is_leaf=True)  # primals_25
	    buf24 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf24, (384, 384), is_leaf=True)  # primals_26
	    buf25 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf25, (384,), is_leaf=True)  # primals_27
	    buf26 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf26, (1024, 384), is_leaf=True)  # primals_28
	    buf27 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf27, (384, 1024), is_leaf=True)  # primals_29
	    buf28 = reader.storage(None, 1536*s6, device=device(type='cuda', index=0))
	    reader.tensor(buf28, (s6, 384), is_leaf=True)  # primals_30
	    buf29 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf29, (257,), dtype=torch.int64, is_leaf=True)  # primals_31
	    buf30 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf30, (s8, 0), is_leaf=True)  # primals_32
	    buf31 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf31, (s9, 0), is_leaf=True)  # primals_33
	    reader.symint(j2)  # primals_34
	    buf32 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf32, (384,), is_leaf=True)  # primals_35
	    buf33 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf33, (1152, 384), is_leaf=True)  # primals_36
	    buf34 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf34, (384, 384), is_leaf=True)  # primals_37
	    buf35 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf35, (384,), is_leaf=True)  # primals_38
	    buf36 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf36, (384, 384), is_leaf=True)  # primals_39
	    buf37 = reader.storage(None, 1179648, device=device(type='cuda', index=0))
	    reader.tensor(buf37, (768, 384), is_leaf=True)  # primals_40
	    buf38 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf38, (384, 384), is_leaf=True)  # primals_41
	    buf39 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf39, (384,), is_leaf=True)  # primals_42
	    buf40 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf40, (1024, 384), is_leaf=True)  # primals_43
	    buf41 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf41, (384, 1024), is_leaf=True)  # primals_44
	    buf42 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf42, (384,), is_leaf=True)  # primals_45
	    buf43 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf43, (1152, 384), is_leaf=True)  # primals_46
	    buf44 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf44, (384, 384), is_leaf=True)  # primals_47
	    buf45 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf45, (384,), is_leaf=True)  # primals_48
	    buf46 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf46, (384, 384), is_leaf=True)  # primals_49
	    buf47 = reader.storage(None, 1179648, device=device(type='cuda', index=0))
	    reader.tensor(buf47, (768, 384), is_leaf=True)  # primals_50
	    buf48 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf48, (384, 384), is_leaf=True)  # primals_51
	    buf49 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf49, (384,), is_leaf=True)  # primals_52
	    buf50 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf50, (1024, 384), is_leaf=True)  # primals_53
	    buf51 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf51, (384, 1024), is_leaf=True)  # primals_54
	    buf52 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf52, (384,), is_leaf=True)  # primals_55
	    buf53 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf53, (1152, 384), is_leaf=True)  # primals_56
	    buf54 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf54, (384, 384), is_leaf=True)  # primals_57
	    buf55 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf55, (384,), is_leaf=True)  # primals_58
	    buf56 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf56, (384, 384), is_leaf=True)  # primals_59
	    buf57 = reader.storage(None, 1179648, device=device(type='cuda', index=0))
	    reader.tensor(buf57, (768, 384), is_leaf=True)  # primals_60
	    buf58 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf58, (384, 384), is_leaf=True)  # primals_61
	    buf59 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf59, (384,), is_leaf=True)  # primals_62
	    buf60 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf60, (1024, 384), is_leaf=True)  # primals_63
	    buf61 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf61, (384, 1024), is_leaf=True)  # primals_64
	    buf62 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf62, (384,), is_leaf=True)  # primals_65
	    buf63 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf63, (1152, 384), is_leaf=True)  # primals_66
	    buf64 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf64, (384, 384), is_leaf=True)  # primals_67
	    buf65 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf65, (384,), is_leaf=True)  # primals_68
	    buf66 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf66, (384, 384), is_leaf=True)  # primals_69
	    buf67 = reader.storage(None, 1179648, device=device(type='cuda', index=0))
	    reader.tensor(buf67, (768, 384), is_leaf=True)  # primals_70
	    buf68 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf68, (384, 384), is_leaf=True)  # primals_71
	    buf69 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf69, (384,), is_leaf=True)  # primals_72
	    buf70 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf70, (1024, 384), is_leaf=True)  # primals_73
	    buf71 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf71, (384, 1024), is_leaf=True)  # primals_74
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:09:55.018703 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "7c14de10bc06c25f3de5c4c1acba0605"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0", primals_30: "f32[s6, 384][384, 1]cuda:0", primals_31: "i64[257][1]cuda:0", primals_32: "f32[s8, 0][1, 1]cuda:0", primals_33: "f32[s9, 0][1, 1]cuda:0", primals_34: "Sym(s5)", primals_35: "f32[384][1]cuda:0", primals_36: "f32[1152, 384][384, 1]cuda:0", primals_37: "f32[384, 384][384, 1]cuda:0", primals_38: "f32[384][1]cuda:0", primals_39: "f32[384, 384][384, 1]cuda:0", primals_40: "f32[768, 384][384, 1]cuda:0", primals_41: "f32[384, 384][384, 1]cuda:0", primals_42: "f32[384][1]cuda:0", primals_43: "f32[1024, 384][384, 1]cuda:0", primals_44: "f32[384, 1024][1024, 1]cuda:0", primals_45: "f32[384][1]cuda:0", primals_46: "f32[1152, 384][384, 1]cuda:0", primals_47: "f32[384, 384][384, 1]cuda:0", primals_48: "f32[384][1]cuda:0", primals_49: "f32[384, 384][384, 1]cuda:0", primals_50: "f32[768, 384][384, 1]cuda:0", primals_51: "f32[384, 384][384, 1]cuda:0", primals_52: "f32[384][1]cuda:0", primals_53: "f32[1024, 384][384, 1]cuda:0", primals_54: "f32[384, 1024][1024, 1]cuda:0", primals_55: "f32[384][1]cuda:0", primals_56: "f32[1152, 384][384, 1]cuda:0", primals_57: "f32[384, 384][384, 1]cuda:0", primals_58: "f32[384][1]cuda:0", primals_59: "f32[384, 384][384, 1]cuda:0", primals_60: "f32[768, 384][384, 1]cuda:0", primals_61: "f32[384, 384][384, 1]cuda:0", primals_62: "f32[384][1]cuda:0", primals_63: "f32[1024, 384][384, 1]cuda:0", primals_64: "f32[384, 1024][1024, 1]cuda:0", primals_65: "f32[384][1]cuda:0", primals_66: "f32[1152, 384][384, 1]cuda:0", primals_67: "f32[384, 384][384, 1]cuda:0", primals_68: "f32[384][1]cuda:0", primals_69: "f32[384, 384][384, 1]cuda:0", primals_70: "f32[768, 384][384, 1]cuda:0", primals_71: "f32[384, 384][384, 1]cuda:0", primals_72: "f32[384][1]cuda:0", primals_73: "f32[1024, 384][384, 1]cuda:0", primals_74: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[28][1]cuda:0" = torch.ops.prims.inductor_seeds.default(28, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_27, 0.3);  inductor_random_default_27 = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_1: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_26, 0.3);  inductor_random_default_26 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_2: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2)
	        inductor_random_default_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_25, 0.3);  inductor_random_default_25 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12);  mul_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_3: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 3)
	        inductor_random_default_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_3, 'rand');  inductor_lookup_seed_default_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_24, 0.3);  inductor_random_default_24 = None
	        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
	        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
	        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0, 2]);  view_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_16, 0);  permute_16 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_18, 0);  permute_18 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
	        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
	        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
	        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_3, view_7, permute_21);  view_7 = permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_10, 2)
	        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_10, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15);  mul_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_4: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 4)
	        inductor_random_default_23: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_4, 'rand');  inductor_lookup_seed_default_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_23, 0.3);  inductor_random_default_23 = None
	        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_5: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 5)
	        inductor_random_default_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_5, 'rand');  inductor_lookup_seed_default_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_22, 0.3);  inductor_random_default_22 = None
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_10, mul_21);  mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18);  mul_22 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_6: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 6)
	        inductor_random_default_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_6, 'rand');  inductor_lookup_seed_default_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_21, 0.3);  inductor_random_default_21 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
	        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
	        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_8, [1, 0, 2]);  view_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_9, [1, 0, 2]);  view_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_10, [1, 0, 2]);  view_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_25, [1, 0, 2]);  permute_25 = None
	        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_26, [1, 0, 2]);  permute_26 = None
	        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_27, [1, 0, 2]);  permute_27 = None
	        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_28, 0);  permute_28 = None
	        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_29, 0);  permute_29 = None
	        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_30, 0);  permute_30 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
	        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
	        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
	        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_7, view_11, permute_33);  view_11 = permute_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_9, 2)
	        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21);  mul_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_7: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 7)
	        inductor_random_default_20: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_7, 'rand');  inductor_lookup_seed_default_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_20, 0.3);  inductor_random_default_20 = None
	        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_8: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 8)
	        inductor_random_default_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_8, 'rand');  inductor_lookup_seed_default_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_19, 0.3);  inductor_random_default_19 = None
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_9, mul_32);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24);  mul_33 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_9: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 9)
	        inductor_random_default_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_9, 'rand');  inductor_lookup_seed_default_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_18, 0.3);  inductor_random_default_18 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
	        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
	        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_12, [1, 0, 2]);  view_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_13, [1, 0, 2]);  view_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_14, [1, 0, 2]);  view_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_37, [1, 0, 2]);  permute_37 = None
	        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_38, [1, 0, 2]);  permute_38 = None
	        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_39, [1, 0, 2]);  permute_39 = None
	        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_40, 0);  permute_40 = None
	        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_41, 0);  permute_41 = None
	        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_42, 0);  permute_42 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
	        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
	        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
	        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_11, view_15, permute_45);  view_15 = permute_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_8, 2)
	        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_8, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27);  mul_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_10: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 10)
	        inductor_random_default_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_10, 'rand');  inductor_lookup_seed_default_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_17, 0.3);  inductor_random_default_17 = None
	        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_11: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 11)
	        inductor_random_default_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_11, 'rand');  inductor_lookup_seed_default_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_16, 0.3);  inductor_random_default_16 = None
	        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_8, mul_43);  mul_43 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_9: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_9, [1], True);  pow_9 = None
	        add_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_8, 1e-06);  mean_8 = None
	        rsqrt_8: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_16);  add_16 = None
	        mul_44: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_8)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_45: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_44, primals_35);  mul_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int_3: "Sym(s6)" = torch.ops.aten.sym_size.int(primals_30, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_12: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 12)
	        inductor_random_default_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_12, 'rand');  inductor_lookup_seed_default_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_12: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_15, 0.3);  inductor_random_default_15 = None
	        mul_46: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_12, mul_45);  mul_45 = None
	        mul_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_46, 1.4285714285714286);  mul_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_48: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_36, [1, 0])
	        mm_16: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_47, permute_48);  permute_48 = None
	        split_4 = torch.ops.aten.split.Tensor(mm_16, 384, 1);  mm_16 = None
	        getitem_36: "f32[s6, 384][1152, 1]cuda:0" = split_4[0]
	        getitem_37: "f32[s6, 384][1152, 1]cuda:0" = split_4[1]
	        getitem_38: "f32[s6, 384][1152, 1]cuda:0" = split_4[2];  split_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_16: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_36, [sym_size_int_3, 6, 64]);  getitem_36 = None
	        permute_49: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_16, [1, 0, 2]);  view_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_17: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_37, [sym_size_int_3, 6, 64]);  getitem_37 = None
	        permute_50: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_17, [1, 0, 2]);  view_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_18: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_38, [sym_size_int_3, 6, 64]);  getitem_38 = None
	        permute_51: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_18, [1, 0, 2]);  view_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_52: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_49, [1, 0, 2]);  permute_49 = None
	        permute_53: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_50, [1, 0, 2]);  permute_50 = None
	        permute_54: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_51, [1, 0, 2]);  permute_51 = None
	        convert_element_type_8: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_12: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_52, 0);  permute_52 = None
	        unsqueeze_13: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_53, 0);  permute_53 = None
	        unsqueeze_14: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_54, 0);  permute_54 = None
	        sym_size_int_4: "Sym(s9)" = torch.ops.aten.sym_size.int(primals_33, 0)
	        _efficient_attention_forward_4 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_12, unsqueeze_13, unsqueeze_14, None, convert_element_type_8, convert_element_type_8, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_39: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_4[0]
	        getitem_40: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_4[1]
	        getitem_41: "i64[][]cuda:0" = _efficient_attention_forward_4[2]
	        getitem_42: "i64[][]cuda:0" = _efficient_attention_forward_4[3];  _efficient_attention_forward_4 = None
	        squeeze_4: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_39, 0)
	        permute_55: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_4, [1, 0, 2]);  squeeze_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_56: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_55, [1, 0, 2]);  permute_55 = None
	        view_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_56, [sym_size_int_3, 384]);  permute_56 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_57: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_37, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_7: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(primals_30, view_19, permute_57);  view_19 = permute_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_30, 2)
	        mean_9: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_10, [1], True);  pow_10 = None
	        add_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_9, 1e-06);  mean_9 = None
	        rsqrt_9: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_18);  add_18 = None
	        mul_48: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_30, rsqrt_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_49: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_48, primals_38);  mul_48 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_13: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 13)
	        inductor_random_default_14: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_13, 'rand');  inductor_lookup_seed_default_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_13: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_14, 0.3);  inductor_random_default_14 = None
	        mul_50: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_13, mul_49);  mul_49 = None
	        mul_51: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_50, 1.4285714285714286);  mul_50 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_58: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_39, [1, 0])
	        mm_18: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_51, permute_58);  permute_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_59: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_40, [1, 0])
	        mm_19: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_59);  permute_59 = None
	        split_5 = torch.ops.aten.split.Tensor(mm_19, 384, 1);  mm_19 = None
	        getitem_45: "f32[s1, 384][768, 1]cuda:0" = split_5[0]
	        getitem_46: "f32[s1, 384][768, 1]cuda:0" = split_5[1];  split_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_20: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.reshape.default(mm_18, [sym_size_int_3, 6, 64]);  mm_18 = None
	        permute_60: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_20, [1, 0, 2]);  view_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_21: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_45, [sym_size_int, 6, 64]);  getitem_45 = None
	        permute_61: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_21, [1, 0, 2]);  view_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_22: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_46, [sym_size_int, 6, 64]);  getitem_46 = None
	        permute_62: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_22, [1, 0, 2]);  view_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_63: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_60, [1, 0, 2]);  permute_60 = None
	        permute_64: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_61, [1, 0, 2]);  permute_61 = None
	        permute_65: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_62, [1, 0, 2]);  permute_62 = None
	        convert_element_type_10: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_11: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_15: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_63, 0);  permute_63 = None
	        unsqueeze_16: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_64, 0);  permute_64 = None
	        unsqueeze_17: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_65, 0);  permute_65 = None
	        _efficient_attention_forward_5 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_15, unsqueeze_16, unsqueeze_17, None, convert_element_type_10, convert_element_type_11, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_47: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_5[0]
	        getitem_48: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_5[1]
	        getitem_49: "i64[][]cuda:0" = _efficient_attention_forward_5[2]
	        getitem_50: "i64[][]cuda:0" = _efficient_attention_forward_5[3];  _efficient_attention_forward_5 = None
	        squeeze_5: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_47, 0)
	        permute_66: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_5, [1, 0, 2]);  squeeze_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_67: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_66, [1, 0, 2]);  permute_66 = None
	        view_23: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_67, [sym_size_int_3, 384]);  permute_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_68: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_41, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_6: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(addmm_default_7, view_23, permute_68);  addmm_default_7 = view_23 = permute_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_6, 2)
	        mean_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_11, [1], True);  pow_11 = None
	        add_20: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_10, 1e-06);  mean_10 = None
	        rsqrt_10: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_20);  add_20 = None
	        mul_52: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_6, rsqrt_10)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_53: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, primals_42);  mul_52 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_69: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_43, [1, 0])
	        mm_21: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_53, permute_69);  permute_69 = None
	        sigmoid_4: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_21)
	        mul_54: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_21, sigmoid_4);  sigmoid_4 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_14: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 14)
	        inductor_random_default_13: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_14, 'rand');  inductor_lookup_seed_default_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_14: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_13, 0.3);  inductor_random_default_13 = None
	        mul_55: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_14, mul_54);  mul_54 = None
	        mul_56: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, 1.4285714285714286);  mul_55 = None
	        permute_70: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_44, [1, 0])
	        mm_22: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_56, permute_70);  permute_70 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_15: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 15)
	        inductor_random_default_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_15, 'rand');  inductor_lookup_seed_default_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_15: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_12, 0.3);  inductor_random_default_12 = None
	        mul_57: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_15, mm_22);  mm_22 = None
	        mul_58: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_57, 1.4285714285714286);  mul_57 = None
	        add_21: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_6, mul_58);  mul_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_12: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_11: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_12, [1], True);  pow_12 = None
	        add_22: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None
	        rsqrt_11: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
	        mul_59: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_11)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_60: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_59, primals_45);  mul_59 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_16: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 16)
	        inductor_random_default_11: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_16, 'rand');  inductor_lookup_seed_default_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_16: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_11, 0.3);  inductor_random_default_11 = None
	        mul_61: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_16, mul_60);  mul_60 = None
	        mul_62: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_61, 1.4285714285714286);  mul_61 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_71: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_46, [1, 0])
	        mm_23: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_62, permute_71);  permute_71 = None
	        split_6 = torch.ops.aten.split.Tensor(mm_23, 384, 1);  mm_23 = None
	        getitem_53: "f32[s6, 384][1152, 1]cuda:0" = split_6[0]
	        getitem_54: "f32[s6, 384][1152, 1]cuda:0" = split_6[1]
	        getitem_55: "f32[s6, 384][1152, 1]cuda:0" = split_6[2];  split_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_24: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_53, [sym_size_int_3, 6, 64]);  getitem_53 = None
	        permute_72: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_24, [1, 0, 2]);  view_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_25: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_54, [sym_size_int_3, 6, 64]);  getitem_54 = None
	        permute_73: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_25, [1, 0, 2]);  view_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_26: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_55, [sym_size_int_3, 6, 64]);  getitem_55 = None
	        permute_74: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_26, [1, 0, 2]);  view_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_75: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_72, [1, 0, 2]);  permute_72 = None
	        permute_76: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_73, [1, 0, 2]);  permute_73 = None
	        permute_77: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_74, [1, 0, 2]);  permute_74 = None
	        convert_element_type_12: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_18: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_75, 0);  permute_75 = None
	        unsqueeze_19: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_76, 0);  permute_76 = None
	        unsqueeze_20: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_77, 0);  permute_77 = None
	        _efficient_attention_forward_6 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_18, unsqueeze_19, unsqueeze_20, None, convert_element_type_12, convert_element_type_12, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_56: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_6[0]
	        getitem_57: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_6[1]
	        getitem_58: "i64[][]cuda:0" = _efficient_attention_forward_6[2]
	        getitem_59: "i64[][]cuda:0" = _efficient_attention_forward_6[3];  _efficient_attention_forward_6 = None
	        squeeze_6: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_56, 0)
	        permute_78: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_6, [1, 0, 2]);  squeeze_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_79: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_78, [1, 0, 2]);  permute_78 = None
	        view_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_79, [sym_size_int_3, 384]);  permute_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_80: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_47, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_5: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_21, view_27, permute_80);  view_27 = permute_80 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_13: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_21, 2)
	        mean_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_13, [1], True);  pow_13 = None
	        add_24: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None
	        rsqrt_12: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_24);  add_24 = None
	        mul_63: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_21, rsqrt_12)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_64: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_63, primals_48)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_17: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 17)
	        inductor_random_default_10: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_17, 'rand');  inductor_lookup_seed_default_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_17: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_10, 0.3);  inductor_random_default_10 = None
	        mul_65: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_17, mul_64);  mul_64 = None
	        mul_66: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, 1.4285714285714286);  mul_65 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_81: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_49, [1, 0])
	        mm_25: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_66, permute_81);  permute_81 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_82: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_50, [1, 0])
	        mm_26: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_82);  permute_82 = None
	        split_7 = torch.ops.aten.split.Tensor(mm_26, 384, 1);  mm_26 = None
	        getitem_62: "f32[s1, 384][768, 1]cuda:0" = split_7[0]
	        getitem_63: "f32[s1, 384][768, 1]cuda:0" = split_7[1];  split_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_28: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.reshape.default(mm_25, [sym_size_int_3, 6, 64]);  mm_25 = None
	        permute_83: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_28, [1, 0, 2]);  view_28 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_29: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_62, [sym_size_int, 6, 64]);  getitem_62 = None
	        permute_84: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_29, [1, 0, 2]);  view_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_30: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_63, [sym_size_int, 6, 64]);  getitem_63 = None
	        permute_85: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_30, [1, 0, 2]);  view_30 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_86: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_83, [1, 0, 2]);  permute_83 = None
	        permute_87: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_84, [1, 0, 2]);  permute_84 = None
	        permute_88: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_85, [1, 0, 2]);  permute_85 = None
	        convert_element_type_14: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_15: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_21: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_86, 0);  permute_86 = None
	        unsqueeze_22: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_87, 0);  permute_87 = None
	        unsqueeze_23: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_88, 0);  permute_88 = None
	        _efficient_attention_forward_7 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_21, unsqueeze_22, unsqueeze_23, None, convert_element_type_14, convert_element_type_15, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_64: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_7[0]
	        getitem_65: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_7[1]
	        getitem_66: "i64[][]cuda:0" = _efficient_attention_forward_7[2]
	        getitem_67: "i64[][]cuda:0" = _efficient_attention_forward_7[3];  _efficient_attention_forward_7 = None
	        squeeze_7: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_64, 0)
	        permute_89: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_7, [1, 0, 2]);  squeeze_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_90: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_89, [1, 0, 2]);  permute_89 = None
	        view_31: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_90, [sym_size_int_3, 384]);  permute_90 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_91: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_51, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_4: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(addmm_default_5, view_31, permute_91);  addmm_default_5 = view_31 = permute_91 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_14: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_4, 2)
	        mean_13: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_14, [1], True);  pow_14 = None
	        add_26: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None
	        rsqrt_13: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_26);  add_26 = None
	        mul_67: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_4, rsqrt_13)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_68: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_67, primals_52);  mul_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_92: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_53, [1, 0])
	        mm_28: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_68, permute_92);  permute_92 = None
	        sigmoid_5: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_28)
	        mul_69: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_28, sigmoid_5);  sigmoid_5 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_18: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 18)
	        inductor_random_default_9: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_18, 'rand');  inductor_lookup_seed_default_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_18: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_9, 0.3);  inductor_random_default_9 = None
	        mul_70: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_18, mul_69);  mul_69 = None
	        mul_71: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_70, 1.4285714285714286);  mul_70 = None
	        permute_93: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_54, [1, 0])
	        mm_29: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_71, permute_93);  permute_93 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_19: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 19)
	        inductor_random_default_8: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_19, 'rand');  inductor_lookup_seed_default_19 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_19: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_8, 0.3);  inductor_random_default_8 = None
	        mul_72: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_19, mm_29);  mm_29 = None
	        mul_73: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_72, 1.4285714285714286);  mul_72 = None
	        add_27: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_4, mul_73);  mul_73 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_15: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_15, [1], True);  pow_15 = None
	        add_28: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None
	        rsqrt_14: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_28);  add_28 = None
	        mul_74: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_14)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_75: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_74, primals_55);  mul_74 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_20: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 20)
	        inductor_random_default_7: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_20, 'rand');  inductor_lookup_seed_default_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_20: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_7, 0.3);  inductor_random_default_7 = None
	        mul_76: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_20, mul_75);  mul_75 = None
	        mul_77: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_76, 1.4285714285714286);  mul_76 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_94: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_56, [1, 0])
	        mm_30: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_77, permute_94);  permute_94 = None
	        split_8 = torch.ops.aten.split.Tensor(mm_30, 384, 1);  mm_30 = None
	        getitem_70: "f32[s6, 384][1152, 1]cuda:0" = split_8[0]
	        getitem_71: "f32[s6, 384][1152, 1]cuda:0" = split_8[1]
	        getitem_72: "f32[s6, 384][1152, 1]cuda:0" = split_8[2];  split_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_32: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_70, [sym_size_int_3, 6, 64]);  getitem_70 = None
	        permute_95: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_32, [1, 0, 2]);  view_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_33: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_71, [sym_size_int_3, 6, 64]);  getitem_71 = None
	        permute_96: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_33, [1, 0, 2]);  view_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_34: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_72, [sym_size_int_3, 6, 64]);  getitem_72 = None
	        permute_97: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_34, [1, 0, 2]);  view_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_98: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_95, [1, 0, 2]);  permute_95 = None
	        permute_99: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_96, [1, 0, 2]);  permute_96 = None
	        permute_100: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_97, [1, 0, 2]);  permute_97 = None
	        convert_element_type_16: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_24: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_98, 0);  permute_98 = None
	        unsqueeze_25: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_99, 0);  permute_99 = None
	        unsqueeze_26: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_100, 0);  permute_100 = None
	        _efficient_attention_forward_8 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_24, unsqueeze_25, unsqueeze_26, None, convert_element_type_16, convert_element_type_16, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_73: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_8[0]
	        getitem_74: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_8[1]
	        getitem_75: "i64[][]cuda:0" = _efficient_attention_forward_8[2]
	        getitem_76: "i64[][]cuda:0" = _efficient_attention_forward_8[3];  _efficient_attention_forward_8 = None
	        squeeze_8: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_73, 0)
	        permute_101: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_8, [1, 0, 2]);  squeeze_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_102: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_101, [1, 0, 2]);  permute_101 = None
	        view_35: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_102, [sym_size_int_3, 384]);  permute_102 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_103: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_57, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_3: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_27, view_35, permute_103);  view_35 = permute_103 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_16: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_27, 2)
	        mean_15: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_16, [1], True);  pow_16 = None
	        add_30: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None
	        rsqrt_15: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_30);  add_30 = None
	        mul_78: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_27, rsqrt_15)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_79: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_78, primals_58)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_21: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 21)
	        inductor_random_default_6: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_21, 'rand');  inductor_lookup_seed_default_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_21: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_6, 0.3);  inductor_random_default_6 = None
	        mul_80: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_21, mul_79);  mul_79 = None
	        mul_81: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, 1.4285714285714286);  mul_80 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_104: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_59, [1, 0])
	        mm_32: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_81, permute_104);  permute_104 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_105: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_60, [1, 0])
	        mm_33: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_105);  permute_105 = None
	        split_9 = torch.ops.aten.split.Tensor(mm_33, 384, 1);  mm_33 = None
	        getitem_79: "f32[s1, 384][768, 1]cuda:0" = split_9[0]
	        getitem_80: "f32[s1, 384][768, 1]cuda:0" = split_9[1];  split_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_36: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.reshape.default(mm_32, [sym_size_int_3, 6, 64]);  mm_32 = None
	        permute_106: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_36, [1, 0, 2]);  view_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_37: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_79, [sym_size_int, 6, 64]);  getitem_79 = None
	        permute_107: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_37, [1, 0, 2]);  view_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_38: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_80, [sym_size_int, 6, 64]);  getitem_80 = None
	        permute_108: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_38, [1, 0, 2]);  view_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_109: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_106, [1, 0, 2]);  permute_106 = None
	        permute_110: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_107, [1, 0, 2]);  permute_107 = None
	        permute_111: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_108, [1, 0, 2]);  permute_108 = None
	        convert_element_type_18: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_19: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_27: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_109, 0);  permute_109 = None
	        unsqueeze_28: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_110, 0);  permute_110 = None
	        unsqueeze_29: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_111, 0);  permute_111 = None
	        _efficient_attention_forward_9 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_27, unsqueeze_28, unsqueeze_29, None, convert_element_type_18, convert_element_type_19, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_81: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_9[0]
	        getitem_82: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_9[1]
	        getitem_83: "i64[][]cuda:0" = _efficient_attention_forward_9[2]
	        getitem_84: "i64[][]cuda:0" = _efficient_attention_forward_9[3];  _efficient_attention_forward_9 = None
	        squeeze_9: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_81, 0)
	        permute_112: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_9, [1, 0, 2]);  squeeze_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_113: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_112, [1, 0, 2]);  permute_112 = None
	        view_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_113, [sym_size_int_3, 384]);  permute_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_114: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_61, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_2: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(addmm_default_3, view_39, permute_114);  addmm_default_3 = view_39 = permute_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_17: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_2, 2)
	        mean_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_17, [1], True);  pow_17 = None
	        add_32: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None
	        rsqrt_16: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_32);  add_32 = None
	        mul_82: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_2, rsqrt_16)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_83: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_82, primals_62);  mul_82 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_115: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_63, [1, 0])
	        mm_35: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_83, permute_115);  permute_115 = None
	        sigmoid_6: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_35)
	        mul_84: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_35, sigmoid_6);  sigmoid_6 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_22: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 22)
	        inductor_random_default_5: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_22, 'rand');  inductor_lookup_seed_default_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_22: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_5, 0.3);  inductor_random_default_5 = None
	        mul_85: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_22, mul_84);  mul_84 = None
	        mul_86: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, 1.4285714285714286);  mul_85 = None
	        permute_116: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_64, [1, 0])
	        mm_36: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_86, permute_116);  permute_116 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_23: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 23)
	        inductor_random_default_4: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_23, 'rand');  inductor_lookup_seed_default_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_23: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_4, 0.3);  inductor_random_default_4 = None
	        mul_87: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_23, mm_36);  mm_36 = None
	        mul_88: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, 1.4285714285714286);  mul_87 = None
	        add_33: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_2, mul_88);  mul_88 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_18: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_17: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_18, [1], True);  pow_18 = None
	        add_34: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None
	        rsqrt_17: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_34);  add_34 = None
	        mul_89: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_17)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_90: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_89, primals_65);  mul_89 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_24: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 24)
	        inductor_random_default_3: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_24, 'rand');  inductor_lookup_seed_default_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_24: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_3, 0.3);  inductor_random_default_3 = None
	        mul_91: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_24, mul_90);  mul_90 = None
	        mul_92: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_91, 1.4285714285714286);  mul_91 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_117: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_66, [1, 0])
	        mm_37: "f32[s6, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_92, permute_117);  permute_117 = None
	        split_10 = torch.ops.aten.split.Tensor(mm_37, 384, 1);  mm_37 = None
	        getitem_87: "f32[s6, 384][1152, 1]cuda:0" = split_10[0]
	        getitem_88: "f32[s6, 384][1152, 1]cuda:0" = split_10[1]
	        getitem_89: "f32[s6, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_40: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_87, [sym_size_int_3, 6, 64]);  getitem_87 = None
	        permute_118: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_40, [1, 0, 2]);  view_40 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_41: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_88, [sym_size_int_3, 6, 64]);  getitem_88 = None
	        permute_119: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_41, [1, 0, 2]);  view_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_42: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_89, [sym_size_int_3, 6, 64]);  getitem_89 = None
	        permute_120: "f32[6, s6, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_42, [1, 0, 2]);  view_42 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_121: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_118, [1, 0, 2]);  permute_118 = None
	        permute_122: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_119, [1, 0, 2]);  permute_119 = None
	        permute_123: "f32[s6, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_120, [1, 0, 2]);  permute_120 = None
	        convert_element_type_20: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        unsqueeze_30: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_121, 0);  permute_121 = None
	        unsqueeze_31: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_122, 0);  permute_122 = None
	        unsqueeze_32: "f32[1, s6, 6, 64][1152*s6, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_123, 0);  permute_123 = None
	        _efficient_attention_forward_10 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_30, unsqueeze_31, unsqueeze_32, None, convert_element_type_20, convert_element_type_20, sym_size_int_4, sym_size_int_4, 0.0, 1, True)
	        getitem_90: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_10[0]
	        getitem_91: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_10[1]
	        getitem_92: "i64[][]cuda:0" = _efficient_attention_forward_10[2]
	        getitem_93: "i64[][]cuda:0" = _efficient_attention_forward_10[3];  _efficient_attention_forward_10 = None
	        squeeze_10: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_90, 0)
	        permute_124: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_10, [1, 0, 2]);  squeeze_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_125: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_124, [1, 0, 2]);  permute_124 = None
	        view_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_125, [sym_size_int_3, 384]);  permute_125 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_126: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_67, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_1: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_33, view_43, permute_126);  view_43 = permute_126 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_19: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_33, 2)
	        mean_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_19, [1], True);  pow_19 = None
	        add_36: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None
	        rsqrt_18: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_36);  add_36 = None
	        mul_93: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_33, rsqrt_18)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_94: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_93, primals_68)
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_25: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 25)
	        inductor_random_default_2: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_25, 'rand');  inductor_lookup_seed_default_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:79 in forward, code: x=self.do(self.cross_attn_norm(x)), x_kv=x_kv, padding_mask=padding_mask, is_causal=False, jagged=jagged, use_cache=not self.training and self.enable_kv_cache
	        gt_25: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_95: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_25, mul_94);  mul_94 = None
	        mul_96: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_95, 1.4285714285714286);  mul_95 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:197 in forward, code: queries = self.q(x)
	        permute_127: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_69, [1, 0])
	        mm_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_96, permute_127);  permute_127 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:198 in forward, code: keys, values = self.kv(x_kv).chunk(2, dim=-1)
	        permute_128: "f32[384, 768][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_70, [1, 0])
	        mm_40: "f32[s1, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_15, permute_128);  permute_128 = None
	        split_11 = torch.ops.aten.split.Tensor(mm_40, 384, 1);  mm_40 = None
	        getitem_96: "f32[s1, 384][768, 1]cuda:0" = split_11[0]
	        getitem_97: "f32[s1, 384][768, 1]cuda:0" = split_11[1];  split_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_44: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.reshape.default(mm_39, [sym_size_int_3, 6, 64]);  mm_39 = None
	        permute_129: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_44, [1, 0, 2]);  view_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_45: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_96, [sym_size_int, 6, 64]);  getitem_96 = None
	        permute_130: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_45, [1, 0, 2]);  view_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_46: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_97, [sym_size_int, 6, 64]);  getitem_97 = None
	        permute_131: "f32[6, s1, 64][64, 768, 1]cuda:0" = torch.ops.aten.permute.default(view_46, [1, 0, 2]);  view_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_132: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_129, [1, 0, 2]);  permute_129 = None
	        permute_133: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_130, [1, 0, 2]);  permute_130 = None
	        permute_134: "f32[s1, 6, 64][768, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_131, [1, 0, 2]);  permute_131 = None
	        convert_element_type_22: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_31, torch.int32)
	        convert_element_type_23: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_33: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_132, 0);  permute_132 = None
	        unsqueeze_34: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_133, 0);  permute_133 = None
	        unsqueeze_35: "f32[1, s1, 6, 64][768*s1, 768, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_134, 0);  permute_134 = None
	        _efficient_attention_forward_11 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_33, unsqueeze_34, unsqueeze_35, None, convert_element_type_22, convert_element_type_23, sym_size_int_4, sym_size_int_1, 0.0, 0, True)
	        getitem_98: "f32[1, s6, 6, 64][384*s6, 384, 64, 1]cuda:0" = _efficient_attention_forward_11[0]
	        getitem_99: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s9, 32))][192*CeilToInt(IntTrueDiv(s9, 32)), 32*CeilToInt(IntTrueDiv(s9, 32)), 1]cuda:0" = _efficient_attention_forward_11[1]
	        getitem_100: "i64[][]cuda:0" = _efficient_attention_forward_11[2]
	        getitem_101: "i64[][]cuda:0" = _efficient_attention_forward_11[3];  _efficient_attention_forward_11 = None
	        squeeze_11: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0)
	        permute_135: "f32[6, s6, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_11, [1, 0, 2]);  squeeze_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_136: "f32[s6, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_135, [1, 0, 2]);  permute_135 = None
	        view_47: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_136, [sym_size_int_3, 384]);  permute_136 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_137: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_71, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(addmm_default_1, view_47, permute_137);  addmm_default_1 = view_47 = permute_137 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_20: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default, 2)
	        mean_19: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_20, [1], True);  pow_20 = None
	        add_38: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None
	        rsqrt_19: "f32[s6, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_38);  add_38 = None
	        mul_97: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default, rsqrt_19)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_98: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_97, primals_72);  mul_97 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_138: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_73, [1, 0])
	        mm_42: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_98, permute_138);  permute_138 = None
	        sigmoid_7: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_42)
	        mul_99: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, sigmoid_7);  sigmoid_7 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_26: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 26)
	        inductor_random_default_1: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 1024], inductor_lookup_seed_default_26, 'rand');  inductor_lookup_seed_default_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_26: "b8[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_100: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_26, mul_99);  mul_99 = None
	        mul_101: "f32[s6, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_100, 1.4285714285714286);  mul_100 = None
	        permute_139: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_74, [1, 0])
	        mm_43: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_101, permute_139);  permute_139 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_27: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 27);  inductor_seeds_default = None
	        inductor_random_default: "f32[s6, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int_3, 384], inductor_lookup_seed_default_27, 'rand');  inductor_lookup_seed_default_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_27: "b8[s6, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_102: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_27, mm_43);  mm_43 = None
	        mul_103: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, 1.4285714285714286);  mul_102 = None
	        add_39: "f32[s6, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default, mul_103);  mul_103 = None
	        return (add_39, primals_31, primals_32, primals_33, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, add_3, rsqrt_2, gt_3, mul_14, convert_element_type_2, unsqueeze_3, unsqueeze_4, unsqueeze_5, getitem_12, getitem_13, getitem_14, getitem_15, addmm_default_10, rsqrt_3, mul_16, mm_6, gt_4, mul_19, gt_5, add_7, rsqrt_4, gt_6, mul_25, convert_element_type_4, unsqueeze_6, unsqueeze_7, unsqueeze_8, getitem_21, getitem_22, getitem_23, getitem_24, addmm_default_9, rsqrt_5, mul_27, mm_10, gt_7, mul_30, gt_8, add_11, rsqrt_6, gt_9, mul_36, convert_element_type_6, unsqueeze_9, unsqueeze_10, unsqueeze_11, getitem_30, getitem_31, getitem_32, getitem_33, addmm_default_8, rsqrt_7, mul_38, mm_14, gt_10, mul_41, gt_11, add_15, rsqrt_8, gt_12, mul_47, convert_element_type_8, unsqueeze_12, unsqueeze_13, unsqueeze_14, getitem_39, getitem_40, getitem_41, getitem_42, rsqrt_9, gt_13, mul_51, convert_element_type_10, convert_element_type_11, unsqueeze_15, unsqueeze_16, unsqueeze_17, getitem_47, getitem_48, getitem_49, getitem_50, addmm_default_6, rsqrt_10, mul_53, mm_21, gt_14, mul_56, gt_15, add_21, rsqrt_11, gt_16, mul_62, convert_element_type_12, unsqueeze_18, unsqueeze_19, unsqueeze_20, getitem_56, getitem_57, getitem_58, getitem_59, rsqrt_12, mul_63, gt_17, mul_66, convert_element_type_14, convert_element_type_15, unsqueeze_21, unsqueeze_22, unsqueeze_23, getitem_64, getitem_65, getitem_66, getitem_67, addmm_default_4, rsqrt_13, mul_68, mm_28, gt_18, mul_71, gt_19, add_27, rsqrt_14, gt_20, mul_77, convert_element_type_16, unsqueeze_24, unsqueeze_25, unsqueeze_26, getitem_73, getitem_74, getitem_75, getitem_76, rsqrt_15, mul_78, gt_21, mul_81, convert_element_type_18, convert_element_type_19, unsqueeze_27, unsqueeze_28, unsqueeze_29, getitem_81, getitem_82, getitem_83, getitem_84, addmm_default_2, rsqrt_16, mul_83, mm_35, gt_22, mul_86, gt_23, add_33, rsqrt_17, gt_24, mul_92, convert_element_type_20, unsqueeze_30, unsqueeze_31, unsqueeze_32, getitem_90, getitem_91, getitem_92, getitem_93, rsqrt_18, mul_93, gt_25, mul_96, convert_element_type_22, convert_element_type_23, unsqueeze_33, unsqueeze_34, unsqueeze_35, getitem_98, getitem_99, getitem_100, getitem_101, addmm_default, rsqrt_19, mul_98, mm_42, gt_26, mul_101, gt_27, sym_size_int, sym_size_int_1, sym_size_int_3, sym_size_int_4)
	        
V0303 09:09:55.022884 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "3a4a0fd4587bdf7641da84ccfe38943d"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992995022792.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:55.427748 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "bfba9c2aaa1da8729d2e81e4681f9356"}
	{
	"name": "GraphLowering.run",
	"ts": 1740992995427676.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:55.433369 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "70b6b67ff39cd8198e5675cdf66e65ab"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992995433293.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:55.433563 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "f00979f9ceff58cae86b77da2eda8773"}
	{
	"name": "code_gen",
	"ts": 1740992995433293.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:55.437265 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "124495626f5fd4b4324911487453aa60"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992995437199.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:55.821654 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "b889e452f543f194949a5ba37fb9aa50"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740992995821587.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:55.821948 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "99097c7eb24d3304f41aaee11686c4eb"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992995821879.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.040716 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "594b2f8d49ce6db7313ebd6b0a6e179e"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740992996040444.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.041339 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "d112e302203ca9824dac14b27c0a58bb"}
	{
	"name": "code_gen",
	"ts": 1740992996041275.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.041562 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "7ca4feb7149742a365f96bbe031e3728"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740992996041513.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.042356 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "2b9bb7603fe44d3a6b93da7f2f02ab59"}
	{
	"name": "inductor_compile",
	"ts": 1740992996042301.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.042574 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "9c53bbffb6dc06a71c473a5cacc8412f"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992996042523.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.043330 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "e8f82b84d14aab8aa07154dea9dc1195"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992996043275.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.046545 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "f93e49d511faac5cd29e5e7752ba6011"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992996046487.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.047033 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "47bb1a6025f3e7b1dcb56e31f083cf1c"}
	{
	"name": "backend_compile",
	"ts": 1740992996046984.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.047239 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "5d2c1dbad56c82b33e4826b4b706005e"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992996047194.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.049973 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "ebf6562ac3d40f308bfacb2e6782b77d"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992996049920.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.050173 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0, "has_payload": "c2eb4937816304920fa878dcda2d5da7"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992996050129.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 6,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.050518 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "6/0", "frame_key": "21", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py", "co_firstlineno": 174, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992984.292553, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp7jha6wvz/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmp7jha6wvz/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp7jha6wvz', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 11.757811784744263, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 6, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.057198 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 122, "name": "forward", "filename": 8}]}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.057576 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "efac46a2404b8699e7b9116b6afe2dab"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740992996057465.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.057824 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "daa556271b8915b20c68d653085deafa"}
	{
	"name": "entire_frame_compile",
	"ts": 1740992996057465.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.069385 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 18, "size": 6291456}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.069703 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 18, "size": 6291456}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.069981 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [384, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4069d10>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.070161 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 18, "size": 2056}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.070393 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.070562 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 18, "size": 0}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.070785 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.070948 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 18, "size": 0}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.071165 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.071748 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j1", 384, 1], "storage": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9f4069630>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.071913 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 4, "source": "L['x']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.097388 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.097670 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12090>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.097850 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 6, "source": "L['self']._modules['layers']._modules['0']._modules['attn_norm']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.109517 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 18, "size": 1769472}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.109789 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12c70>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.109968 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 7, "source": "L['self']._modules['layers']._modules['0']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.157763 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 18, "size": 589824}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.158052 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 32, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12d60>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.158217 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 32, "source": "L['self']._modules['layers']._modules['0']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.178304 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.178576 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 35, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e50>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.178742 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 35, "source": "L['self']._modules['layers']._modules['0']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.185599 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.185973 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 36, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e00>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.186141 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 36, "source": "L['self']._modules['layers']._modules['0']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.202326 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.202598 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 38, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b123b0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.202765 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 38, "source": "L['self']._modules['layers']._modules['0']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.229094 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 11, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.229370 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 41, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 11, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12270>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.229533 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 41, "source": "L['self']._modules['layers']._modules['1']._modules['attn_norm']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.239951 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 12, "describer_id": 18, "size": 1769472}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.240221 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 42, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 12, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12ea0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.240383 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 42, "source": "L['self']._modules['layers']._modules['1']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.283611 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 13, "describer_id": 18, "size": 589824}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.283883 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 67, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 13, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12f90>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.284051 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 67, "source": "L['self']._modules['layers']._modules['1']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.303739 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 14, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.304011 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 70, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 14, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b124a0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.304182 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 70, "source": "L['self']._modules['layers']._modules['1']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.310948 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 15, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.311215 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 71, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 15, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12b30>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.311475 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 71, "source": "L['self']._modules['layers']._modules['1']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.326410 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 16, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.326680 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 73, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 16, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b122c0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.326848 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 73, "source": "L['self']._modules['layers']._modules['1']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.352509 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 17, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.352778 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 76, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 17, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17950>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.352941 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 76, "source": "L['self']._modules['layers']._modules['2']._modules['attn_norm']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.363373 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 18, "describer_id": 18, "size": 1769472}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.363705 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 77, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 18, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17bd0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.363874 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 77, "source": "L['self']._modules['layers']._modules['2']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.406178 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 19, "describer_id": 18, "size": 589824}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.406448 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 102, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 19, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17680>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.406617 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 102, "source": "L['self']._modules['layers']._modules['2']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.426350 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 20, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.426620 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 105, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 20, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17d60>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.426784 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 105, "source": "L['self']._modules['layers']._modules['2']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.433521 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 21, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.433789 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 106, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 21, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17c70>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.434060 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 106, "source": "L['self']._modules['layers']._modules['2']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.449033 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 22, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.449303 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 108, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 22, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17b30>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.449471 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 108, "source": "L['self']._modules['layers']._modules['2']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.474974 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 23, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.475252 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 111, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 23, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b179a0>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.475417 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 111, "source": "L['self']._modules['layers']._modules['3']._modules['attn_norm']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.485752 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 24, "describer_id": 18, "size": 1769472}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.486616 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 112, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 24, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17810>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.486779 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 112, "source": "L['self']._modules['layers']._modules['3']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.528881 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 25, "describer_id": 18, "size": 589824}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.529154 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 137, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 25, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17130>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.529321 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 137, "source": "L['self']._modules['layers']._modules['3']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.548970 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 26, "describer_id": 18, "size": 1536}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.549248 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 140, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 26, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17540>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.549415 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 140, "source": "L['self']._modules['layers']._modules['3']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.556196 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 27, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.556463 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 141, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 27, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17860>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.556739 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 141, "source": "L['self']._modules['layers']._modules['3']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.571617 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 28, "describer_id": 18, "size": 1572864}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.571882 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 143, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 28, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b17900>", "describer_id": 18}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.572047 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 18, "id": 143, "source": "L['self']._modules['layers']._modules['3']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:09:56.620100 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_": [384], "l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_": [384], "l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_": [384], "l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024], "l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_": [384], "l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024]}}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "07205d63c258bf1d9ce8d0a6e66826ac"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_
	        l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_
	        l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_
	        l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_
	        l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_
	        l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_
	        l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_
	        l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_
	        l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1.pow(2)
	        mean: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        add: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean + 1e-06;  mean = None
	        rsqrt: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        mul: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1 * rsqrt;  float_1 = rsqrt = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul.type_as(l_x_);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output * l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_;  output = l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_1, 0.3, True, False);  mul_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout, l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_, None);  dropout = l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk = linear.chunk(3, dim = -1);  linear = None
	        queries: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[0]
	        keys: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[1]
	        values: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[2];  chunk = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries.unflatten(-1, [6, 64]);  queries = None
	        queries_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_1: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys.unflatten(-1, [6, 64]);  keys = None
	        keys_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_2: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values.unflatten(-1, [6, 64]);  values = None
	        values_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_1, keys_1, values_1, dropout_p = False, is_causal = False);  queries_1 = keys_1 = values_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_3: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
	        context_vec_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_1, l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_, None);  context_vec_1 = l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_ + context_vec_2;  l_x_ = context_vec_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2.pow(2)
	        mean_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_2.mean(-1, keepdim = True);  pow_2 = None
	        add_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_1 + 1e-06;  mean_1 = None
	        rsqrt_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_2);  add_2 = None
	        mul_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2 * rsqrt_1;  float_2 = rsqrt_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_2.type_as(attn_out);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_1 * l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_;  output_1 = l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_2: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_1, l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_1 = l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_3: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_2, inplace = False);  input_2 = None
	        input_4: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_3, 0.3, True, False);  input_3 = None
	        input_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_4, l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_4 = l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_5, 0.3, True, False);  input_5 = None
	        proj_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out + input_6;  attn_out = input_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_3.pow(2)
	        mean_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_3.mean(-1, keepdim = True);  pow_3 = None
	        add_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_2 + 1e-06;  mean_2 = None
	        rsqrt_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_4);  add_4 = None
	        mul_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_3 * rsqrt_2;  float_3 = rsqrt_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_4.type_as(proj_out);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_2 * l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_;  output_2 = l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_5, 0.3, True, False);  mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_4: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_3, l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_, None);  dropout_3 = l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_1 = linear_4.chunk(3, dim = -1);  linear_4 = None
	        queries_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[0]
	        keys_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[1]
	        values_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[2];  chunk_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_3: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_2.unflatten(-1, [6, 64]);  queries_2 = None
	        queries_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_3.transpose(1, 2);  unflatten_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_4: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_2.unflatten(-1, [6, 64]);  keys_2 = None
	        keys_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_4.transpose(1, 2);  unflatten_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_5: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_2.unflatten(-1, [6, 64]);  values_2 = None
	        values_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_5.transpose(1, 2);  unflatten_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_3: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_3, keys_3, values_3, dropout_p = False, is_causal = False);  queries_3 = keys_3 = values_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_7: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_3.transpose(1, 2);  context_vec_3 = None
	        context_vec_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_7.flatten(-2);  transpose_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_4, l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_, None);  context_vec_4 = l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out + context_vec_5;  proj_out = context_vec_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_1.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_4.pow(2)
	        mean_3: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_4.mean(-1, keepdim = True);  pow_4 = None
	        add_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_3 + 1e-06;  mean_3 = None
	        rsqrt_3: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_6);  add_6 = None
	        mul_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_4 * rsqrt_3;  float_4 = rsqrt_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_6.type_as(attn_out_1);  mul_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_3 * l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_;  output_3 = l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_8: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_7, l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_7 = l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_9: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_8, inplace = False);  input_8 = None
	        input_10: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_9, 0.3, True, False);  input_9 = None
	        input_11: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_10, l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_10 = l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_12: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_11, 0.3, True, False);  input_11 = None
	        proj_out_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_1 + input_12;  attn_out_1 = input_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_1.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_5.pow(2)
	        mean_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_5.mean(-1, keepdim = True);  pow_5 = None
	        add_8: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_4 + 1e-06;  mean_4 = None
	        rsqrt_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_8);  add_8 = None
	        mul_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_5 * rsqrt_4;  float_5 = rsqrt_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_8.type_as(proj_out_1);  mul_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_9: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_4 * l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_;  output_4 = l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_9, 0.3, True, False);  mul_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_8: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_6, l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_, None);  dropout_6 = l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_2 = linear_8.chunk(3, dim = -1);  linear_8 = None
	        queries_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[0]
	        keys_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[1]
	        values_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[2];  chunk_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_6: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_4.unflatten(-1, [6, 64]);  queries_4 = None
	        queries_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_6.transpose(1, 2);  unflatten_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_7: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_4.unflatten(-1, [6, 64]);  keys_4 = None
	        keys_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_7.transpose(1, 2);  unflatten_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_8: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_4.unflatten(-1, [6, 64]);  values_4 = None
	        values_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_8.transpose(1, 2);  unflatten_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_6: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_5, keys_5, values_5, dropout_p = False, is_causal = False);  queries_5 = keys_5 = values_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_11: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_6.transpose(1, 2);  context_vec_6 = None
	        context_vec_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_11.flatten(-2);  transpose_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_7, l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_, None);  context_vec_7 = l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_1 + context_vec_8;  proj_out_1 = context_vec_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_2.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_6.pow(2)
	        mean_5: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_6.mean(-1, keepdim = True);  pow_6 = None
	        add_10: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_5 + 1e-06;  mean_5 = None
	        rsqrt_5: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_10);  add_10 = None
	        mul_10: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_6 * rsqrt_5;  float_6 = rsqrt_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_10.type_as(attn_out_2);  mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_13: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_5 * l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_;  output_5 = l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_14: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_13, l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_13 = l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_15: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_14, inplace = False);  input_14 = None
	        input_16: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_15, 0.3, True, False);  input_15 = None
	        input_17: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_16, l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_16 = l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_18: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_17, 0.3, True, False);  input_17 = None
	        proj_out_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_2 + input_18;  attn_out_2 = input_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_2.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_7.pow(2)
	        mean_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_7.mean(-1, keepdim = True);  pow_7 = None
	        add_12: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_6 + 1e-06;  mean_6 = None
	        rsqrt_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_12);  add_12 = None
	        mul_12: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_7 * rsqrt_6;  float_7 = rsqrt_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_12.type_as(proj_out_2);  mul_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_13: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_6 * l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_;  output_6 = l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout_9: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_13, 0.3, True, False);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear_12: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_9, l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_, None);  dropout_9 = l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk_3 = linear_12.chunk(3, dim = -1);  linear_12 = None
	        queries_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[0]
	        keys_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[1]
	        values_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[2];  chunk_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_9: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_6.unflatten(-1, [6, 64]);  queries_6 = None
	        queries_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_9.transpose(1, 2);  unflatten_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_10: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_6.unflatten(-1, [6, 64]);  keys_6 = None
	        keys_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_10.transpose(1, 2);  unflatten_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_11: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_6.unflatten(-1, [6, 64]);  values_6 = None
	        values_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_11.transpose(1, 2);  unflatten_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec_9: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_7, keys_7, values_7, dropout_p = False, is_causal = False);  queries_7 = keys_7 = values_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_15: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_9.transpose(1, 2);  context_vec_9 = None
	        context_vec_10: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_15.flatten(-2);  transpose_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_11: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_10, l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_, None);  context_vec_10 = l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_2 + context_vec_11;  proj_out_2 = context_vec_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_3.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_8.pow(2)
	        mean_7: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_8.mean(-1, keepdim = True);  pow_8 = None
	        add_14: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_7 + 1e-06;  mean_7 = None
	        rsqrt_7: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_14);  add_14 = None
	        mul_14: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_8 * rsqrt_7;  float_8 = rsqrt_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_14.type_as(attn_out_3);  mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_19: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_7 * l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_;  output_7 = l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_20: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_19, l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_19 = l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_21: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_20, inplace = False);  input_20 = None
	        input_22: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_21, 0.3, True, False);  input_21 = None
	        input_23: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_22, l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_22 = l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_24: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_23, 0.3, True, False);  input_23 = None
	        proj_out_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_3 + input_24;  attn_out_3 = input_24 = None
	        return (proj_out_3,)
	        
V0303 09:09:56.620709 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "8438572e0ca79052b0ace450d4eff4ee"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740992996620634.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.620897 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "c2a687a6d8ece54e7bb933f84b59224e"}
	{
	"name": "backend_compile",
	"ts": 1740992996620634.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:56.682824 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "0d52f3bb14d65c974ba7e34dda9f5de4"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740992996682763.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:59.031026 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "9749d79e3083a7f6d526d63bbb5d62eb"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[384][1]cuda:0"; primals_7: "f32[1152, 384][384, 1]cuda:0"; primals_8: "f32[384, 384][384, 1]cuda:0"; primals_9: "f32[384][1]cuda:0"; primals_10: "f32[1024, 384][384, 1]cuda:0"; primals_11: "f32[384, 1024][1024, 1]cuda:0"; primals_12: "f32[384][1]cuda:0"; primals_13: "f32[1152, 384][384, 1]cuda:0"; primals_14: "f32[384, 384][384, 1]cuda:0"; primals_15: "f32[384][1]cuda:0"; primals_16: "f32[1024, 384][384, 1]cuda:0"; primals_17: "f32[384, 1024][1024, 1]cuda:0"; primals_18: "f32[384][1]cuda:0"; primals_19: "f32[1152, 384][384, 1]cuda:0"; primals_20: "f32[384, 384][384, 1]cuda:0"; primals_21: "f32[384][1]cuda:0"; primals_22: "f32[1024, 384][384, 1]cuda:0"; primals_23: "f32[384, 1024][1024, 1]cuda:0"; primals_24: "f32[384][1]cuda:0"; primals_25: "f32[1152, 384][384, 1]cuda:0"; primals_26: "f32[384, 384][384, 1]cuda:0"; primals_27: "f32[384][1]cuda:0"; primals_28: "f32[1024, 384][384, 1]cuda:0"; primals_29: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        alias: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
	        alias_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
	        alias_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        rand: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        alias_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        alias_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        alias_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_6: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_6, [1, 0, 2]);  alias_6 = None
	        alias_7: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_7, [1, 0, 2]);  alias_7 = None
	        alias_8: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_8, [1, 0, 2]);  alias_8 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_11: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
	        alias_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
	        alias_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_11, 0);  alias_11 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_12, 0);  alias_12 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_13, 0);  alias_13 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        alias_14: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_3)
	        alias_15: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0);  getitem_3 = None
	        alias_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_17: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_7);  permute_7 = None
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_17, [1, 0, 2]);  alias_17 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        alias_18: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_1)
	        alias_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
	        alias_20: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        rand_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_1, 0.3);  rand_1 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_2, 0.3);  rand_2 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        alias_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_2)
	        alias_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
	        alias_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_3, 0.3);  rand_3 = None
	        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
	        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
	        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        alias_24: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_4);  view_4 = None
	        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        alias_25: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_5);  view_5 = None
	        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        alias_26: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_6);  view_6 = None
	        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_26, [1, 0, 2]);  alias_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_13);  permute_13 = None
	        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_27, [1, 0, 2]);  alias_27 = None
	        alias_28: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_14);  permute_14 = None
	        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_28, [1, 0, 2]);  alias_28 = None
	        alias_29: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_15);  permute_15 = None
	        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_29, [1, 0, 2]);  alias_29 = None
	        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_3: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_32: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_16);  permute_16 = None
	        alias_33: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_17);  permute_17 = None
	        alias_34: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_18);  permute_18 = None
	        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_32, 0);  alias_32 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_33, 0);  alias_33 = None
	        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_34, 0);  alias_34 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_3, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
	        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
	        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
	        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        alias_35: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_12)
	        alias_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_35);  alias_35 = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0);  getitem_12 = None
	        alias_37: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_37, [1, 0, 2]);  alias_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_38: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_19);  permute_19 = None
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_38, [1, 0, 2]);  alias_38 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
	        mm_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_7, permute_21);  permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
	        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        alias_39: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_3)
	        alias_40: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_39);  alias_39 = None
	        alias_41: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_40);  alias_40 = None
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        rand_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_4, 0.3);  rand_4 = None
	        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_5, 0.3);  rand_5 = None
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        alias_42: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_4)
	        alias_43: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_42);  alias_42 = None
	        alias_44: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_43);  alias_43 = None
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_6, 0.3);  rand_6 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
	        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
	        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        alias_45: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_8);  view_8 = None
	        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_45, [1, 0, 2]);  alias_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        alias_46: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_9);  view_9 = None
	        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_46, [1, 0, 2]);  alias_46 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        alias_47: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_10);  view_10 = None
	        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_47, [1, 0, 2]);  alias_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_48: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_25);  permute_25 = None
	        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_48, [1, 0, 2]);  alias_48 = None
	        alias_49: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_26);  permute_26 = None
	        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_49, [1, 0, 2]);  alias_49 = None
	        alias_50: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_27);  permute_27 = None
	        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_50, [1, 0, 2]);  alias_50 = None
	        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_5: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_53: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_28);  permute_28 = None
	        alias_54: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_29);  permute_29 = None
	        alias_55: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_30);  permute_30 = None
	        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_53, 0);  alias_53 = None
	        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_54, 0);  alias_54 = None
	        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_55, 0);  alias_55 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_5, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
	        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
	        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
	        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        alias_56: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_21)
	        alias_57: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_56);  alias_56 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0);  getitem_21 = None
	        alias_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_58, [1, 0, 2]);  alias_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_59: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_31);  permute_31 = None
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_59, [1, 0, 2]);  alias_59 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
	        mm_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_11, permute_33);  permute_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
	        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        alias_60: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_5)
	        alias_61: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_60);  alias_60 = None
	        alias_62: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_61);  alias_61 = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        rand_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_7, 0.3);  rand_7 = None
	        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_8, 0.3);  rand_8 = None
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        alias_63: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_6)
	        alias_64: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_63);  alias_63 = None
	        alias_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_64);  alias_64 = None
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        rand_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_9, 0.3);  rand_9 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
	        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
	        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        alias_66: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_12);  view_12 = None
	        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_66, [1, 0, 2]);  alias_66 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        alias_67: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_13);  view_13 = None
	        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_67, [1, 0, 2]);  alias_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        alias_68: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_14);  view_14 = None
	        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_68, [1, 0, 2]);  alias_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_69: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_37);  permute_37 = None
	        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_69, [1, 0, 2]);  alias_69 = None
	        alias_70: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_38);  permute_38 = None
	        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_70, [1, 0, 2]);  alias_70 = None
	        alias_71: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_39);  permute_39 = None
	        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_71, [1, 0, 2]);  alias_71 = None
	        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_7: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_74: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_40);  permute_40 = None
	        alias_75: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_41);  permute_41 = None
	        alias_76: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_42);  permute_42 = None
	        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_74, 0);  alias_74 = None
	        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_75, 0);  alias_75 = None
	        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_76, 0);  alias_76 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_7, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
	        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
	        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
	        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        alias_77: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_30)
	        alias_78: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_77);  alias_77 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0);  getitem_30 = None
	        alias_79: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_79, [1, 0, 2]);  alias_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_43);  permute_43 = None
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_80, [1, 0, 2]);  alias_80 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
	        mm_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_15, permute_45);  permute_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
	        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        alias_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_7)
	        alias_82: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_81);  alias_81 = None
	        alias_83: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_82);  alias_82 = None
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        rand_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_10, 0.3);  rand_10 = None
	        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_11, 0.3);  rand_11 = None
	        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
	        convert_element_type_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_11, torch.float32);  gt_11 = None
	        mul_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_8, 1.4285714285714286);  convert_element_type_8 = None
	        mul_45: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_44);  mul_44 = None
	        clone: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_45, memory_format = torch.contiguous_format);  mul_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone, primals_29);  primals_29 = None
	        permute_48: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone, [1, 0]);  clone = None
	        mm_17: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_48, mul_41);  permute_48 = mul_41 = None
	        convert_element_type_9: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_10, torch.float32);  gt_10 = None
	        mul_46: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_9, 1.4285714285714286);  convert_element_type_9 = None
	        mul_47: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_16, mul_46);  mm_16 = mul_46 = None
	        clone_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_47, memory_format = torch.contiguous_format);  mul_47 = None
	        sigmoid_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        full_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_4);  full_8 = None
	        mul_48: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sub);  mm_14 = sub = None
	        add_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_48, 1);  mul_48 = None
	        mul_49: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_4, add_16);  sigmoid_4 = add_16 = None
	        mul_50: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_1, mul_49);  clone_1 = mul_49 = None
	        mm_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_50, primals_28);  primals_28 = None
	        permute_50: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_50, [1, 0]);  mul_50 = None
	        mm_19: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_50, mul_38);  permute_50 = mul_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, mul_37);  mul_37 = None
	        mul_52: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, primals_27);  mm_18 = primals_27 = None
	        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_51, [0], True);  mul_51 = None
	        unsqueeze_12: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view_16: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_12, [384]);  unsqueeze_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_53: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, add_13)
	        mul_54: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, rsqrt_7);  mul_52 = rsqrt_7 = None
	        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_53, [1], True);  mul_53 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_54);  tangents_1 = mul_54 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_84: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_83);  alias_83 = None
	        alias_85: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_84);  alias_84 = None
	        alias_86: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_85);  alias_85 = None
	        pow_9: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_86, 3);  alias_86 = None
	        mul_55: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
	        mul_56: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, pow_9);  mul_55 = pow_9 = None
	        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_56, [-1, 384]);  mul_56 = None
	        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
	        pow_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 1.0);  add_13 = None
	        mul_57: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_10, 2.0);  pow_10 = None
	        mul_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_57);  div = mul_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_17, mul_58);  add_17 = mul_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_18, primals_26);  primals_26 = None
	        permute_51: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_18, [1, 0])
	        mm_21: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_51, view_15);  permute_51 = view_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [sym_size_int, 6, 64]);  mm_20 = None
	        alias_87: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_17);  view_17 = None
	        permute_52: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_87, [1, 0, 2]);  alias_87 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_88: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_52);  permute_52 = None
	        permute_53: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_88, [1, 0, 2]);  alias_88 = None
	        alias_89: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_53);  permute_53 = None
	        unsqueeze_13: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_89, 0);  alias_89 = None
	        alias_90: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_78);  alias_78 = None
	        alias_91: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_90);  alias_90 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_13, unsqueeze_9, unsqueeze_10, unsqueeze_11, None, alias_91, convert_element_type_6, convert_element_type_7, sym_size_int_1, sym_size_int_1, getitem_31, 0.0, getitem_32, getitem_33, 0, False);  unsqueeze_13 = unsqueeze_9 = unsqueeze_10 = unsqueeze_11 = alias_91 = convert_element_type_6 = convert_element_type_7 = getitem_31 = getitem_32 = getitem_33 = None
	        getitem_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_37: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_38: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_38, 0);  getitem_38 = None
	        squeeze_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_37, 0);  getitem_37 = None
	        squeeze_6: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_36, 0);  getitem_36 = None
	        alias_92: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_4);  squeeze_4 = None
	        permute_54: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_92, [1, 0, 2]);  alias_92 = None
	        alias_93: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_5);  squeeze_5 = None
	        permute_55: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_93, [1, 0, 2]);  alias_93 = None
	        alias_94: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_6);  squeeze_6 = None
	        permute_56: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_94, [1, 0, 2]);  alias_94 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_95: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_54);  permute_54 = None
	        permute_57: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_95, [1, 0, 2]);  alias_95 = None
	        view_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_57, [sym_size_int, 384]);  permute_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_96: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_55);  permute_55 = None
	        permute_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_96, [1, 0, 2]);  alias_96 = None
	        view_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_58, [sym_size_int, 384]);  permute_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_97: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_56);  permute_56 = None
	        permute_59: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_97, [1, 0, 2]);  alias_97 = None
	        view_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_59, [sym_size_int, 384]);  permute_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_4 = torch.ops.aten.split.Tensor(full_9, 384, 1)
	        getitem_40: "f32[s1, 384][1152, 1]cuda:0" = split_4[0];  split_4 = None
	        alias_98: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_40);  getitem_40 = None
	        alias_99: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_20);  view_20 = None
	        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_98, alias_99);  alias_98 = alias_99 = None
	        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy, 1, 0, 384);  full_9 = copy = None
	        alias_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_19);  view_19 = None
	        split_7 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_50: "f32[s1, 384][1152, 1]cuda:0" = split_7[1];  split_7 = None
	        alias_103: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_50);  getitem_50 = None
	        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_103, alias_102);  alias_103 = alias_102 = None
	        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        alias_106: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_18);  view_18 = None
	        split_10 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
	        getitem_60: "f32[s1, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
	        alias_107: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_60);  getitem_60 = None
	        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_107, alias_106);  alias_107 = alias_106 = None
	        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
	        mm_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_25);  primals_25 = None
	        permute_61: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
	        mm_23: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_61, mul_36);  permute_61 = mul_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_9, torch.float32);  gt_9 = None
	        mul_59: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_10, 1.4285714285714286);  convert_element_type_10 = None
	        mul_60: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_22, mul_59);  mm_22 = mul_59 = None
	        clone_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_60, memory_format = torch.contiguous_format);  mul_60 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_61: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, mul_33);  mul_33 = None
	        mul_62: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, primals_24);  clone_2 = primals_24 = None
	        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_61, [0], True);  mul_61 = None
	        unsqueeze_14: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
	        view_21: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_14, [384]);  unsqueeze_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_63: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, add_11)
	        mul_64: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, rsqrt_6);  mul_62 = rsqrt_6 = None
	        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_63, [1], True);  mul_63 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_18, mul_64);  add_18 = mul_64 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_109: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_65);  alias_65 = None
	        alias_110: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_109);  alias_109 = None
	        alias_111: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_110);  alias_110 = None
	        pow_11: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_111, 3);  alias_111 = None
	        mul_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
	        mul_66: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, pow_11);  mul_65 = pow_11 = None
	        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_66, [-1, 384]);  mul_66 = None
	        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
	        pow_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 1.0);  add_11 = None
	        mul_67: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_12, 2.0);  pow_12 = None
	        mul_68: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_67);  div_1 = mul_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, mul_68);  add_19 = mul_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_8, torch.float32);  gt_8 = None
	        mul_69: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, 1.4285714285714286);  convert_element_type_11 = None
	        mul_70: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_20, mul_69);  mul_69 = None
	        clone_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_70, memory_format = torch.contiguous_format);  mul_70 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_24: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_3, primals_23);  primals_23 = None
	        permute_62: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_3, [1, 0]);  clone_3 = None
	        mm_25: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_62, mul_30);  permute_62 = mul_30 = None
	        convert_element_type_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_7, torch.float32);  gt_7 = None
	        mul_71: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, 1.4285714285714286);  convert_element_type_12 = None
	        mul_72: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_24, mul_71);  mm_24 = mul_71 = None
	        clone_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_72, memory_format = torch.contiguous_format);  mul_72 = None
	        sigmoid_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        full_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_10, sigmoid_5);  full_10 = None
	        mul_73: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sub_1);  mm_10 = sub_1 = None
	        add_21: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_73, 1);  mul_73 = None
	        mul_74: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_5, add_21);  sigmoid_5 = add_21 = None
	        mul_75: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_4, mul_74);  clone_4 = mul_74 = None
	        mm_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_75, primals_22);  primals_22 = None
	        permute_64: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_75, [1, 0]);  mul_75 = None
	        mm_27: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_64, mul_27);  permute_64 = mul_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_76: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, mul_26);  mul_26 = None
	        mul_77: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, primals_21);  mm_26 = primals_21 = None
	        sum_5: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_76, [0], True);  mul_76 = None
	        unsqueeze_15: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_5, 0);  sum_5 = None
	        view_22: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_15, [384]);  unsqueeze_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, add_9)
	        mul_79: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, rsqrt_5);  mul_77 = rsqrt_5 = None
	        sum_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_78, [1], True);  mul_78 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_20, mul_79);  add_20 = mul_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_112: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_62);  alias_62 = None
	        alias_113: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_112);  alias_112 = None
	        alias_114: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_113);  alias_113 = None
	        pow_13: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_114, 3);  alias_114 = None
	        mul_80: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_6, -0.5);  sum_6 = None
	        mul_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, pow_13);  mul_80 = pow_13 = None
	        expand_2: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_81, [-1, 384]);  mul_81 = None
	        div_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_2, 384);  expand_2 = None
	        pow_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 1.0);  add_9 = None
	        mul_82: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_14, 2.0);  pow_14 = None
	        mul_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_2, mul_82);  div_2 = mul_82 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_22, mul_83);  add_22 = mul_83 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_23, primals_20);  primals_20 = None
	        permute_65: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_23, [1, 0])
	        mm_29: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_65, view_11);  permute_65 = view_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_28, [sym_size_int, 6, 64]);  mm_28 = None
	        alias_115: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_23);  view_23 = None
	        permute_66: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_115, [1, 0, 2]);  alias_115 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_116: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_66);  permute_66 = None
	        permute_67: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_116, [1, 0, 2]);  alias_116 = None
	        alias_117: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_67);  permute_67 = None
	        unsqueeze_16: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_117, 0);  alias_117 = None
	        alias_118: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_57);  alias_57 = None
	        alias_119: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_118);  alias_118 = None
	        _efficient_attention_backward_1 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_16, unsqueeze_6, unsqueeze_7, unsqueeze_8, None, alias_119, convert_element_type_4, convert_element_type_5, sym_size_int_1, sym_size_int_1, getitem_22, 0.0, getitem_23, getitem_24, 0, False);  unsqueeze_16 = unsqueeze_6 = unsqueeze_7 = unsqueeze_8 = alias_119 = convert_element_type_4 = convert_element_type_5 = getitem_22 = getitem_23 = getitem_24 = None
	        getitem_67: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[0]
	        getitem_68: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[1]
	        getitem_69: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[2];  _efficient_attention_backward_1 = None
	        squeeze_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_69, 0);  getitem_69 = None
	        squeeze_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_68, 0);  getitem_68 = None
	        squeeze_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_67, 0);  getitem_67 = None
	        alias_120: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_7);  squeeze_7 = None
	        permute_68: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_120, [1, 0, 2]);  alias_120 = None
	        alias_121: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_8);  squeeze_8 = None
	        permute_69: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_121, [1, 0, 2]);  alias_121 = None
	        alias_122: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_9);  squeeze_9 = None
	        permute_70: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_122, [1, 0, 2]);  alias_122 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_123: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_68);  permute_68 = None
	        permute_71: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_123, [1, 0, 2]);  alias_123 = None
	        view_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_71, [sym_size_int, 384]);  permute_71 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_124: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_69);  permute_69 = None
	        permute_72: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_124, [1, 0, 2]);  alias_124 = None
	        view_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_72, [sym_size_int, 384]);  permute_72 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_125: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_70);  permute_70 = None
	        permute_73: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_125, [1, 0, 2]);  alias_125 = None
	        view_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_73, [sym_size_int, 384]);  permute_73 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_11: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_13 = torch.ops.aten.split.Tensor(full_11, 384, 1)
	        getitem_71: "f32[s1, 384][1152, 1]cuda:0" = split_13[0];  split_13 = None
	        alias_126: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_71);  getitem_71 = None
	        alias_127: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_26);  view_26 = None
	        copy_3: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_126, alias_127);  alias_126 = alias_127 = None
	        slice_scatter_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_11, copy_3, 1, 0, 384);  full_11 = copy_3 = None
	        alias_130: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_25);  view_25 = None
	        split_16 = torch.ops.aten.split.Tensor(slice_scatter_3, 384, 1)
	        getitem_81: "f32[s1, 384][1152, 1]cuda:0" = split_16[1];  split_16 = None
	        alias_131: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_81);  getitem_81 = None
	        copy_4: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_131, alias_130);  alias_131 = alias_130 = None
	        slice_scatter_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, copy_4, 1, 384, 768);  slice_scatter_3 = copy_4 = None
	        alias_134: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_24);  view_24 = None
	        split_19 = torch.ops.aten.split.Tensor(slice_scatter_4, 384, 1)
	        getitem_91: "f32[s1, 384][1152, 1]cuda:0" = split_19[2];  split_19 = None
	        alias_135: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_91);  getitem_91 = None
	        copy_5: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_135, alias_134);  alias_135 = alias_134 = None
	        slice_scatter_5: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_4, copy_5, 1, 768, 1152);  slice_scatter_4 = copy_5 = None
	        mm_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_5, primals_19);  primals_19 = None
	        permute_75: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_5, [1, 0]);  slice_scatter_5 = None
	        mm_31: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_75, mul_25);  permute_75 = mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_6, torch.float32);  gt_6 = None
	        mul_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_13, 1.4285714285714286);  convert_element_type_13 = None
	        mul_85: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_30, mul_84);  mm_30 = mul_84 = None
	        clone_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_85, memory_format = torch.contiguous_format);  mul_85 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_86: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_5, mul_22);  mul_22 = None
	        mul_87: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_5, primals_18);  clone_5 = primals_18 = None
	        sum_7: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_86, [0], True);  mul_86 = None
	        unsqueeze_17: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_7, 0);  sum_7 = None
	        view_27: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_17, [384]);  unsqueeze_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_88: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, add_7)
	        mul_89: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, rsqrt_4);  mul_87 = rsqrt_4 = None
	        sum_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_88, [1], True);  mul_88 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, mul_89);  add_23 = mul_89 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_137: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_44);  alias_44 = None
	        alias_138: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_137);  alias_137 = None
	        alias_139: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_138);  alias_138 = None
	        pow_15: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_139, 3);  alias_139 = None
	        mul_90: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_8, -0.5);  sum_8 = None
	        mul_91: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_90, pow_15);  mul_90 = pow_15 = None
	        expand_3: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_91, [-1, 384]);  mul_91 = None
	        div_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_3, 384);  expand_3 = None
	        pow_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 1.0);  add_7 = None
	        mul_92: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_16, 2.0);  pow_16 = None
	        mul_93: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_3, mul_92);  div_3 = mul_92 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_24, mul_93);  add_24 = mul_93 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_5, torch.float32);  gt_5 = None
	        mul_94: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_14, 1.4285714285714286);  convert_element_type_14 = None
	        mul_95: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, mul_94);  mul_94 = None
	        clone_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_95, memory_format = torch.contiguous_format);  mul_95 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_32: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_6, primals_17);  primals_17 = None
	        permute_76: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_6, [1, 0]);  clone_6 = None
	        mm_33: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_76, mul_19);  permute_76 = mul_19 = None
	        convert_element_type_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_4, torch.float32);  gt_4 = None
	        mul_96: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_15, 1.4285714285714286);  convert_element_type_15 = None
	        mul_97: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_32, mul_96);  mm_32 = mul_96 = None
	        clone_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_97, memory_format = torch.contiguous_format);  mul_97 = None
	        sigmoid_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        full_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_12, sigmoid_6);  full_12 = None
	        mul_98: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sub_2);  mm_6 = sub_2 = None
	        add_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_98, 1);  mul_98 = None
	        mul_99: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_6, add_26);  sigmoid_6 = add_26 = None
	        mul_100: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_7, mul_99);  clone_7 = mul_99 = None
	        mm_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_100, primals_16);  primals_16 = None
	        permute_78: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_100, [1, 0]);  mul_100 = None
	        mm_35: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_78, mul_16);  permute_78 = mul_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_101: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, mul_15);  mul_15 = None
	        mul_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, primals_15);  mm_34 = primals_15 = None
	        sum_9: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_101, [0], True);  mul_101 = None
	        unsqueeze_18: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_9, 0);  sum_9 = None
	        view_28: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_18, [384]);  unsqueeze_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_103: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, add_5)
	        mul_104: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, rsqrt_3);  mul_102 = rsqrt_3 = None
	        sum_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_103, [1], True);  mul_103 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_25, mul_104);  add_25 = mul_104 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_140: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_41);  alias_41 = None
	        alias_141: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_140);  alias_140 = None
	        alias_142: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_141);  alias_141 = None
	        pow_17: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_142, 3);  alias_142 = None
	        mul_105: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_10, -0.5);  sum_10 = None
	        mul_106: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_105, pow_17);  mul_105 = pow_17 = None
	        expand_4: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_106, [-1, 384]);  mul_106 = None
	        div_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_4, 384);  expand_4 = None
	        pow_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 1.0);  add_5 = None
	        mul_107: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_18, 2.0);  pow_18 = None
	        mul_108: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_4, mul_107);  div_4 = mul_107 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, mul_108);  add_27 = mul_108 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_28, primals_14);  primals_14 = None
	        permute_79: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_28, [1, 0])
	        mm_37: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_79, view_7);  permute_79 = view_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_29: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_36, [sym_size_int, 6, 64]);  mm_36 = None
	        alias_143: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_29);  view_29 = None
	        permute_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_143, [1, 0, 2]);  alias_143 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_144: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_80);  permute_80 = None
	        permute_81: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_144, [1, 0, 2]);  alias_144 = None
	        alias_145: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_81);  permute_81 = None
	        unsqueeze_19: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_145, 0);  alias_145 = None
	        alias_146: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_36);  alias_36 = None
	        alias_147: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_146);  alias_146 = None
	        _efficient_attention_backward_2 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_19, unsqueeze_3, unsqueeze_4, unsqueeze_5, None, alias_147, convert_element_type_2, convert_element_type_3, sym_size_int_1, sym_size_int_1, getitem_13, 0.0, getitem_14, getitem_15, 0, False);  unsqueeze_19 = unsqueeze_3 = unsqueeze_4 = unsqueeze_5 = alias_147 = convert_element_type_2 = convert_element_type_3 = getitem_13 = getitem_14 = getitem_15 = None
	        getitem_98: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[0]
	        getitem_99: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[1]
	        getitem_100: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[2];  _efficient_attention_backward_2 = None
	        squeeze_10: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_100, 0);  getitem_100 = None
	        squeeze_11: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_99, 0);  getitem_99 = None
	        squeeze_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0);  getitem_98 = None
	        alias_148: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_10);  squeeze_10 = None
	        permute_82: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_148, [1, 0, 2]);  alias_148 = None
	        alias_149: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_11);  squeeze_11 = None
	        permute_83: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_149, [1, 0, 2]);  alias_149 = None
	        alias_150: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_12);  squeeze_12 = None
	        permute_84: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_150, [1, 0, 2]);  alias_150 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_151: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_82);  permute_82 = None
	        permute_85: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_151, [1, 0, 2]);  alias_151 = None
	        view_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_85, [sym_size_int, 384]);  permute_85 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_152: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_83);  permute_83 = None
	        permute_86: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_152, [1, 0, 2]);  alias_152 = None
	        view_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_86, [sym_size_int, 384]);  permute_86 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_153: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_84);  permute_84 = None
	        permute_87: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_153, [1, 0, 2]);  alias_153 = None
	        view_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_87, [sym_size_int, 384]);  permute_87 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_13: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_22 = torch.ops.aten.split.Tensor(full_13, 384, 1)
	        getitem_102: "f32[s1, 384][1152, 1]cuda:0" = split_22[0];  split_22 = None
	        alias_154: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_102);  getitem_102 = None
	        alias_155: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_32);  view_32 = None
	        copy_6: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_154, alias_155);  alias_154 = alias_155 = None
	        slice_scatter_6: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_13, copy_6, 1, 0, 384);  full_13 = copy_6 = None
	        alias_158: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_31);  view_31 = None
	        split_25 = torch.ops.aten.split.Tensor(slice_scatter_6, 384, 1)
	        getitem_112: "f32[s1, 384][1152, 1]cuda:0" = split_25[1];  split_25 = None
	        alias_159: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_112);  getitem_112 = None
	        copy_7: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_159, alias_158);  alias_159 = alias_158 = None
	        slice_scatter_7: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_6, copy_7, 1, 384, 768);  slice_scatter_6 = copy_7 = None
	        alias_162: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_30);  view_30 = None
	        split_28 = torch.ops.aten.split.Tensor(slice_scatter_7, 384, 1)
	        getitem_122: "f32[s1, 384][1152, 1]cuda:0" = split_28[2];  split_28 = None
	        alias_163: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_122);  getitem_122 = None
	        copy_8: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_163, alias_162);  alias_163 = alias_162 = None
	        slice_scatter_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_7, copy_8, 1, 768, 1152);  slice_scatter_7 = copy_8 = None
	        mm_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_8, primals_13);  primals_13 = None
	        permute_89: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_8, [1, 0]);  slice_scatter_8 = None
	        mm_39: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_89, mul_14);  permute_89 = mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_3, torch.float32);  gt_3 = None
	        mul_109: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_16, 1.4285714285714286);  convert_element_type_16 = None
	        mul_110: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_38, mul_109);  mm_38 = mul_109 = None
	        clone_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_110, memory_format = torch.contiguous_format);  mul_110 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_111: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_8, mul_11);  mul_11 = None
	        mul_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_8, primals_12);  clone_8 = primals_12 = None
	        sum_11: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_111, [0], True);  mul_111 = None
	        unsqueeze_20: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_11, 0);  sum_11 = None
	        view_33: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_20, [384]);  unsqueeze_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_113: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, add_3)
	        mul_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, rsqrt_2);  mul_112 = rsqrt_2 = None
	        sum_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_113, [1], True);  mul_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_28, mul_114);  add_28 = mul_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_165: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_23);  alias_23 = None
	        alias_166: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_165);  alias_165 = None
	        alias_167: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_166);  alias_166 = None
	        pow_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_167, 3);  alias_167 = None
	        mul_115: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_12, -0.5);  sum_12 = None
	        mul_116: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, pow_19);  mul_115 = pow_19 = None
	        expand_5: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_116, [-1, 384]);  mul_116 = None
	        div_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_5, 384);  expand_5 = None
	        pow_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 1.0);  add_3 = None
	        mul_117: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_20, 2.0);  pow_20 = None
	        mul_118: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_5, mul_117);  div_5 = mul_117 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_29, mul_118);  add_29 = mul_118 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
	        mul_119: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_17, 1.4285714285714286);  convert_element_type_17 = None
	        mul_120: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_30, mul_119);  mul_119 = None
	        clone_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_120, memory_format = torch.contiguous_format);  mul_120 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_9, primals_11);  primals_11 = None
	        permute_90: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_9, [1, 0]);  clone_9 = None
	        mm_41: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_90, mul_8);  permute_90 = mul_8 = None
	        convert_element_type_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
	        mul_121: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_18, 1.4285714285714286);  convert_element_type_18 = None
	        mul_122: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_40, mul_121);  mm_40 = mul_121 = None
	        clone_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_122, memory_format = torch.contiguous_format);  mul_122 = None
	        sigmoid_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        full_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_14, sigmoid_7);  full_14 = None
	        mul_123: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub_3);  mm_2 = sub_3 = None
	        add_31: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_123, 1);  mul_123 = None
	        mul_124: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_7, add_31);  sigmoid_7 = add_31 = None
	        mul_125: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_10, mul_124);  clone_10 = mul_124 = None
	        mm_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_125, primals_10);  primals_10 = None
	        permute_92: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_125, [1, 0]);  mul_125 = None
	        mm_43: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_92, mul_5);  permute_92 = mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_126: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, mul_4);  mul_4 = None
	        mul_127: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, primals_9);  mm_42 = primals_9 = None
	        sum_13: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_126, [0], True);  mul_126 = None
	        unsqueeze_21: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_13, 0);  sum_13 = None
	        view_34: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_21, [384]);  unsqueeze_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_128: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, add_1)
	        mul_129: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, rsqrt_1);  mul_127 = rsqrt_1 = None
	        sum_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_128, [1], True);  mul_128 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_30, mul_129);  add_30 = mul_129 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_168: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
	        alias_169: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_168);  alias_168 = None
	        alias_170: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_169);  alias_169 = None
	        pow_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_170, 3);  alias_170 = None
	        mul_130: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_14, -0.5);  sum_14 = None
	        mul_131: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_130, pow_21);  mul_130 = pow_21 = None
	        expand_6: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_131, [-1, 384]);  mul_131 = None
	        div_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_6, 384);  expand_6 = None
	        pow_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
	        mul_132: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_22, 2.0);  pow_22 = None
	        mul_133: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_6, mul_132);  div_6 = mul_132 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_32, mul_133);  add_32 = mul_133 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_33, primals_8);  primals_8 = None
	        permute_93: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_33, [1, 0])
	        mm_45: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_93, view_3);  permute_93 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_35: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_44, [sym_size_int, 6, 64]);  mm_44 = None
	        alias_171: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_35);  view_35 = None
	        permute_94: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_171, [1, 0, 2]);  alias_171 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_172: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_94);  permute_94 = None
	        permute_95: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_172, [1, 0, 2]);  alias_172 = None
	        alias_173: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_95);  permute_95 = None
	        unsqueeze_22: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_173, 0);  alias_173 = None
	        alias_174: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
	        alias_175: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_174);  alias_174 = None
	        _efficient_attention_backward_3 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_22, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_175, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_22 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_175 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_129: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[0]
	        getitem_130: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[1]
	        getitem_131: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[2];  _efficient_attention_backward_3 = None
	        squeeze_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_131, 0);  getitem_131 = None
	        squeeze_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_130, 0);  getitem_130 = None
	        squeeze_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_129, 0);  getitem_129 = None
	        alias_176: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_13);  squeeze_13 = None
	        permute_96: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_176, [1, 0, 2]);  alias_176 = None
	        alias_177: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_14);  squeeze_14 = None
	        permute_97: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_177, [1, 0, 2]);  alias_177 = None
	        alias_178: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_15);  squeeze_15 = None
	        permute_98: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_178, [1, 0, 2]);  alias_178 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_179: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_96);  permute_96 = None
	        permute_99: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_179, [1, 0, 2]);  alias_179 = None
	        view_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_99, [sym_size_int, 384]);  permute_99 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_180: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_97);  permute_97 = None
	        permute_100: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_180, [1, 0, 2]);  alias_180 = None
	        view_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_100, [sym_size_int, 384]);  permute_100 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_181: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_98);  permute_98 = None
	        permute_101: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_181, [1, 0, 2]);  alias_181 = None
	        view_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_101, [sym_size_int, 384]);  permute_101 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_15: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        split_31 = torch.ops.aten.split.Tensor(full_15, 384, 1)
	        getitem_133: "f32[s1, 384][1152, 1]cuda:0" = split_31[0];  split_31 = None
	        alias_182: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_133);  getitem_133 = None
	        alias_183: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_38);  view_38 = None
	        copy_9: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_182, alias_183);  alias_182 = alias_183 = None
	        slice_scatter_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_15, copy_9, 1, 0, 384);  full_15 = copy_9 = None
	        alias_186: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_37);  view_37 = None
	        split_34 = torch.ops.aten.split.Tensor(slice_scatter_9, 384, 1)
	        getitem_143: "f32[s1, 384][1152, 1]cuda:0" = split_34[1];  split_34 = None
	        alias_187: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_143);  getitem_143 = None
	        copy_10: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_187, alias_186);  alias_187 = alias_186 = None
	        slice_scatter_10: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_9, copy_10, 1, 384, 768);  slice_scatter_9 = copy_10 = None
	        alias_190: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_36);  view_36 = None
	        split_37 = torch.ops.aten.split.Tensor(slice_scatter_10, 384, 1)
	        getitem_153: "f32[s1, 384][1152, 1]cuda:0" = split_37[2];  split_37 = None
	        alias_191: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_153);  getitem_153 = None
	        copy_11: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_191, alias_190);  alias_191 = alias_190 = None
	        slice_scatter_11: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, copy_11, 1, 768, 1152);  slice_scatter_10 = copy_11 = None
	        mm_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_11, primals_7);  primals_7 = None
	        permute_103: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_11, [1, 0]);  slice_scatter_11 = None
	        mm_47: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_103, mul_3);  permute_103 = mul_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_134: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_19, 1.4285714285714286);  convert_element_type_19 = None
	        mul_135: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, mul_134);  mm_46 = mul_134 = None
	        clone_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_135, memory_format = torch.contiguous_format);  mul_135 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_136: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_11, mul);  mul = None
	        mul_137: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_11, primals_6);  clone_11 = primals_6 = None
	        sum_15: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_136, [0], True);  mul_136 = None
	        unsqueeze_23: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_15, 0);  sum_15 = None
	        view_39: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_23, [384]);  unsqueeze_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_138: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, primals_1)
	        mul_139: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, rsqrt);  mul_137 = rsqrt = None
	        sum_16: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_138, [1], True);  mul_138 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_33, mul_139);  add_33 = mul_139 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_193: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
	        alias_194: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_193);  alias_193 = None
	        alias_195: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_194);  alias_194 = None
	        pow_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_195, 3);  alias_195 = None
	        mul_140: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_16, -0.5);  sum_16 = None
	        mul_141: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_140, pow_23);  mul_140 = pow_23 = None
	        expand_7: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_141, [-1, 384]);  mul_141 = None
	        div_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_7, 384);  expand_7 = None
	        pow_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
	        mul_142: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_24, 2.0);  pow_24 = None
	        mul_143: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_7, mul_142);  div_7 = mul_142 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_34, mul_143);  add_34 = mul_143 = None
	        return pytree.tree_unflatten([add_15, primals_2, primals_3, primals_4, add_35, tangents_2, tangents_3, tangents_4, None, view_39, mm_47, mm_45, view_34, mm_43, mm_41, view_33, mm_39, mm_37, view_28, mm_35, mm_33, view_27, mm_31, mm_29, view_22, mm_27, mm_25, view_21, mm_23, mm_21, view_16, mm_19, mm_17], self._out_spec)
	        
V0303 09:09:59.823755 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "b0801b796e895f2e81e6e2906d287394"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[12][1]cuda:0" = torch.ops.prims.inductor_seeds.default(12, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_11, 0.3);  inductor_random_default_11 = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_1: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_10, 0.3);  inductor_random_default_10 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_2: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2)
	        inductor_random_default_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_9, 0.3);  inductor_random_default_9 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12);  mul_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_3: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 3)
	        inductor_random_default_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_3, 'rand');  inductor_lookup_seed_default_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_8, 0.3);  inductor_random_default_8 = None
	        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
	        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
	        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0, 2]);  view_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_16, 0);  permute_16 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_18, 0);  permute_18 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
	        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
	        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
	        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
	        mm_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_7, permute_21);  view_7 = permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
	        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15);  mul_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_4: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 4)
	        inductor_random_default_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_4, 'rand');  inductor_lookup_seed_default_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_7, 0.3);  inductor_random_default_7 = None
	        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_5: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 5)
	        inductor_random_default_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_5, 'rand');  inductor_lookup_seed_default_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_6, 0.3);  inductor_random_default_6 = None
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18);  mul_22 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_6: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 6)
	        inductor_random_default_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_6, 'rand');  inductor_lookup_seed_default_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_5, 0.3);  inductor_random_default_5 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
	        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
	        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_8, [1, 0, 2]);  view_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_9, [1, 0, 2]);  view_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_10, [1, 0, 2]);  view_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_25, [1, 0, 2]);  permute_25 = None
	        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_26, [1, 0, 2]);  permute_26 = None
	        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_27, [1, 0, 2]);  permute_27 = None
	        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_28, 0);  permute_28 = None
	        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_29, 0);  permute_29 = None
	        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_30, 0);  permute_30 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
	        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
	        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
	        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
	        mm_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_11, permute_33);  view_11 = permute_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
	        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21);  mul_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_7: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 7)
	        inductor_random_default_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_7, 'rand');  inductor_lookup_seed_default_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_4, 0.3);  inductor_random_default_4 = None
	        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_8: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 8)
	        inductor_random_default_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_8, 'rand');  inductor_lookup_seed_default_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_3, 0.3);  inductor_random_default_3 = None
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24);  mul_33 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_9: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 9)
	        inductor_random_default_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_9, 'rand');  inductor_lookup_seed_default_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
	        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
	        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_12, [1, 0, 2]);  view_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_13, [1, 0, 2]);  view_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_14, [1, 0, 2]);  view_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_37, [1, 0, 2]);  permute_37 = None
	        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_38, [1, 0, 2]);  permute_38 = None
	        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_39, [1, 0, 2]);  permute_39 = None
	        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_40, 0);  permute_40 = None
	        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_41, 0);  permute_41 = None
	        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_42, 0);  permute_42 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
	        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
	        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
	        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
	        mm_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_15, permute_45);  view_15 = permute_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
	        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27);  mul_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_10: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 10)
	        inductor_random_default_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_10, 'rand');  inductor_lookup_seed_default_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_11: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 11);  inductor_seeds_default = None
	        inductor_random_default: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_11, 'rand');  inductor_lookup_seed_default_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
	        return (add_15, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, add_3, rsqrt_2, gt_3, mul_14, convert_element_type_2, unsqueeze_3, unsqueeze_4, unsqueeze_5, getitem_12, getitem_13, getitem_14, getitem_15, add_5, rsqrt_3, mul_16, mm_6, gt_4, mul_19, gt_5, add_7, rsqrt_4, gt_6, mul_25, convert_element_type_4, unsqueeze_6, unsqueeze_7, unsqueeze_8, getitem_21, getitem_22, getitem_23, getitem_24, add_9, rsqrt_5, mul_27, mm_10, gt_7, mul_30, gt_8, add_11, rsqrt_6, gt_9, mul_36, convert_element_type_6, unsqueeze_9, unsqueeze_10, unsqueeze_11, getitem_30, getitem_31, getitem_32, getitem_33, add_13, rsqrt_7, mul_38, mm_14, gt_10, mul_41, gt_11, sym_size_int, sym_size_int_1)
	        
V0303 09:09:59.846803 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "e1a0614d157ec260bb8b1047e74bc061"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0", rsqrt: "f32[s1, 1][1, 1]cuda:0", gt: "b8[s1, 384][384, 1]cuda:0", mul_3: "f32[s1, 384][384, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_5: "i64[][]cuda:0", getitem_6: "i64[][]cuda:0", mm_1: "f32[s1, 384][384, 1]cuda:0", rsqrt_1: "f32[s1, 1][1, 1]cuda:0", mul_5: "f32[s1, 384][384, 1]cuda:0", mm_2: "f32[s1, 1024][1024, 1]cuda:0", gt_1: "b8[s1, 1024][1024, 1]cuda:0", mul_8: "f32[s1, 1024][1024, 1]cuda:0", gt_2: "b8[s1, 384][384, 1]cuda:0", add_3: "f32[s1, 384][384, 1]cuda:0", rsqrt_2: "f32[s1, 1][1, 1]cuda:0", gt_3: "b8[s1, 384][384, 1]cuda:0", mul_14: "f32[s1, 384][384, 1]cuda:0", convert_element_type_2: "i32[257][1]cuda:0", unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_14: "i64[][]cuda:0", getitem_15: "i64[][]cuda:0", add_5: "f32[s1, 384][384, 1]cuda:0", rsqrt_3: "f32[s1, 1][1, 1]cuda:0", mul_16: "f32[s1, 384][384, 1]cuda:0", mm_6: "f32[s1, 1024][1024, 1]cuda:0", gt_4: "b8[s1, 1024][1024, 1]cuda:0", mul_19: "f32[s1, 1024][1024, 1]cuda:0", gt_5: "b8[s1, 384][384, 1]cuda:0", add_7: "f32[s1, 384][384, 1]cuda:0", rsqrt_4: "f32[s1, 1][1, 1]cuda:0", gt_6: "b8[s1, 384][384, 1]cuda:0", mul_25: "f32[s1, 384][384, 1]cuda:0", convert_element_type_4: "i32[257][1]cuda:0", unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_23: "i64[][]cuda:0", getitem_24: "i64[][]cuda:0", add_9: "f32[s1, 384][384, 1]cuda:0", rsqrt_5: "f32[s1, 1][1, 1]cuda:0", mul_27: "f32[s1, 384][384, 1]cuda:0", mm_10: "f32[s1, 1024][1024, 1]cuda:0", gt_7: "b8[s1, 1024][1024, 1]cuda:0", mul_30: "f32[s1, 1024][1024, 1]cuda:0", gt_8: "b8[s1, 384][384, 1]cuda:0", add_11: "f32[s1, 384][384, 1]cuda:0", rsqrt_6: "f32[s1, 1][1, 1]cuda:0", gt_9: "b8[s1, 384][384, 1]cuda:0", mul_36: "f32[s1, 384][384, 1]cuda:0", convert_element_type_6: "i32[257][1]cuda:0", unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_32: "i64[][]cuda:0", getitem_33: "i64[][]cuda:0", add_13: "f32[s1, 384][384, 1]cuda:0", rsqrt_7: "f32[s1, 1][1, 1]cuda:0", mul_38: "f32[s1, 384][384, 1]cuda:0", mm_14: "f32[s1, 1024][1024, 1]cuda:0", gt_10: "b8[s1, 1024][1024, 1]cuda:0", mul_41: "f32[s1, 1024][1024, 1]cuda:0", gt_11: "b8[s1, 384][384, 1]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cuda:0", tangents_4: "f32[s4, 0][1, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_11, torch.float32);  gt_11 = None
	        mul_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_8, 1.4285714285714286);  convert_element_type_8 = None
	        mul_45: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_44);  mul_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_45, primals_29);  primals_29 = None
	        permute_48: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_45, [1, 0]);  mul_45 = None
	        mm_17: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_48, mul_41);  permute_48 = mul_41 = None
	        convert_element_type_9: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_10, torch.float32);  gt_10 = None
	        mul_46: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_9, 1.4285714285714286);  convert_element_type_9 = None
	        mul_47: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_16, mul_46);  mm_16 = mul_46 = None
	        sigmoid_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        full_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_4)
	        mul_48: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sub);  mm_14 = sub = None
	        add_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_48, 1);  mul_48 = None
	        mul_49: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_4, add_16);  sigmoid_4 = add_16 = None
	        mul_50: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_47, mul_49);  mul_47 = mul_49 = None
	        mm_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_50, primals_28);  primals_28 = None
	        permute_50: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_50, [1, 0]);  mul_50 = None
	        mm_19: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_50, mul_38);  permute_50 = mul_38 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, mul_37);  mul_37 = None
	        mul_52: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, primals_27);  mm_18 = primals_27 = None
	        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_51, [0], True);  mul_51 = None
	        unsqueeze_12: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view_16: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_12, [384]);  unsqueeze_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_53: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, add_13)
	        mul_54: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, rsqrt_7);  mul_52 = None
	        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_53, [1], True);  mul_53 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_54);  tangents_1 = mul_54 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_9: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_7, 3);  rsqrt_7 = None
	        mul_55: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
	        mul_56: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, pow_9);  mul_55 = pow_9 = None
	        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_56, [-1, 384]);  mul_56 = None
	        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
	        pow_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 1.0);  add_13 = None
	        mul_57: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_10, 2.0);  pow_10 = None
	        mul_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_57);  div = mul_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_17, mul_58);  add_17 = mul_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_18, primals_26);  primals_26 = None
	        permute_51: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_18, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_21: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_51, view_15);  permute_51 = view_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [sym_size_int, 6, 64]);  mm_20 = None
	        permute_52: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_17, [1, 0, 2]);  view_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_53: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_52, [1, 0, 2]);  permute_52 = None
	        unsqueeze_13: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_53, 0);  permute_53 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_13, unsqueeze_9, unsqueeze_10, unsqueeze_11, None, getitem_30, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, getitem_31, 0.0, getitem_32, getitem_33, 0, False);  unsqueeze_13 = unsqueeze_9 = unsqueeze_10 = unsqueeze_11 = getitem_30 = convert_element_type_6 = getitem_31 = getitem_32 = getitem_33 = None
	        getitem_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_37: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_38: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_38, 0);  getitem_38 = None
	        squeeze_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_37, 0);  getitem_37 = None
	        squeeze_6: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_36, 0);  getitem_36 = None
	        permute_54: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_4, [1, 0, 2]);  squeeze_4 = None
	        permute_55: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_5, [1, 0, 2]);  squeeze_5 = None
	        permute_56: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_6, [1, 0, 2]);  squeeze_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_57: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_54, [1, 0, 2]);  permute_54 = None
	        view_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_57, [sym_size_int, 384]);  permute_57 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_55, [1, 0, 2]);  permute_55 = None
	        view_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_58, [sym_size_int, 384]);  permute_58 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_59: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_56, [1, 0, 2]);  permute_56 = None
	        view_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_59, [sym_size_int, 384]);  permute_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        split_4 = torch.ops.aten.split.Tensor(full_9, 384, 1)
	        getitem_40: "f32[s1, 384][1152, 1]cuda:0" = split_4[0];  split_4 = None
	        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_20);  view_20 = None
	        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy, 1, 0, 384);  copy = None
	        split_7 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_50: "f32[s1, 384][1152, 1]cuda:0" = split_7[1];  split_7 = None
	        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_50, view_19);  getitem_50 = view_19 = None
	        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        split_10 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
	        getitem_60: "f32[s1, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
	        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_60, view_18);  getitem_60 = view_18 = None
	        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
	        mm_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_25);  primals_25 = None
	        permute_61: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
	        mm_23: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_61, mul_36);  permute_61 = mul_36 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_9, torch.float32);  gt_9 = None
	        mul_59: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_10, 1.4285714285714286);  convert_element_type_10 = None
	        mul_60: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_22, mul_59);  mm_22 = mul_59 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_61: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_60, mul_33);  mul_33 = None
	        mul_62: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_60, primals_24);  mul_60 = primals_24 = None
	        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_61, [0], True);  mul_61 = None
	        unsqueeze_14: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
	        view_21: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_14, [384]);  unsqueeze_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_63: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, add_11)
	        mul_64: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, rsqrt_6);  mul_62 = None
	        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_63, [1], True);  mul_63 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_18, mul_64);  add_18 = mul_64 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_11: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_6, 3);  rsqrt_6 = None
	        mul_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
	        mul_66: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, pow_11);  mul_65 = pow_11 = None
	        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_66, [-1, 384]);  mul_66 = None
	        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
	        pow_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 1.0);  add_11 = None
	        mul_67: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_12, 2.0);  pow_12 = None
	        mul_68: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_67);  div_1 = mul_67 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, mul_68);  add_19 = mul_68 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_8, torch.float32);  gt_8 = None
	        mul_69: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, 1.4285714285714286);  convert_element_type_11 = None
	        mul_70: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_20, mul_69);  mul_69 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_24: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_70, primals_23);  primals_23 = None
	        permute_62: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_70, [1, 0]);  mul_70 = None
	        mm_25: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_62, mul_30);  permute_62 = mul_30 = None
	        convert_element_type_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_7, torch.float32);  gt_7 = None
	        mul_71: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, 1.4285714285714286);  convert_element_type_12 = None
	        mul_72: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_24, mul_71);  mm_24 = mul_71 = None
	        sigmoid_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        sub_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_5)
	        mul_73: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sub_1);  mm_10 = sub_1 = None
	        add_21: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_73, 1);  mul_73 = None
	        mul_74: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_5, add_21);  sigmoid_5 = add_21 = None
	        mul_75: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_72, mul_74);  mul_72 = mul_74 = None
	        mm_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_75, primals_22);  primals_22 = None
	        permute_64: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_75, [1, 0]);  mul_75 = None
	        mm_27: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_64, mul_27);  permute_64 = mul_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_76: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, mul_26);  mul_26 = None
	        mul_77: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, primals_21);  mm_26 = primals_21 = None
	        sum_5: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_76, [0], True);  mul_76 = None
	        unsqueeze_15: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_5, 0);  sum_5 = None
	        view_22: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_15, [384]);  unsqueeze_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, add_9)
	        mul_79: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, rsqrt_5);  mul_77 = None
	        sum_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_78, [1], True);  mul_78 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_20, mul_79);  add_20 = mul_79 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_13: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_5, 3);  rsqrt_5 = None
	        mul_80: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_6, -0.5);  sum_6 = None
	        mul_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, pow_13);  mul_80 = pow_13 = None
	        expand_2: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_81, [-1, 384]);  mul_81 = None
	        div_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_2, 384);  expand_2 = None
	        pow_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 1.0);  add_9 = None
	        mul_82: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_14, 2.0);  pow_14 = None
	        mul_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_2, mul_82);  div_2 = mul_82 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_22, mul_83);  add_22 = mul_83 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_23, primals_20);  primals_20 = None
	        permute_65: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_23, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_29: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_65, view_11);  permute_65 = view_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_28, [sym_size_int, 6, 64]);  mm_28 = None
	        permute_66: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_23, [1, 0, 2]);  view_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_67: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_66, [1, 0, 2]);  permute_66 = None
	        unsqueeze_16: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_67, 0);  permute_67 = None
	        _efficient_attention_backward_1 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_16, unsqueeze_6, unsqueeze_7, unsqueeze_8, None, getitem_21, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, getitem_22, 0.0, getitem_23, getitem_24, 0, False);  unsqueeze_16 = unsqueeze_6 = unsqueeze_7 = unsqueeze_8 = getitem_21 = convert_element_type_4 = getitem_22 = getitem_23 = getitem_24 = None
	        getitem_67: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[0]
	        getitem_68: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[1]
	        getitem_69: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[2];  _efficient_attention_backward_1 = None
	        squeeze_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_69, 0);  getitem_69 = None
	        squeeze_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_68, 0);  getitem_68 = None
	        squeeze_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_67, 0);  getitem_67 = None
	        permute_68: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_7, [1, 0, 2]);  squeeze_7 = None
	        permute_69: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_8, [1, 0, 2]);  squeeze_8 = None
	        permute_70: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_9, [1, 0, 2]);  squeeze_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_71: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_68, [1, 0, 2]);  permute_68 = None
	        view_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_71, [sym_size_int, 384]);  permute_71 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_72: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_69, [1, 0, 2]);  permute_69 = None
	        view_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_72, [sym_size_int, 384]);  permute_72 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_73: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_70, [1, 0, 2]);  permute_70 = None
	        view_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_73, [sym_size_int, 384]);  permute_73 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_3: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_26);  view_26 = None
	        slice_scatter_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy_3, 1, 0, 384);  copy_3 = None
	        split_16 = torch.ops.aten.split.Tensor(slice_scatter_3, 384, 1)
	        getitem_81: "f32[s1, 384][1152, 1]cuda:0" = split_16[1];  split_16 = None
	        copy_4: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_81, view_25);  getitem_81 = view_25 = None
	        slice_scatter_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, copy_4, 1, 384, 768);  slice_scatter_3 = copy_4 = None
	        split_19 = torch.ops.aten.split.Tensor(slice_scatter_4, 384, 1)
	        getitem_91: "f32[s1, 384][1152, 1]cuda:0" = split_19[2];  split_19 = None
	        copy_5: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_91, view_24);  getitem_91 = view_24 = None
	        slice_scatter_5: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_4, copy_5, 1, 768, 1152);  slice_scatter_4 = copy_5 = None
	        mm_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_5, primals_19);  primals_19 = None
	        permute_75: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_5, [1, 0]);  slice_scatter_5 = None
	        mm_31: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_75, mul_25);  permute_75 = mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_6, torch.float32);  gt_6 = None
	        mul_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_13, 1.4285714285714286);  convert_element_type_13 = None
	        mul_85: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_30, mul_84);  mm_30 = mul_84 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_86: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, mul_22);  mul_22 = None
	        mul_87: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, primals_18);  mul_85 = primals_18 = None
	        sum_7: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_86, [0], True);  mul_86 = None
	        unsqueeze_17: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_7, 0);  sum_7 = None
	        view_27: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_17, [384]);  unsqueeze_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_88: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, add_7)
	        mul_89: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, rsqrt_4);  mul_87 = None
	        sum_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_88, [1], True);  mul_88 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, mul_89);  add_23 = mul_89 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_15: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_4, 3);  rsqrt_4 = None
	        mul_90: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_8, -0.5);  sum_8 = None
	        mul_91: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_90, pow_15);  mul_90 = pow_15 = None
	        expand_3: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_91, [-1, 384]);  mul_91 = None
	        div_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_3, 384);  expand_3 = None
	        pow_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 1.0);  add_7 = None
	        mul_92: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_16, 2.0);  pow_16 = None
	        mul_93: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_3, mul_92);  div_3 = mul_92 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_24, mul_93);  add_24 = mul_93 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_5, torch.float32);  gt_5 = None
	        mul_94: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_14, 1.4285714285714286);  convert_element_type_14 = None
	        mul_95: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, mul_94);  mul_94 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_32: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_95, primals_17);  primals_17 = None
	        permute_76: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_95, [1, 0]);  mul_95 = None
	        mm_33: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_76, mul_19);  permute_76 = mul_19 = None
	        convert_element_type_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_4, torch.float32);  gt_4 = None
	        mul_96: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_15, 1.4285714285714286);  convert_element_type_15 = None
	        mul_97: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_32, mul_96);  mm_32 = mul_96 = None
	        sigmoid_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        sub_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_6)
	        mul_98: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sub_2);  mm_6 = sub_2 = None
	        add_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_98, 1);  mul_98 = None
	        mul_99: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_6, add_26);  sigmoid_6 = add_26 = None
	        mul_100: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_97, mul_99);  mul_97 = mul_99 = None
	        mm_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_100, primals_16);  primals_16 = None
	        permute_78: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_100, [1, 0]);  mul_100 = None
	        mm_35: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_78, mul_16);  permute_78 = mul_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_101: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, mul_15);  mul_15 = None
	        mul_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, primals_15);  mm_34 = primals_15 = None
	        sum_9: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_101, [0], True);  mul_101 = None
	        unsqueeze_18: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_9, 0);  sum_9 = None
	        view_28: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_18, [384]);  unsqueeze_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_103: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, add_5)
	        mul_104: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, rsqrt_3);  mul_102 = None
	        sum_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_103, [1], True);  mul_103 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_25, mul_104);  add_25 = mul_104 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_17: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_3, 3);  rsqrt_3 = None
	        mul_105: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_10, -0.5);  sum_10 = None
	        mul_106: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_105, pow_17);  mul_105 = pow_17 = None
	        expand_4: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_106, [-1, 384]);  mul_106 = None
	        div_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_4, 384);  expand_4 = None
	        pow_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 1.0);  add_5 = None
	        mul_107: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_18, 2.0);  pow_18 = None
	        mul_108: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_4, mul_107);  div_4 = mul_107 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, mul_108);  add_27 = mul_108 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_28, primals_14);  primals_14 = None
	        permute_79: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_28, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_37: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_79, view_7);  permute_79 = view_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_29: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_36, [sym_size_int, 6, 64]);  mm_36 = None
	        permute_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_29, [1, 0, 2]);  view_29 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_81: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_80, [1, 0, 2]);  permute_80 = None
	        unsqueeze_19: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_81, 0);  permute_81 = None
	        _efficient_attention_backward_2 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_19, unsqueeze_3, unsqueeze_4, unsqueeze_5, None, getitem_12, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, getitem_13, 0.0, getitem_14, getitem_15, 0, False);  unsqueeze_19 = unsqueeze_3 = unsqueeze_4 = unsqueeze_5 = getitem_12 = convert_element_type_2 = getitem_13 = getitem_14 = getitem_15 = None
	        getitem_98: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[0]
	        getitem_99: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[1]
	        getitem_100: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[2];  _efficient_attention_backward_2 = None
	        squeeze_10: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_100, 0);  getitem_100 = None
	        squeeze_11: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_99, 0);  getitem_99 = None
	        squeeze_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0);  getitem_98 = None
	        permute_82: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_10, [1, 0, 2]);  squeeze_10 = None
	        permute_83: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_11, [1, 0, 2]);  squeeze_11 = None
	        permute_84: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_12, [1, 0, 2]);  squeeze_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_85: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_82, [1, 0, 2]);  permute_82 = None
	        view_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_85, [sym_size_int, 384]);  permute_85 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_86: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_83, [1, 0, 2]);  permute_83 = None
	        view_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_86, [sym_size_int, 384]);  permute_86 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_87: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_84, [1, 0, 2]);  permute_84 = None
	        view_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_87, [sym_size_int, 384]);  permute_87 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_6: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_32);  view_32 = None
	        slice_scatter_6: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy_6, 1, 0, 384);  copy_6 = None
	        split_25 = torch.ops.aten.split.Tensor(slice_scatter_6, 384, 1)
	        getitem_112: "f32[s1, 384][1152, 1]cuda:0" = split_25[1];  split_25 = None
	        copy_7: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_112, view_31);  getitem_112 = view_31 = None
	        slice_scatter_7: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_6, copy_7, 1, 384, 768);  slice_scatter_6 = copy_7 = None
	        split_28 = torch.ops.aten.split.Tensor(slice_scatter_7, 384, 1)
	        getitem_122: "f32[s1, 384][1152, 1]cuda:0" = split_28[2];  split_28 = None
	        copy_8: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_122, view_30);  getitem_122 = view_30 = None
	        slice_scatter_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_7, copy_8, 1, 768, 1152);  slice_scatter_7 = copy_8 = None
	        mm_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_8, primals_13);  primals_13 = None
	        permute_89: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_8, [1, 0]);  slice_scatter_8 = None
	        mm_39: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_89, mul_14);  permute_89 = mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_3, torch.float32);  gt_3 = None
	        mul_109: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_16, 1.4285714285714286);  convert_element_type_16 = None
	        mul_110: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_38, mul_109);  mm_38 = mul_109 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_111: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_110, mul_11);  mul_11 = None
	        mul_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_110, primals_12);  mul_110 = primals_12 = None
	        sum_11: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_111, [0], True);  mul_111 = None
	        unsqueeze_20: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_11, 0);  sum_11 = None
	        view_33: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_20, [384]);  unsqueeze_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_113: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, add_3)
	        mul_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, rsqrt_2);  mul_112 = None
	        sum_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_113, [1], True);  mul_113 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_28, mul_114);  add_28 = mul_114 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_2, 3);  rsqrt_2 = None
	        mul_115: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_12, -0.5);  sum_12 = None
	        mul_116: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, pow_19);  mul_115 = pow_19 = None
	        expand_5: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_116, [-1, 384]);  mul_116 = None
	        div_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_5, 384);  expand_5 = None
	        pow_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 1.0);  add_3 = None
	        mul_117: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_20, 2.0);  pow_20 = None
	        mul_118: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_5, mul_117);  div_5 = mul_117 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_29, mul_118);  add_29 = mul_118 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
	        mul_119: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_17, 1.4285714285714286);  convert_element_type_17 = None
	        mul_120: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_30, mul_119);  mul_119 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_120, primals_11);  primals_11 = None
	        permute_90: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_120, [1, 0]);  mul_120 = None
	        mm_41: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_90, mul_8);  permute_90 = mul_8 = None
	        convert_element_type_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
	        mul_121: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_18, 1.4285714285714286);  convert_element_type_18 = None
	        mul_122: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_40, mul_121);  mm_40 = mul_121 = None
	        sigmoid_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        sub_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_7);  full_8 = None
	        mul_123: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub_3);  mm_2 = sub_3 = None
	        add_31: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_123, 1);  mul_123 = None
	        mul_124: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_7, add_31);  sigmoid_7 = add_31 = None
	        mul_125: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_122, mul_124);  mul_122 = mul_124 = None
	        mm_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_125, primals_10);  primals_10 = None
	        permute_92: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_125, [1, 0]);  mul_125 = None
	        mm_43: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_92, mul_5);  permute_92 = mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_126: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, mul_4);  mul_4 = None
	        mul_127: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, primals_9);  mm_42 = primals_9 = None
	        sum_13: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_126, [0], True);  mul_126 = None
	        unsqueeze_21: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_13, 0);  sum_13 = None
	        view_34: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_21, [384]);  unsqueeze_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_128: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, add_1)
	        mul_129: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, rsqrt_1);  mul_127 = None
	        sum_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_128, [1], True);  mul_128 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_30, mul_129);  add_30 = mul_129 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_1, 3);  rsqrt_1 = None
	        mul_130: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_14, -0.5);  sum_14 = None
	        mul_131: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_130, pow_21);  mul_130 = pow_21 = None
	        expand_6: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_131, [-1, 384]);  mul_131 = None
	        div_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_6, 384);  expand_6 = None
	        pow_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
	        mul_132: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_22, 2.0);  pow_22 = None
	        mul_133: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_6, mul_132);  div_6 = mul_132 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_32, mul_133);  add_32 = mul_133 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_33, primals_8);  primals_8 = None
	        permute_93: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_33, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_45: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_93, view_3);  permute_93 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_35: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_44, [sym_size_int, 6, 64]);  mm_44 = None
	        permute_94: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_35, [1, 0, 2]);  view_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_95: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_94, [1, 0, 2]);  permute_94 = None
	        unsqueeze_22: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_95, 0);  permute_95 = None
	        _efficient_attention_backward_3 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_22, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem_3, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_22 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem_3 = convert_element_type = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_129: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[0]
	        getitem_130: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[1]
	        getitem_131: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[2];  _efficient_attention_backward_3 = None
	        squeeze_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_131, 0);  getitem_131 = None
	        squeeze_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_130, 0);  getitem_130 = None
	        squeeze_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_129, 0);  getitem_129 = None
	        permute_96: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_13, [1, 0, 2]);  squeeze_13 = None
	        permute_97: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_14, [1, 0, 2]);  squeeze_14 = None
	        permute_98: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_15, [1, 0, 2]);  squeeze_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_99: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_96, [1, 0, 2]);  permute_96 = None
	        view_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_99, [sym_size_int, 384]);  permute_99 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_100: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_97, [1, 0, 2]);  permute_97 = None
	        view_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_100, [sym_size_int, 384]);  permute_100 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_101: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_98, [1, 0, 2]);  permute_98 = None
	        view_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_101, [sym_size_int, 384]);  permute_101 = sym_size_int = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        copy_9: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_38);  getitem_40 = view_38 = None
	        slice_scatter_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy_9, 1, 0, 384);  full_9 = copy_9 = None
	        split_34 = torch.ops.aten.split.Tensor(slice_scatter_9, 384, 1)
	        getitem_143: "f32[s1, 384][1152, 1]cuda:0" = split_34[1];  split_34 = None
	        copy_10: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_143, view_37);  getitem_143 = view_37 = None
	        slice_scatter_10: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_9, copy_10, 1, 384, 768);  slice_scatter_9 = copy_10 = None
	        split_37 = torch.ops.aten.split.Tensor(slice_scatter_10, 384, 1)
	        getitem_153: "f32[s1, 384][1152, 1]cuda:0" = split_37[2];  split_37 = None
	        copy_11: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_153, view_36);  getitem_153 = view_36 = None
	        slice_scatter_11: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, copy_11, 1, 768, 1152);  slice_scatter_10 = copy_11 = None
	        mm_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_11, primals_7);  primals_7 = None
	        permute_103: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_11, [1, 0]);  slice_scatter_11 = None
	        mm_47: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_103, mul_3);  permute_103 = mul_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_134: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_19, 1.4285714285714286);  convert_element_type_19 = None
	        mul_135: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, mul_134);  mm_46 = mul_134 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_136: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_135, mul);  mul = None
	        mul_137: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_135, primals_6);  mul_135 = primals_6 = None
	        sum_15: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_136, [0], True);  mul_136 = None
	        unsqueeze_23: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_15, 0);  sum_15 = None
	        view_39: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_23, [384]);  unsqueeze_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_138: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, primals_1)
	        mul_139: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, rsqrt);  mul_137 = None
	        sum_16: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_138, [1], True);  mul_138 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_33, mul_139);  add_33 = mul_139 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
	        mul_140: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_16, -0.5);  sum_16 = None
	        mul_141: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_140, pow_23);  mul_140 = pow_23 = None
	        expand_7: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_141, [-1, 384]);  mul_141 = None
	        div_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_7, 384);  expand_7 = None
	        pow_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
	        mul_142: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_24, 2.0);  pow_24 = None
	        mul_143: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_7, mul_142);  div_7 = mul_142 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_34, mul_143);  add_34 = mul_143 = None
	        return (add_35, tangents_2, tangents_3, tangents_4, None, view_39, mm_47, mm_45, view_34, mm_43, mm_41, view_33, mm_39, mm_37, view_28, mm_35, mm_33, view_27, mm_31, mm_29, view_22, mm_27, mm_25, view_21, mm_23, mm_21, view_16, mm_19, mm_17)
	        
V0303 09:09:59.847502 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "10560c352dd4dc0eff39c72aecc69545"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740992999847175.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:59.847900 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "7c09f6877e1d3f61a2631b96ac31ad59"}
	{
	"name": "compile_fx_inner",
	"ts": 1740992999847836.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:59.848076 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "11099340692d89dd15e4d5f8958c57bb"}
	{
	"name": "inductor_compile",
	"ts": 1740992999847836.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:09:59.883897 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "ebe85f965b69453c77460ae183c6aea5"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29):
	        pow_1 = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt = torch.ops.aten.rsqrt.default(add);  add = None
	        mul = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        mul_1 = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
	        inductor_seeds_default = torch.ops.prims.inductor_seeds.default(12, device(type='cuda', index=0))
	        inductor_lookup_seed_default = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_11 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        gt = torch.ops.aten.gt.Scalar(inductor_random_default_11, 0.3);  inductor_random_default_11 = None
	        mul_2 = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3 = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        permute = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem = split[0]
	        getitem_1 = split[1]
	        getitem_2 = split[2];  split = None
	        view = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1 = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        view_1 = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2 = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        view_2 = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3 = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        permute_4 = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5 = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6 = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1 = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2 = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1 = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3 = _efficient_attention_forward[0]
	        getitem_4 = _efficient_attention_forward[1]
	        getitem_5 = _efficient_attention_forward[2]
	        getitem_6 = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7 = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3 = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        permute_9 = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1 = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        add_1 = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        pow_2 = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1 = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2 = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1 = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4 = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        mul_5 = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        permute_10 = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2 = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6 = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        inductor_lookup_seed_default_1 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_10 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        gt_1 = torch.ops.aten.gt.Scalar(inductor_random_default_10, 0.3);  inductor_random_default_10 = None
	        mul_7 = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8 = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11 = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3 = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        inductor_lookup_seed_default_2 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2)
	        inductor_random_default_9 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        gt_2 = torch.ops.aten.gt.Scalar(inductor_random_default_9, 0.3);  inductor_random_default_9 = None
	        mul_9 = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10 = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3 = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        pow_3 = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2 = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4 = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2 = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        mul_11 = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        mul_12 = torch.ops.aten.mul.Tensor(mul_11, primals_12);  mul_11 = None
	        inductor_lookup_seed_default_3 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 3)
	        inductor_random_default_8 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_3, 'rand');  inductor_lookup_seed_default_3 = None
	        gt_3 = torch.ops.aten.gt.Scalar(inductor_random_default_8, 0.3);  inductor_random_default_8 = None
	        mul_13 = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14 = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        permute_12 = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4 = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9 = split_1[0]
	        getitem_10 = split_1[1]
	        getitem_11 = split_1[2];  split_1 = None
	        view_4 = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        permute_13 = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        view_5 = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        permute_14 = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        view_6 = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        permute_15 = torch.ops.aten.permute.default(view_6, [1, 0, 2]);  view_6 = None
	        permute_16 = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        permute_17 = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        permute_18 = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        convert_element_type_2 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_3 = torch.ops.aten.unsqueeze.default(permute_16, 0);  permute_16 = None
	        unsqueeze_4 = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        unsqueeze_5 = torch.ops.aten.unsqueeze.default(permute_18, 0);  permute_18 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12 = _efficient_attention_forward_1[0]
	        getitem_13 = _efficient_attention_forward_1[1]
	        getitem_14 = _efficient_attention_forward_1[2]
	        getitem_15 = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        squeeze_1 = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19 = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        permute_20 = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7 = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        permute_21 = torch.ops.aten.permute.default(primals_14, [1, 0])
	        mm_5 = torch.ops.aten.mm.default(view_7, permute_21);  view_7 = permute_21 = None
	        add_5 = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
	        pow_4 = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
	        mean_3 = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6 = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3 = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        mul_15 = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
	        mul_16 = torch.ops.aten.mul.Tensor(mul_15, primals_15);  mul_15 = None
	        permute_22 = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6 = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1 = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17 = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        inductor_lookup_seed_default_4 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 4)
	        inductor_random_default_7 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_4, 'rand');  inductor_lookup_seed_default_4 = None
	        gt_4 = torch.ops.aten.gt.Scalar(inductor_random_default_7, 0.3);  inductor_random_default_7 = None
	        mul_18 = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19 = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23 = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7 = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        inductor_lookup_seed_default_5 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 5)
	        inductor_random_default_6 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_5, 'rand');  inductor_lookup_seed_default_5 = None
	        gt_5 = torch.ops.aten.gt.Scalar(inductor_random_default_6, 0.3);  inductor_random_default_6 = None
	        mul_20 = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21 = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7 = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
	        pow_5 = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4 = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8 = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4 = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        mul_22 = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        mul_23 = torch.ops.aten.mul.Tensor(mul_22, primals_18);  mul_22 = None
	        inductor_lookup_seed_default_6 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 6)
	        inductor_random_default_5 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_6, 'rand');  inductor_lookup_seed_default_6 = None
	        gt_6 = torch.ops.aten.gt.Scalar(inductor_random_default_5, 0.3);  inductor_random_default_5 = None
	        mul_24 = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25 = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        permute_24 = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8 = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18 = split_2[0]
	        getitem_19 = split_2[1]
	        getitem_20 = split_2[2];  split_2 = None
	        view_8 = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        permute_25 = torch.ops.aten.permute.default(view_8, [1, 0, 2]);  view_8 = None
	        view_9 = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        permute_26 = torch.ops.aten.permute.default(view_9, [1, 0, 2]);  view_9 = None
	        view_10 = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        permute_27 = torch.ops.aten.permute.default(view_10, [1, 0, 2]);  view_10 = None
	        permute_28 = torch.ops.aten.permute.default(permute_25, [1, 0, 2]);  permute_25 = None
	        permute_29 = torch.ops.aten.permute.default(permute_26, [1, 0, 2]);  permute_26 = None
	        permute_30 = torch.ops.aten.permute.default(permute_27, [1, 0, 2]);  permute_27 = None
	        convert_element_type_4 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_6 = torch.ops.aten.unsqueeze.default(permute_28, 0);  permute_28 = None
	        unsqueeze_7 = torch.ops.aten.unsqueeze.default(permute_29, 0);  permute_29 = None
	        unsqueeze_8 = torch.ops.aten.unsqueeze.default(permute_30, 0);  permute_30 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21 = _efficient_attention_forward_2[0]
	        getitem_22 = _efficient_attention_forward_2[1]
	        getitem_23 = _efficient_attention_forward_2[2]
	        getitem_24 = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        squeeze_2 = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31 = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        permute_32 = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11 = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        permute_33 = torch.ops.aten.permute.default(primals_20, [1, 0])
	        mm_9 = torch.ops.aten.mm.default(view_11, permute_33);  view_11 = permute_33 = None
	        add_9 = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
	        pow_6 = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
	        mean_5 = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10 = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5 = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        mul_26 = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
	        mul_27 = torch.ops.aten.mul.Tensor(mul_26, primals_21);  mul_26 = None
	        permute_34 = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10 = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2 = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28 = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        inductor_lookup_seed_default_7 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 7)
	        inductor_random_default_4 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_7, 'rand');  inductor_lookup_seed_default_7 = None
	        gt_7 = torch.ops.aten.gt.Scalar(inductor_random_default_4, 0.3);  inductor_random_default_4 = None
	        mul_29 = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30 = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35 = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11 = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        inductor_lookup_seed_default_8 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 8)
	        inductor_random_default_3 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_8, 'rand');  inductor_lookup_seed_default_8 = None
	        gt_8 = torch.ops.aten.gt.Scalar(inductor_random_default_3, 0.3);  inductor_random_default_3 = None
	        mul_31 = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32 = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11 = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
	        pow_7 = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6 = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12 = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6 = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        mul_33 = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        mul_34 = torch.ops.aten.mul.Tensor(mul_33, primals_24);  mul_33 = None
	        inductor_lookup_seed_default_9 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 9)
	        inductor_random_default_2 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_9, 'rand');  inductor_lookup_seed_default_9 = None
	        gt_9 = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_35 = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36 = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        permute_36 = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12 = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27 = split_3[0]
	        getitem_28 = split_3[1]
	        getitem_29 = split_3[2];  split_3 = None
	        view_12 = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        permute_37 = torch.ops.aten.permute.default(view_12, [1, 0, 2]);  view_12 = None
	        view_13 = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        permute_38 = torch.ops.aten.permute.default(view_13, [1, 0, 2]);  view_13 = None
	        view_14 = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        permute_39 = torch.ops.aten.permute.default(view_14, [1, 0, 2]);  view_14 = None
	        permute_40 = torch.ops.aten.permute.default(permute_37, [1, 0, 2]);  permute_37 = None
	        permute_41 = torch.ops.aten.permute.default(permute_38, [1, 0, 2]);  permute_38 = None
	        permute_42 = torch.ops.aten.permute.default(permute_39, [1, 0, 2]);  permute_39 = None
	        convert_element_type_6 = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_9 = torch.ops.aten.unsqueeze.default(permute_40, 0);  permute_40 = None
	        unsqueeze_10 = torch.ops.aten.unsqueeze.default(permute_41, 0);  permute_41 = None
	        unsqueeze_11 = torch.ops.aten.unsqueeze.default(permute_42, 0);  permute_42 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30 = _efficient_attention_forward_3[0]
	        getitem_31 = _efficient_attention_forward_3[1]
	        getitem_32 = _efficient_attention_forward_3[2]
	        getitem_33 = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        squeeze_3 = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43 = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        permute_44 = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15 = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        permute_45 = torch.ops.aten.permute.default(primals_26, [1, 0])
	        mm_13 = torch.ops.aten.mm.default(view_15, permute_45);  view_15 = permute_45 = None
	        add_13 = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
	        pow_8 = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
	        mean_7 = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14 = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7 = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        mul_37 = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
	        mul_38 = torch.ops.aten.mul.Tensor(mul_37, primals_27);  mul_37 = None
	        permute_46 = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14 = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3 = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39 = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        inductor_lookup_seed_default_10 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 10)
	        inductor_random_default_1 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_10, 'rand');  inductor_lookup_seed_default_10 = None
	        gt_10 = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_40 = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41 = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47 = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15 = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        inductor_lookup_seed_default_11 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 11);  inductor_seeds_default = None
	        inductor_random_default = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_11, 'rand');  inductor_lookup_seed_default_11 = None
	        gt_11 = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_42 = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43 = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15 = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
	        return (add_15, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, add_3, rsqrt_2, gt_3, mul_14, convert_element_type_2, unsqueeze_3, unsqueeze_4, unsqueeze_5, getitem_12, getitem_13, getitem_14, getitem_15, add_5, rsqrt_3, mul_16, mm_6, gt_4, mul_19, gt_5, add_7, rsqrt_4, gt_6, mul_25, convert_element_type_4, unsqueeze_6, unsqueeze_7, unsqueeze_8, getitem_21, getitem_22, getitem_23, getitem_24, add_9, rsqrt_5, mul_27, mm_10, gt_7, mul_30, gt_8, add_11, rsqrt_6, gt_9, mul_36, convert_element_type_6, unsqueeze_9, unsqueeze_10, unsqueeze_11, getitem_30, getitem_31, getitem_32, getitem_33, add_13, rsqrt_7, mul_38, mm_14, gt_10, mul_41, gt_11, sym_size_int, sym_size_int_1)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s1, 384), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j1)  # primals_5
	    buf4 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (384,), is_leaf=True)  # primals_6
	    buf5 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf5, (1152, 384), is_leaf=True)  # primals_7
	    buf6 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf6, (384, 384), is_leaf=True)  # primals_8
	    buf7 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf7, (384,), is_leaf=True)  # primals_9
	    buf8 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf8, (1024, 384), is_leaf=True)  # primals_10
	    buf9 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf9, (384, 1024), is_leaf=True)  # primals_11
	    buf10 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf10, (384,), is_leaf=True)  # primals_12
	    buf11 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf11, (1152, 384), is_leaf=True)  # primals_13
	    buf12 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf12, (384, 384), is_leaf=True)  # primals_14
	    buf13 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf13, (384,), is_leaf=True)  # primals_15
	    buf14 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf14, (1024, 384), is_leaf=True)  # primals_16
	    buf15 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf15, (384, 1024), is_leaf=True)  # primals_17
	    buf16 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf16, (384,), is_leaf=True)  # primals_18
	    buf17 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf17, (1152, 384), is_leaf=True)  # primals_19
	    buf18 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf18, (384, 384), is_leaf=True)  # primals_20
	    buf19 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf19, (384,), is_leaf=True)  # primals_21
	    buf20 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf20, (1024, 384), is_leaf=True)  # primals_22
	    buf21 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf21, (384, 1024), is_leaf=True)  # primals_23
	    buf22 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf22, (384,), is_leaf=True)  # primals_24
	    buf23 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf23, (1152, 384), is_leaf=True)  # primals_25
	    buf24 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf24, (384, 384), is_leaf=True)  # primals_26
	    buf25 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf25, (384,), is_leaf=True)  # primals_27
	    buf26 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf26, (1024, 384), is_leaf=True)  # primals_28
	    buf27 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf27, (384, 1024), is_leaf=True)  # primals_29
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:10:00.135593 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "78d83b1bd782e460f1365195aea3fecd"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[12][1]cuda:0" = torch.ops.prims.inductor_seeds.default(12, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_11, 0.3);  inductor_random_default_11 = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_1: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_10, 0.3);  inductor_random_default_10 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_2: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2)
	        inductor_random_default_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_9, 0.3);  inductor_random_default_9 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
	        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
	        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
	        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12);  mul_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_3: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 3)
	        inductor_random_default_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_3, 'rand');  inductor_lookup_seed_default_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_8, 0.3);  inductor_random_default_8 = None
	        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
	        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
	        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
	        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
	        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
	        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
	        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
	        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
	        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
	        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0, 2]);  view_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_16, 0);  permute_16 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_18, 0);  permute_18 = None
	        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
	        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
	        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
	        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_20, [sym_size_int, 384]);  permute_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_3, view_7, permute_21);  view_7 = permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_2, 2)
	        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
	        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
	        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
	        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_2, rsqrt_3)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15);  mul_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
	        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_4: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 4)
	        inductor_random_default_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_4, 'rand');  inductor_lookup_seed_default_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_7, 0.3);  inductor_random_default_7 = None
	        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
	        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
	        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
	        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_5: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 5)
	        inductor_random_default_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_5, 'rand');  inductor_lookup_seed_default_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_6, 0.3);  inductor_random_default_6 = None
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_2, mul_21);  mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
	        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
	        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
	        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
	        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18);  mul_22 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_6: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 6)
	        inductor_random_default_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_6, 'rand');  inductor_lookup_seed_default_6 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_5, 0.3);  inductor_random_default_5 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
	        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
	        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
	        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
	        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
	        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
	        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_8, [1, 0, 2]);  view_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
	        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_9, [1, 0, 2]);  view_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
	        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_10, [1, 0, 2]);  view_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_25, [1, 0, 2]);  permute_25 = None
	        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_26, [1, 0, 2]);  permute_26 = None
	        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_27, [1, 0, 2]);  permute_27 = None
	        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_28, 0);  permute_28 = None
	        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_29, 0);  permute_29 = None
	        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_30, 0);  permute_30 = None
	        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
	        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
	        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
	        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
	        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
	        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_32, [sym_size_int, 384]);  permute_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_7, view_11, permute_33);  view_11 = permute_33 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default_1, 2)
	        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
	        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
	        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default_1, rsqrt_5)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21);  mul_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
	        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
	        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
	        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_7: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 7)
	        inductor_random_default_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_7, 'rand');  inductor_lookup_seed_default_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_4, 0.3);  inductor_random_default_4 = None
	        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
	        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
	        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
	        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_8: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 8)
	        inductor_random_default_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_8, 'rand');  inductor_lookup_seed_default_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_3, 0.3);  inductor_random_default_3 = None
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
	        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
	        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default_1, mul_32);  mul_32 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
	        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
	        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
	        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
	        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24);  mul_33 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_9: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 9)
	        inductor_random_default_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_9, 'rand');  inductor_lookup_seed_default_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
	        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
	        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
	        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
	        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
	        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
	        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
	        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_12, [1, 0, 2]);  view_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
	        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_13, [1, 0, 2]);  view_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
	        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_14, [1, 0, 2]);  view_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_37, [1, 0, 2]);  permute_37 = None
	        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_38, [1, 0, 2]);  permute_38 = None
	        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_39, [1, 0, 2]);  permute_39 = None
	        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_40, 0);  permute_40 = None
	        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_41, 0);  permute_41 = None
	        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_42, 0);  permute_42 = None
	        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
	        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
	        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
	        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
	        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
	        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_44, [sym_size_int, 384]);  permute_44 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
	        
	        # No stacktrace found for following nodes
	        addmm_default: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.addmm.default(add_11, view_15, permute_45);  view_15 = permute_45 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(addmm_default, 2)
	        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
	        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
	        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
	        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(addmm_default, rsqrt_7)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27);  mul_37 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
	        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
	        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
	        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_10: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 10)
	        inductor_random_default_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_10, 'rand');  inductor_lookup_seed_default_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
	        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
	        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
	        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_11: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 11);  inductor_seeds_default = None
	        inductor_random_default: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_11, 'rand');  inductor_lookup_seed_default_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
	        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
	        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(addmm_default, mul_43);  mul_43 = None
	        return (add_15, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, add_3, rsqrt_2, gt_3, mul_14, convert_element_type_2, unsqueeze_3, unsqueeze_4, unsqueeze_5, getitem_12, getitem_13, getitem_14, getitem_15, addmm_default_2, rsqrt_3, mul_16, mm_6, gt_4, mul_19, gt_5, add_7, rsqrt_4, gt_6, mul_25, convert_element_type_4, unsqueeze_6, unsqueeze_7, unsqueeze_8, getitem_21, getitem_22, getitem_23, getitem_24, addmm_default_1, rsqrt_5, mul_27, mm_10, gt_7, mul_30, gt_8, add_11, rsqrt_6, gt_9, mul_36, convert_element_type_6, unsqueeze_9, unsqueeze_10, unsqueeze_11, getitem_30, getitem_31, getitem_32, getitem_33, addmm_default, rsqrt_7, mul_38, mm_14, gt_10, mul_41, gt_11, sym_size_int, sym_size_int_1)
	        
V0303 09:10:00.137520 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "d2cf112cba2aa18fbf89832626e99d6a"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993000137443.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.285663 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "63a5713551b5ab761fd5b7cc9071fd81"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993000285600.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.287891 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "8ad755fc07b87c416fe28c83a484b58b"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993000287819.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.288081 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "28c06d9b663ffb5321e1a218bc4dc002"}
	{
	"name": "code_gen",
	"ts": 1740993000287819.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.289808 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "e8cd3a4ba91505ffffc2ae9526d9d5ef"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993000289742.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.432315 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "11f2ac195f250d367509b98af3e60e05"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993000432260.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.432569 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "6071d2a69a1902389e7050d3db5f35e9"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993000432506.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.638705 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "5ccccb96d188056c473216a1c076537e"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993000638436.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.639286 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "b7c0dc95c7183247e0d4d3d3d4d3aee0"}
	{
	"name": "code_gen",
	"ts": 1740993000639229.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.639494 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "225cbadb215cd2484e9692a8610eb6e8"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993000639445.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.640262 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "17da9bd919f6da0a1b9aa30412834dbc"}
	{
	"name": "inductor_compile",
	"ts": 1740993000640210.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.640483 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "43e56927ef5ae280eb68a038bd87887e"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993000640438.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.641458 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "184f7c12089e297c04b3ea128100dc08"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993000641389.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.644690 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "3baba4468ef977537c0bc9e08b4d7962"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993000644634.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.645222 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "be3c300d9fb4fb16c2a2a82696fc6786"}
	{
	"name": "backend_compile",
	"ts": 1740993000645170.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.645430 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "90cce7132fded5a6dee2aeca3ae60be4"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993000645384.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.647273 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "6190ef0300a5ee69c865473ba56b9dd3"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993000647221.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.647490 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0, "has_payload": "f186a47d30e946f76b2f3e3cbd7f9f03"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993000647444.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 7,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.647836 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "7/0", "frame_key": "22", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py", "co_firstlineno": 122, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740992996.0574522, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmphh2en008/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmphh2en008/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmphh2en008', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 4.59023380279541, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 7, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.653654 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 131, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 68, "name": "forward", "filename": 8}]}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.654041 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "62ecb4cf0e96f510d1f3adccde34ffc6"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993000653928.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.654278 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "e4d1d7c8aa1939749edeee4e889cf7de"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993000653928.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.662052 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 20, "size": 6291456}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.662513 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 20, "size": 6291456}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.662904 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [384, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4069d10>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.663169 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 20, "size": 2056}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.663510 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.663767 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 20, "size": 0}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.664114 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.664358 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 20, "size": 0}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.664697 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.665715 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j1", 384, 1], "storage": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9f4069630>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.665980 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 4, "source": "L['x']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.694153 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 20, "size": 1536}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.694471 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12090>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.694642 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 6, "source": "L['self']._modules['attn_norm']._parameters['weight']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.706583 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 20, "size": 1769472}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.706867 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12c70>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.707034 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 7, "source": "L['self']._modules['attention']._modules['qkv']._parameters['weight']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.755261 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 20, "size": 589824}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.755533 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 32, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 7, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12d60>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.755691 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 32, "source": "L['self']._modules['attention']._modules['proj']._parameters['weight']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.775245 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 8, "describer_id": 20, "size": 1536}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.775512 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 35, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 8, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e50>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.775670 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 35, "source": "L['self']._modules['ff']._modules['0']._parameters['weight']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.782301 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 9, "describer_id": 20, "size": 1572864}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.782567 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 36, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 9, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e00>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.782728 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 36, "source": "L['self']._modules['ff']._modules['1']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.798489 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 10, "describer_id": 20, "size": 1572864}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.798758 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 38, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 10, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b123b0>", "describer_id": 20}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.798925 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 20, "id": 38, "source": "L['self']._modules['ff']._modules['1']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:00.820450 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_modules_attn_norm_parameters_weight_": [384], "l_self_modules_attention_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_attention_modules_proj_parameters_weight_": [384, 384], "l_self_modules_ff_modules_0_parameters_weight_": [384], "l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_": [384, 1024]}}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "4f02002f9e30afe7213b77a1e8974ac4"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_attn_norm_parameters_weight_ = L_self_modules_attn_norm_parameters_weight_
	        l_self_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_attention_modules_qkv_parameters_weight_
	        l_self_modules_attention_modules_proj_parameters_weight_ = L_self_modules_attention_modules_proj_parameters_weight_
	        l_self_modules_ff_modules_0_parameters_weight_ = L_self_modules_ff_modules_0_parameters_weight_
	        l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1.pow(2)
	        mean: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
	        add: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean + 1e-06;  mean = None
	        rsqrt: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        mul: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1 * rsqrt;  float_1 = rsqrt = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul.type_as(l_x_);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output * l_self_modules_attn_norm_parameters_weight_;  output = l_self_modules_attn_norm_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        dropout: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_1, 0.3, True, False);  mul_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout, l_self_modules_attention_modules_qkv_parameters_weight_, None);  dropout = l_self_modules_attention_modules_qkv_parameters_weight_ = None
	        chunk = linear.chunk(3, dim = -1);  linear = None
	        queries: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[0]
	        keys: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[1]
	        values: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[2];  chunk = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries.unflatten(-1, [6, 64]);  queries = None
	        queries_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_1: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys.unflatten(-1, [6, 64]);  keys = None
	        keys_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_2: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values.unflatten(-1, [6, 64]);  values = None
	        values_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_1, keys_1, values_1, dropout_p = False, is_causal = False);  queries_1 = keys_1 = values_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_3: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
	        context_vec_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_1, l_self_modules_attention_modules_proj_parameters_weight_, None);  context_vec_1 = l_self_modules_attention_modules_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        attn_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_ + context_vec_2;  l_x_ = context_vec_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        float_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out.float()
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2.pow(2)
	        mean_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_2.mean(-1, keepdim = True);  pow_2 = None
	        add_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_1 + 1e-06;  mean_1 = None
	        rsqrt_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_2);  add_2 = None
	        mul_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2 * rsqrt_1;  float_2 = rsqrt_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
	        output_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_2.type_as(attn_out);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        input_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_1 * l_self_modules_ff_modules_0_parameters_weight_;  output_1 = l_self_modules_ff_modules_0_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_2: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_1, l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_1 = l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
	        input_3: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_2, inplace = False);  input_2 = None
	        input_4: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_3, 0.3, True, False);  input_3 = None
	        input_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_4, l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_4 = l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        input_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_5, 0.3, True, False);  input_5 = None
	        proj_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out + input_6;  attn_out = input_6 = None
	        return (proj_out,)
	        
V0303 09:10:00.820983 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "995da339dca79cfc2f7ef27ca6a66e3d"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993000820906.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.821166 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "a2383c70ab3df40aa47762dc7e1c7a2d"}
	{
	"name": "backend_compile",
	"ts": 1740993000820906.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:00.838984 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "30220e9b2fe9065e2e71f7c9e670ccfe"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993000838924.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.463734 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "d514860b44c0bff207ce50043f20c2f9"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[384][1]cuda:0"; primals_7: "f32[1152, 384][384, 1]cuda:0"; primals_8: "f32[384, 384][384, 1]cuda:0"; primals_9: "f32[384][1]cuda:0"; primals_10: "f32[1024, 384][384, 1]cuda:0"; primals_11: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        alias: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
	        alias_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
	        alias_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        rand: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        alias_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        alias_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        alias_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_6: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_6, [1, 0, 2]);  alias_6 = None
	        alias_7: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_7, [1, 0, 2]);  alias_7 = None
	        alias_8: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_8, [1, 0, 2]);  alias_8 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        alias_11: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
	        alias_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
	        alias_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_11, 0);  alias_11 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_12, 0);  alias_12 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_13, 0);  alias_13 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        alias_14: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_3)
	        alias_15: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0);  getitem_3 = None
	        alias_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_17: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_7);  permute_7 = None
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_17, [1, 0, 2]);  alias_17 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        alias_18: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_1)
	        alias_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
	        alias_20: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        rand_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_1, 0.3);  rand_1 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        rand_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_2, 0.3);  rand_2 = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  mul_10 = None
	        convert_element_type_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, 1.4285714285714286);  convert_element_type_2 = None
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_11);  mul_11 = None
	        clone: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_12, memory_format = torch.contiguous_format);  mul_12 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone, primals_11);  primals_11 = None
	        permute_12: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone, [1, 0]);  clone = None
	        mm_5: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_12, mul_8);  permute_12 = mul_8 = None
	        convert_element_type_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
	        mul_13: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_3, 1.4285714285714286);  convert_element_type_3 = None
	        mul_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_4, mul_13);  mm_4 = mul_13 = None
	        clone_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_14, memory_format = torch.contiguous_format);  mul_14 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        full_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_2, sigmoid_1);  full_2 = None
	        mul_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub);  mm_2 = sub = None
	        add_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_15, 1);  mul_15 = None
	        mul_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_4);  sigmoid_1 = add_4 = None
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_1, mul_16);  clone_1 = mul_16 = None
	        mm_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_17, primals_10);  primals_10 = None
	        permute_14: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_17, [1, 0]);  mul_17 = None
	        mm_7: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_14, mul_5);  permute_14 = mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, mul_4);  mul_4 = None
	        mul_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, primals_9);  mm_6 = primals_9 = None
	        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_18, [0], True);  mul_18 = None
	        unsqueeze_3: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view_4: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_3, [384]);  unsqueeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, add_1)
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, rsqrt_1);  mul_19 = rsqrt_1 = None
	        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_20, [1], True);  mul_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_21);  tangents_1 = mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
	        alias_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
	        alias_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
	        pow_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_23, 3);  alias_23 = None
	        mul_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
	        mul_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, pow_3);  mul_22 = pow_3 = None
	        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_23, [-1, 384]);  mul_23 = None
	        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_4, 2.0);  pow_4 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_24);  div = mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_25);  add_5 = mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_6, primals_8);  primals_8 = None
	        permute_15: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_6, [1, 0])
	        mm_9: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_15, view_3);  permute_15 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [sym_size_int, 6, 64]);  mm_8 = None
	        alias_24: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_5);  view_5 = None
	        permute_16: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_25: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_16);  permute_16 = None
	        permute_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
	        alias_26: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_17);  permute_17 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_26, 0);  alias_26 = None
	        alias_27: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
	        alias_28: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_27);  alias_27 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_4, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_28, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_4 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_28 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_9: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_10: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_11, 0);  getitem_11 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_10, 0);  getitem_10 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_9, 0);  getitem_9 = None
	        alias_29: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
	        permute_18: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_29, [1, 0, 2]);  alias_29 = None
	        alias_30: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_30, [1, 0, 2]);  alias_30 = None
	        alias_31: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
	        permute_20: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_31, [1, 0, 2]);  alias_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_32: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_18);  permute_18 = None
	        permute_21: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_32, [1, 0, 2]);  alias_32 = None
	        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_21, [sym_size_int, 384]);  permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_33: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_19);  permute_19 = None
	        permute_22: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_33, [1, 0, 2]);  alias_33 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_22, [sym_size_int, 384]);  permute_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_34: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_20);  permute_20 = None
	        permute_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_34, [1, 0, 2]);  alias_34 = None
	        view_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_23, [sym_size_int, 384]);  permute_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        split_1 = torch.ops.aten.split.Tensor(full_3, 384, 1)
	        getitem_13: "f32[s1, 384][1152, 1]cuda:0" = split_1[0];  split_1 = None
	        alias_35: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_13);  getitem_13 = None
	        alias_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_8);  view_8 = None
	        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_35, alias_36);  alias_35 = alias_36 = None
	        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, copy, 1, 0, 384);  full_3 = copy = None
	        alias_39: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_7);  view_7 = None
	        split_4 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_23: "f32[s1, 384][1152, 1]cuda:0" = split_4[1];  split_4 = None
	        alias_40: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_23);  getitem_23 = None
	        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_40, alias_39);  alias_40 = alias_39 = None
	        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        alias_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_6);  view_6 = None
	        split_7 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
	        getitem_33: "f32[s1, 384][1152, 1]cuda:0" = split_7[2];  split_7 = None
	        alias_44: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_33);  getitem_33 = None
	        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_44, alias_43);  alias_44 = alias_43 = None
	        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
	        mm_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_7);  primals_7 = None
	        permute_25: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
	        mm_11: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_25, mul_3);  permute_25 = mul_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_4, 1.4285714285714286);  convert_element_type_4 = None
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, mul_26);  mm_10 = mul_26 = None
	        clone_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_27, memory_format = torch.contiguous_format);  mul_27 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, mul);  mul = None
	        mul_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, primals_6);  clone_2 = primals_6 = None
	        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_28, [0], True);  mul_28 = None
	        unsqueeze_5: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
	        view_9: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_5, [384]);  unsqueeze_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, primals_1)
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, rsqrt);  mul_29 = rsqrt = None
	        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_30, [1], True);  mul_30 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6, mul_31);  add_6 = mul_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        alias_46: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
	        alias_47: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_46);  alias_46 = None
	        alias_48: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_47);  alias_47 = None
	        pow_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_48, 3);  alias_48 = None
	        mul_32: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
	        mul_33: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_32, pow_5);  mul_32 = pow_5 = None
	        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_33, [-1, 384]);  mul_33 = None
	        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_6, 2.0);  pow_6 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_34);  div_1 = mul_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mul_35);  add_7 = mul_35 = None
	        return pytree.tree_unflatten([add_3, primals_2, primals_3, primals_4, add_8, tangents_2, tangents_3, tangents_4, None, view_9, mm_11, mm_9, view_4, mm_7, mm_5], self._out_spec)
	        
V0303 09:10:01.571735 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "9b25f8d9e4e2ed551eac91d8be0f444a"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[3][1]cuda:0" = torch.ops.prims.inductor_seeds.default(3, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_1: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_2: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2);  inductor_seeds_default = None
	        inductor_random_default: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        return (add_3, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, sym_size_int, sym_size_int_1)
	        
V0303 09:10:01.578081 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "759ff4797287e9c356ad6dacc128065a"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", rsqrt: "f32[s1, 1][1, 1]cuda:0", gt: "b8[s1, 384][384, 1]cuda:0", mul_3: "f32[s1, 384][384, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_5: "i64[][]cuda:0", getitem_6: "i64[][]cuda:0", mm_1: "f32[s1, 384][384, 1]cuda:0", rsqrt_1: "f32[s1, 1][1, 1]cuda:0", mul_5: "f32[s1, 384][384, 1]cuda:0", mm_2: "f32[s1, 1024][1024, 1]cuda:0", gt_1: "b8[s1, 1024][1024, 1]cuda:0", mul_8: "f32[s1, 1024][1024, 1]cuda:0", gt_2: "b8[s1, 384][384, 1]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cuda:0", tangents_4: "f32[s4, 0][1, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        convert_element_type_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
	        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, 1.4285714285714286);  convert_element_type_2 = None
	        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_11);  mul_11 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_12, primals_11);  primals_11 = None
	        permute_12: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_12, [1, 0]);  mul_12 = None
	        mm_5: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_12, mul_8);  permute_12 = mul_8 = None
	        convert_element_type_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
	        mul_13: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_3, 1.4285714285714286);  convert_element_type_3 = None
	        mul_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_4, mul_13);  mm_4 = mul_13 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        full_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_2, sigmoid_1);  full_2 = None
	        mul_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub);  mm_2 = sub = None
	        add_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_15, 1);  mul_15 = None
	        mul_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_4);  sigmoid_1 = add_4 = None
	        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_14, mul_16);  mul_14 = mul_16 = None
	        mm_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_17, primals_10);  primals_10 = None
	        permute_14: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_17, [1, 0]);  mul_17 = None
	        mm_7: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_14, mul_5);  permute_14 = mul_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, mul_4);  mul_4 = None
	        mul_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, primals_9);  mm_6 = primals_9 = None
	        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_18, [0], True);  mul_18 = None
	        unsqueeze_3: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
	        view_4: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_3, [384]);  unsqueeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, add_1)
	        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, rsqrt_1);  mul_19 = None
	        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_20, [1], True);  mul_20 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_21);  tangents_1 = mul_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_1, 3);  rsqrt_1 = None
	        mul_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
	        mul_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, pow_3);  mul_22 = pow_3 = None
	        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_23, [-1, 384]);  mul_23 = None
	        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
	        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
	        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_4, 2.0);  pow_4 = None
	        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_24);  div = mul_24 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_25);  add_5 = mul_25 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_6, primals_8);  primals_8 = None
	        permute_15: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_6, [1, 0])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_9: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_15, view_3);  permute_15 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [sym_size_int, 6, 64]);  mm_8 = None
	        permute_16: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0, 2]);  permute_16 = None
	        unsqueeze_4: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_4, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem_3, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_4 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem_3 = convert_element_type = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_9: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_10: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_11, 0);  getitem_11 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_10, 0);  getitem_10 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_9, 0);  getitem_9 = None
	        permute_18: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        permute_20: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_21: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_18, [1, 0, 2]);  permute_18 = None
	        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_21, [sym_size_int, 384]);  permute_21 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_22: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_22, [sym_size_int, 384]);  permute_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_20, [1, 0, 2]);  permute_20 = None
	        view_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_23, [sym_size_int, 384]);  permute_23 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        split_1 = torch.ops.aten.split.Tensor(full_3, 384, 1)
	        getitem_13: "f32[s1, 384][1152, 1]cuda:0" = split_1[0];  split_1 = None
	        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_13, view_8);  getitem_13 = view_8 = None
	        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, copy, 1, 0, 384);  full_3 = copy = None
	        split_4 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_23: "f32[s1, 384][1152, 1]cuda:0" = split_4[1];  split_4 = None
	        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_23, view_7);  getitem_23 = view_7 = None
	        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        split_7 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
	        getitem_33: "f32[s1, 384][1152, 1]cuda:0" = split_7[2];  split_7 = None
	        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_33, view_6);  getitem_33 = view_6 = None
	        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
	        mm_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_7);  primals_7 = None
	        permute_25: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
	        mm_11: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_25, mul_3);  permute_25 = mul_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        convert_element_type_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_4, 1.4285714285714286);  convert_element_type_4 = None
	        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, mul_26);  mm_10 = mul_26 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_27, mul);  mul = None
	        mul_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_27, primals_6);  mul_27 = primals_6 = None
	        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_28, [0], True);  mul_28 = None
	        unsqueeze_5: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
	        view_9: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_5, [384]);  unsqueeze_5 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        mul_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, primals_1)
	        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, rsqrt);  mul_29 = None
	        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_30, [1], True);  mul_30 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6, mul_31);  add_6 = mul_31 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
	        mul_32: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
	        mul_33: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_32, pow_5);  mul_32 = pow_5 = None
	        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_33, [-1, 384]);  mul_33 = None
	        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
	        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
	        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_6, 2.0);  pow_6 = None
	        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_34);  div_1 = mul_34 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        add_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mul_35);  add_7 = mul_35 = None
	        return (add_8, tangents_2, tangents_3, tangents_4, None, view_9, mm_11, mm_9, view_4, mm_7, mm_5)
	        
V0303 09:10:01.578556 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "7443a50b765c27e0795194a70427e8c4"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993001578337.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.578871 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "f53877045d53f826b7fe030cfca586d8"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993001578812.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.579045 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "a85a6c48227ace716a6504950e42a24e"}
	{
	"name": "inductor_compile",
	"ts": 1740993001578812.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.592782 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "1cde0105a89787f3d02892cd4d4434e2"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11):
	        pow_1 = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt = torch.ops.aten.rsqrt.default(add);  add = None
	        mul = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        mul_1 = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
	        inductor_seeds_default = torch.ops.prims.inductor_seeds.default(3, device(type='cuda', index=0))
	        inductor_lookup_seed_default = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_2 = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        gt = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_2 = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3 = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        permute = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem = split[0]
	        getitem_1 = split[1]
	        getitem_2 = split[2];  split = None
	        view = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1 = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        view_1 = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2 = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        view_2 = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3 = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        permute_4 = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5 = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6 = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1 = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2 = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1 = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3 = _efficient_attention_forward[0]
	        getitem_4 = _efficient_attention_forward[1]
	        getitem_5 = _efficient_attention_forward[2]
	        getitem_6 = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7 = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3 = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        permute_9 = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1 = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        add_1 = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        pow_2 = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1 = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2 = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1 = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4 = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        mul_5 = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        permute_10 = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2 = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6 = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        inductor_lookup_seed_default_1 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_1 = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        gt_1 = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_7 = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8 = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11 = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3 = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        inductor_lookup_seed_default_2 = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2);  inductor_seeds_default = None
	        inductor_random_default = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        gt_2 = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_9 = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10 = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3 = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        return (add_3, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, sym_size_int, sym_size_int_1)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s1, 384), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j1)  # primals_5
	    buf4 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (384,), is_leaf=True)  # primals_6
	    buf5 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf5, (1152, 384), is_leaf=True)  # primals_7
	    buf6 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf6, (384, 384), is_leaf=True)  # primals_8
	    buf7 = reader.storage(None, 1536, device=device(type='cuda', index=0))
	    reader.tensor(buf7, (384,), is_leaf=True)  # primals_9
	    buf8 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf8, (1024, 384), is_leaf=True)  # primals_10
	    buf9 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf9, (384, 1024), is_leaf=True)  # primals_11
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:10:01.649595 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "017ab17040176ecb0c09596134904fb7"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
	        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
	        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
	        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
	        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6);  mul = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[3][1]cuda:0" = torch.ops.prims.inductor_seeds.default(3, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0)
	        inductor_random_default_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_2, 0.3);  inductor_random_default_2 = None
	        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
	        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
	        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
	        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
	        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
	        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
	        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
	        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
	        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9);  mul_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_1: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 1)
	        inductor_random_default_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default_1, 'rand');  inductor_lookup_seed_default_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default_1, 0.3);  inductor_random_default_1 = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
	        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
	        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
	        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
	        
	        # No stacktrace found for following nodes
	        inductor_lookup_seed_default_2: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 2);  inductor_seeds_default = None
	        inductor_random_default: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 384], inductor_lookup_seed_default_2, 'rand');  inductor_lookup_seed_default_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
	        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
	        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
	        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  add_1 = mul_10 = None
	        return (add_3, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, rsqrt, gt, mul_3, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, mm_1, rsqrt_1, mul_5, mm_2, gt_1, mul_8, gt_2, sym_size_int, sym_size_int_1)
	        
V0303 09:10:01.650502 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "c540ac25969953c87fd7cee0fe5e3d36"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993001650436.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.694520 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "76a89df561702578d6cd9bb3bd3589b4"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993001694466.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.695380 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "7697818ffb7ac3722227eeab8671911a"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993001695316.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.695568 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "f2c96dad10173fb74c284eff04bf9148"}
	{
	"name": "code_gen",
	"ts": 1740993001695316.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.696514 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "47c85e4397a71fff4afcafbb63fc2a55"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993001696450.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.735103 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "4e6a36933c7d2f73eae7b0739121327a"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993001735048.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.735348 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "e3943c3ab484e08919793a70b22c0fde"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993001735286.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.937671 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "83e69d4ade3a7e36408dade24f3cafe9"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993001937408.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.938292 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "3a72fc96b0d66c2b21fc8e8219d1626d"}
	{
	"name": "code_gen",
	"ts": 1740993001938225.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.938506 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "d017bc961caf4f649749e10cd57aeea5"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993001938458.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.939281 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "f79ebce2d92c0c6e936ea961504ece70"}
	{
	"name": "inductor_compile",
	"ts": 1740993001939226.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.939494 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "aa122e4315b67dbffb11b365902c4fd8"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993001939447.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.940249 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "95ebb41aa7532a6851e89b6990368c7e"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993001940197.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.943919 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "3755ddee9976eb5be1f44db99842b02f"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993001943844.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.944553 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "bb5dd68c4d69aa08726d56934801be20"}
	{
	"name": "backend_compile",
	"ts": 1740993001944484.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.944864 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "42149174090b811ab82dfeab6597bf8b"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993001944800.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.946141 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "7439796f17588fcab79cd62cb0d4193f"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993001946082.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.946353 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0, "has_payload": "fd59906f6a407d47b6091f58ec6cd8fb"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993001946307.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 8,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.946712 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "8/0", "frame_key": "23", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py", "co_firstlineno": 68, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740993000.6539152, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp8zw2l8xc/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmp8zw2l8xc/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp8zw2l8xc', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 1.292642593383789, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 8, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.955393 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py", 9]}
V0303 09:10:01.955631 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 131, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 76, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 184, "name": "forward", "filename": 9}]}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.955998 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "2e734288eec5a8aa581fc42022aca5dc"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993001955887.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.956239 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "c69e14b09a7f2e72feb5f18aefc62037"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993001955887.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:01.962898 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 22, "size": 1769472}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.963205 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1152, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12c70>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.963371 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 22, "id": 0, "source": "L['self']._modules['qkv']._parameters['weight']"}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.965455 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 22, "size": 6291456}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.965753 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 22, "size": 6291456}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.966012 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [384, 1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb92b380450>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.966184 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 22, "size": 2056}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.966412 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.966576 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 22, "size": 0}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.966795 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.966953 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 22, "size": 0}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.968621 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.969221 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j1", 384, 1], "storage": 1, "attrs": {"_values": 1, "_offsets": 2, "_min_seqlen_tensor": 3, "_max_seqlen_tensor": 4}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb92bc10f90>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:01.969387 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 22, "id": 5, "source": "L['x']"}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.025886 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 22, "size": 589824}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.026175 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 30, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12d60>", "describer_id": 22}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.026342 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 22, "id": 30, "source": "L['self']._modules['proj']._parameters['weight']"}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.034904 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_modules_qkv_parameters_weight_": [1152, 384], "l_self_modules_proj_parameters_weight_": [384, 384]}}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "f11b6f8086262f5eae0feeb6ff46e08c"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_self_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_x_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0"):
	        l_self_modules_qkv_parameters_weight_ = L_self_modules_qkv_parameters_weight_
	        l_x_ = L_x_
	        l_self_modules_proj_parameters_weight_ = L_self_modules_proj_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        linear: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_qkv_parameters_weight_, None);  l_x_ = l_self_modules_qkv_parameters_weight_ = None
	        chunk = linear.chunk(3, dim = -1);  linear = None
	        queries: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[0]
	        keys: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[1]
	        values: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[2];  chunk = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries.unflatten(-1, [6, 64]);  queries = None
	        queries_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_1: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys.unflatten(-1, [6, 64]);  keys = None
	        keys_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_2: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values.unflatten(-1, [6, 64]);  values = None
	        values_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_1, keys_1, values_1, dropout_p = False, is_causal = False);  queries_1 = keys_1 = values_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_3: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
	        context_vec_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        context_vec_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_1, l_self_modules_proj_parameters_weight_, None);  context_vec_1 = l_self_modules_proj_parameters_weight_ = None
	        return (context_vec_2,)
	        
V0303 09:10:02.035399 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "e15a9018793f0e9b17822791f5e00187"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993002035327.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.035583 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "23188c6c043338699b70c05b0b569748"}
	{
	"name": "backend_compile",
	"ts": 1740993002035327.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.045451 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "844de0e1db12791dc3d3f39f872a1657"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993002045390.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.397100 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "e90bbbb3b2ec3918429a36597626007d"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[1152, 384][384, 1]cuda:0"; primals_2: "f32[s1, 384][384, 1]cuda:0"; primals_3: "i64[257][1]cuda:0"; primals_4: "f32[s3, 0][1, 1]cuda:0"; primals_5: "f32[s4, 0][1, 1]cuda:0"; primals_6: "Sym(s0)"; primals_7: "f32[384, 384][384, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cpu"; tangents_4: "f32[s4, 0][1, 1]cpu"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_1, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(primals_2, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_2, 0)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        alias: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias, [1, 0, 2]);  alias = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        alias_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_1, [1, 0, 2]);  alias_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        alias_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_2, [1, 0, 2]);  alias_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
	        alias_4: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
	        alias_5: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_3, torch.int32)
	        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_3, torch.int32)
	        alias_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
	        alias_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
	        alias_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_8, 0);  alias_8 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_9, 0);  alias_9 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_10, 0);  alias_10 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_5, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        alias_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_3)
	        alias_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_11);  alias_11 = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0);  getitem_3 = None
	        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        alias_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_13, [1, 0, 2]);  alias_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_14: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_7);  permute_7 = None
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_14, [1, 0, 2]);  alias_14 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  permute_9 = None
	        mm_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(tangents_1, primals_7);  primals_7 = None
	        permute_10: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(tangents_1, [1, 0]);  tangents_1 = None
	        mm_3: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_10, view_3);  permute_10 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [sym_size_int, 6, 64]);  mm_2 = None
	        alias_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_4);  view_4 = None
	        permute_11: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_15, [1, 0, 2]);  alias_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_16: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_11);  permute_11 = None
	        permute_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
	        alias_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_12);  permute_12 = None
	        unsqueeze_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_17, 0);  alias_17 = None
	        alias_18: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_12);  alias_12 = None
	        alias_19: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_3, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_19, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_3 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_19 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_9: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_10: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_11, 0);  getitem_11 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_10, 0);  getitem_10 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_9, 0);  getitem_9 = None
	        alias_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
	        permute_13: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_20, [1, 0, 2]);  alias_20 = None
	        alias_21: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
	        permute_14: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_21, [1, 0, 2]);  alias_21 = None
	        alias_22: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
	        permute_15: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_22, [1, 0, 2]);  alias_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_23: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_13);  permute_13 = None
	        permute_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_23, [1, 0, 2]);  alias_23 = None
	        view_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_16, [sym_size_int, 384]);  permute_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_24: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_14);  permute_14 = None
	        permute_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
	        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_17, [sym_size_int, 384]);  permute_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_25: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_15);  permute_15 = None
	        permute_18: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_18, [sym_size_int, 384]);  permute_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        split_1 = torch.ops.aten.split.Tensor(full_2, 384, 1)
	        getitem_13: "f32[s1, 384][1152, 1]cuda:0" = split_1[0];  split_1 = None
	        alias_26: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_13);  getitem_13 = None
	        alias_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_7);  view_7 = None
	        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_26, alias_27);  alias_26 = alias_27 = None
	        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_2, copy, 1, 0, 384);  full_2 = copy = None
	        alias_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_6);  view_6 = None
	        split_4 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_23: "f32[s1, 384][1152, 1]cuda:0" = split_4[1];  split_4 = None
	        alias_31: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_23);  getitem_23 = None
	        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_31, alias_30);  alias_31 = alias_30 = None
	        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        alias_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_5);  view_5 = None
	        split_7 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
	        getitem_33: "f32[s1, 384][1152, 1]cuda:0" = split_7[2];  split_7 = None
	        alias_35: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_33);  getitem_33 = None
	        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_35, alias_34);  alias_35 = alias_34 = None
	        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
	        mm_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_1);  primals_1 = None
	        permute_20: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
	        mm_5: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_20, primals_2);  permute_20 = primals_2 = None
	        return pytree.tree_unflatten([mm_1, primals_3, full, full_1, mm_5, mm_4, primals_3, primals_4, primals_5, None, mm_3], self._out_spec)
	        
V0303 09:10:02.450925 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "8cf933f8f663113e006321a85a080165"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[1152, 384][384, 1]cuda:0", primals_2: "f32[s1, 384][384, 1]cuda:0", primals_3: "i64[257][1]cuda:0", primals_4: "f32[s3, 0][1, 1]cuda:0", primals_5: "f32[s4, 0][1, 1]cuda:0", primals_6: "Sym(s0)", primals_7: "f32[384, 384][384, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_1, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(primals_2, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_2, 0)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_3, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_5, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        return (mm_1, primals_3, full, full_1, primals_1, primals_2, primals_3, primals_4, primals_5, primals_7, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, sym_size_int, sym_size_int_1)
	        
V0303 09:10:02.454240 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "8f3e22776d8b1fe255594f76e8503bac"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_1: "f32[1152, 384][384, 1]cuda:0", primals_2: "f32[s1, 384][384, 1]cuda:0", primals_3: "i64[257][1]cuda:0", primals_4: "f32[s3, 0][1, 1]cuda:0", primals_5: "f32[s4, 0][1, 1]cuda:0", primals_7: "f32[384, 384][384, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_5: "i64[][]cuda:0", getitem_6: "i64[][]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cpu", tangents_4: "f32[s4, 0][1, 1]cpu"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(tangents_1, primals_7);  primals_7 = None
	        permute_10: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(tangents_1, [1, 0]);  tangents_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        mm_3: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_10, view_3);  permute_10 = view_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [sym_size_int, 6, 64]);  mm_2 = None
	        permute_11: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0, 2]);  permute_11 = None
	        unsqueeze_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_12, 0);  permute_12 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_3, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem_3, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_3 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem_3 = convert_element_type = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
	        getitem_9: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_10: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_11, 0);  getitem_11 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_10, 0);  getitem_10 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_9, 0);  getitem_9 = None
	        permute_13: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        permute_14: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        permute_15: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_13, [1, 0, 2]);  permute_13 = None
	        view_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_16, [sym_size_int, 384]);  permute_16 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_14, [1, 0, 2]);  permute_14 = None
	        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_17, [sym_size_int, 384]);  permute_17 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_18: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_15, [1, 0, 2]);  permute_15 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_18, [sym_size_int, 384]);  permute_18 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        full_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        split_1 = torch.ops.aten.split.Tensor(full_2, 384, 1)
	        getitem_13: "f32[s1, 384][1152, 1]cuda:0" = split_1[0];  split_1 = None
	        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_13, view_7);  getitem_13 = view_7 = None
	        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_2, copy, 1, 0, 384);  full_2 = copy = None
	        split_4 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
	        getitem_23: "f32[s1, 384][1152, 1]cuda:0" = split_4[1];  split_4 = None
	        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_23, view_6);  getitem_23 = view_6 = None
	        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
	        split_7 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
	        getitem_33: "f32[s1, 384][1152, 1]cuda:0" = split_7[2];  split_7 = None
	        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_33, view_5);  getitem_33 = view_5 = None
	        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
	        mm_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_1);  primals_1 = None
	        permute_20: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
	        mm_5: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_20, primals_2);  permute_20 = primals_2 = None
	        return (mm_5, mm_4, primals_3, primals_4, primals_5, None, mm_3)
	        
V0303 09:10:02.454686 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "10a1d0b336a6879308f34c94018b29b2"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993002454462.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.455030 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "8c58b01ac43e39497ffc8b8d899e02c4"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993002454969.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.455209 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "63a588a4e137e47885b38f59d536bf1c"}
	{
	"name": "inductor_compile",
	"ts": 1740993002454969.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.464838 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "196ef3f3be803d39f79df06fb13a3227"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7):
	        permute = torch.ops.aten.permute.default(primals_1, [1, 0])
	        mm = torch.ops.aten.mm.default(primals_2, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem = split[0]
	        getitem_1 = split[1]
	        getitem_2 = split[2];  split = None
	        sym_size_int = torch.ops.aten.sym_size.int(primals_2, 0)
	        view = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1 = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        view_1 = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2 = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        view_2 = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3 = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        permute_4 = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5 = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6 = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type = torch.ops.prims.convert_element_type.default(primals_3, torch.int32)
	        unsqueeze = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1 = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2 = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1 = torch.ops.aten.sym_size.int(primals_5, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3 = _efficient_attention_forward[0]
	        getitem_4 = _efficient_attention_forward[1]
	        getitem_5 = _efficient_attention_forward[2]
	        getitem_6 = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        sym_size_int_2 = torch.ops.aten.sym_size.int(primals_4, 0)
	        full = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1 = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        permute_7 = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        permute_8 = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3 = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        permute_9 = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1 = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        return (mm_1, primals_3, full, full_1, primals_1, primals_2, primals_3, primals_4, primals_5, primals_7, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, sym_size_int, sym_size_int_1)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 1769472, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (1152, 384), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf1, (s1, 384), is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf2, (257,), dtype=torch.int64, is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s3, 0), is_leaf=True)  # primals_4
	    buf4 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (s4, 0), is_leaf=True)  # primals_5
	    reader.symint(j1)  # primals_6
	    buf5 = reader.storage(None, 589824, device=device(type='cuda', index=0))
	    reader.tensor(buf5, (384, 384), is_leaf=True)  # primals_7
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:10:02.489368 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "2a052cb6d7d5bfd2de2467dbfdc8e04d"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[1152, 384][384, 1]cuda:0", primals_2: "f32[s1, 384][384, 1]cuda:0", primals_3: "i64[257][1]cuda:0", primals_4: "f32[s3, 0][1, 1]cuda:0", primals_5: "f32[s4, 0][1, 1]cuda:0", primals_6: "Sym(s0)", primals_7: "f32[384, 384][384, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
	        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_1, [1, 0])
	        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(primals_2, permute);  permute = None
	        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
	        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
	        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
	        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_2, 0)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem, [sym_size_int, 6, 64]);  getitem = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
	        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_3, [1, 0, 2]);  permute_3 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_3, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_6, 0);  permute_6 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_5, 0)
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
	        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_4, 0)
	        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_8, [sym_size_int, 384]);  permute_8 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
	        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  view_3 = permute_9 = None
	        return (mm_1, primals_3, full, full_1, primals_1, primals_2, primals_3, primals_4, primals_5, primals_7, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem_3, getitem_4, getitem_5, getitem_6, sym_size_int, sym_size_int_1)
	        
V0303 09:10:02.490113 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "42123bd515ae36251ba93bdd4fe99ea8"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993002490044.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.512810 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "fb50c2ef9d28c7935ded5880f29ae019"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993002512756.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.513598 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "c2b8819d1a207cc3a8994b506054c9fa"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993002513533.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.513780 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "70c43e7f7686efbaed59f537a9a7921b"}
	{
	"name": "code_gen",
	"ts": 1740993002513533.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.514569 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "7f056e6da6c6536049f7b61421722379"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993002514504.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.518433 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "0fd612ebe4e887b665541b7d2f8cfc93"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993002518381.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.518669 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "e72de1004af936b05248ec77882eea46"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993002518611.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.701478 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "e895296f4a427230196daa5c8ea8f38b"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993002701218.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.702095 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "edd147f895fe2ebcaae7948504fe1cf6"}
	{
	"name": "code_gen",
	"ts": 1740993002702031.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.702319 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "c4eb1f632cb0377b2429b0d20cafce19"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993002702273.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.703102 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "78d1585a233bdecb9acbc87c2a50d6fe"}
	{
	"name": "inductor_compile",
	"ts": 1740993002703047.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.703313 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "695b3ebbb39b266f8262d45d7e14f865"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993002703266.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.704079 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "44f5328502852c646209987583d0dda3"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993002704026.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.707247 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "70b986ba34acecc746e43a32e766f688"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993002707191.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.707680 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "ba9e66335249ba2fa5a88080cfdc3b5d"}
	{
	"name": "backend_compile",
	"ts": 1740993002707631.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.707884 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "85e4a235735197bcba0bea916988925f"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993002707840.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.708692 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "9a0fcbd82820401adf42004fe804c8e3"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993002708643.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.708890 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0, "has_payload": "cbd21e06313bea489515a48db0861d09"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993002708848.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 9,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.709241 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "9/0", "frame_key": "24", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py", "co_firstlineno": 184, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740993001.955873, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpdf35b3ni/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpdf35b3ni/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpdf35b3ni', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.7532169818878174, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 9, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.716504 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 131, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 76, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 113, "name": "jagged_forward", "filename": 9}]}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.716856 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "7865eeaaac8296b967878d36cb1d8159"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993002716766.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.717074 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "7b65436e6e6749a660c94a04cb9e905c"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993002716766.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.722268 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 24, "size": 18874368}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.722581 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 24, "size": 18874368}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.722926 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [1152, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb92b3c8720>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.723160 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 24, "size": 2056}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.723477 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.723700 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 24, "size": 0}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.723992 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.724203 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 24, "size": 0}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.724487 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.725430 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 24, "size": 18874368}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.725864 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 1152], "is_leaf": true, "stride": [1152, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc57dbd0>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.726718 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 6, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 1152], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["1152*j1", 1152, 1], "storage": 5, "attrs": {"_values": 5, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb92b512220>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.727040 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_view": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["1152*j1", 1152, 1], "storage": 0, "base": 6, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "creation_meta": "CreationMeta.MULTI_OUTPUT_NODE", "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb92bb7bc70>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.727216 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 24, "id": 4, "source": "L['qu']"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.769753 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 24, "id": 0, "source": "L['qu']._values"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.782700 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 24, "size": 18874368}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.783100 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 12, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [1152, 1], "storage_offset": 384, "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb92b3de1d0>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.784537 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 13, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_view": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["1152*j1", 1152, 1], "storage_offset": 384, "storage": 6, "base": 6, "attrs": {"_values": 12, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "creation_meta": "CreationMeta.MULTI_OUTPUT_NODE", "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb92bc4edb0>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.784713 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 24, "id": 13, "source": "L['ke']"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.818023 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_utils_internal.py", 10]}
V0303 09:10:02.818203 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py", 11]}
V0303 09:10:02.818342 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", 12]}
V0303 09:10:02.818470 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/variables/builtin.py", 13]}
V0303 09:10:02.818596 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/variables/lazy.py", 14]}
V0303 09:10:02.818787 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/variables/builder.py", 15]}
V0303 09:10:02.818926 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", 16]}
V0303 09:10:02.819051 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py", 17]}
V0303 09:10:02.819173 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_subclasses/meta_utils.py", 18]}
V0303 09:10:02.819293 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/__init__.py", 19]}
V0303 09:10:02.819415 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/fx/experimental/symbolic_shapes.py", 20]}
V0303 09:10:02.819536 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/fx/experimental/sym_node.py", 21]}
V0303 09:10:02.819656 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/fx/experimental/recording.py", 22]}
V0303 09:10:02.819775 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/_logging/_internal.py", 23]}
V0303 09:10:02.819909 12578 torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s9", "sources": ["L['ke'].storage_offset()", "L['ke']._values.storage_offset()"], "value": "384", "reason": "range_refined_to_singleton", "stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 131, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 76, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 225, "name": "forward", "filename": 9}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 1064, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 10}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 11}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 12}, {"line": 983, "name": "run", "filename": 12}, {"line": 895, "name": "step", "filename": 12}, {"line": 1744, "name": "LOAD_ATTR", "filename": 12}, {"line": 1734, "name": "_load_attr", "filename": 12}, {"line": 967, "name": "call_function", "filename": 13}, {"line": 712, "name": "<lambda>", "filename": 13}, {"line": 712, "name": "<listcomp>", "filename": 13}, {"line": 63, "name": "realize", "filename": 14}, {"line": 29, "name": "realize", "filename": 14}, {"line": 377, "name": "__call__", "filename": 15}, {"line": 559, "name": "_wrap", "filename": 15}, {"line": 1593, "name": "wrap_tensor", "filename": 15}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 15}, {"line": 2149, "name": "wrap_fx_proxy_cls", "filename": 15}, {"line": 2709, "name": "wrap_to_fake_tensor_and_record", "filename": 15}, {"line": 1574, "name": "wrap_fake_exception", "filename": 16}, {"line": 2710, "name": "<lambda>", "filename": 15}, {"line": 2238, "name": "from_tensor", "filename": 17}, {"line": 375, "name": "from_real_tensor", "filename": 17}, {"line": 1660, "name": "__call__", "filename": 18}, {"line": 1401, "name": "meta_tensor", "filename": 18}, {"line": 1078, "name": "view_from_base", "filename": 18}, {"line": 1564, "name": "_check", "filename": 19}, {"line": 1527, "name": "_check_with", "filename": 19}, {"line": 1010, "name": "expect_true", "filename": 20}, {"line": 465, "name": "expect_true", "filename": 21}, {"line": 449, "name": "guard_bool", "filename": 21}, {"line": 262, "name": "wrapper", "filename": 22}, {"line": 5122, "name": "evaluate_expr", "filename": 20}, {"line": 5282, "name": "_evaluate_expr", "filename": 20}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 20}, {"line": 5469, "name": "_refine_ranges", "filename": 20}, {"line": 4842, "name": "_set_replacement", "filename": 20}, {"line": 1122, "name": "trace_structured", "filename": 23}], "user_stack": [{"line": 115, "name": "jagged_forward", "filename": 9}]}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.820742 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 24, "id": 12, "source": "L['ke']._values"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.836686 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 7, "describer_id": 24, "size": 18874368}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.837053 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 19, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [1152, 1], "storage_offset": 768, "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb92b527590>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.838730 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 20, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_view": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["1152*j1", 1152, 1], "storage_offset": 768, "storage": 7, "base": 6, "attrs": {"_values": 19, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "creation_meta": "CreationMeta.MULTI_OUTPUT_NODE", "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb929bfcf90>", "describer_id": 24}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.838902 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 24, "id": 20, "source": "L['va']"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.869756 12578 torch/fx/experimental/symbolic_shapes.py:4842] {"symbolic_shape_specialization": {"symbol": "s12", "sources": ["L['va'].storage_offset()", "L['va']._values.storage_offset()"], "value": "768", "reason": "range_refined_to_singleton", "stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 131, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 76, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 225, "name": "forward", "filename": 9}, {"line": 1269, "name": "__call__", "filename": 0}, {"line": 1064, "name": "__call__", "filename": 0}, {"line": 526, "name": "__call__", "filename": 0}, {"line": 924, "name": "_compile", "filename": 0}, {"line": 666, "name": "compile_inner", "filename": 0}, {"line": 87, "name": "wrapper_function", "filename": 10}, {"line": 699, "name": "_compile_inner", "filename": 0}, {"line": 1322, "name": "transform_code_object", "filename": 11}, {"line": 219, "name": "_fn", "filename": 0}, {"line": 634, "name": "transform", "filename": 0}, {"line": 2796, "name": "run", "filename": 12}, {"line": 983, "name": "run", "filename": 12}, {"line": 895, "name": "step", "filename": 12}, {"line": 1744, "name": "LOAD_ATTR", "filename": 12}, {"line": 1734, "name": "_load_attr", "filename": 12}, {"line": 967, "name": "call_function", "filename": 13}, {"line": 712, "name": "<lambda>", "filename": 13}, {"line": 712, "name": "<listcomp>", "filename": 13}, {"line": 63, "name": "realize", "filename": 14}, {"line": 29, "name": "realize", "filename": 14}, {"line": 377, "name": "__call__", "filename": 15}, {"line": 559, "name": "_wrap", "filename": 15}, {"line": 1593, "name": "wrap_tensor", "filename": 15}, {"line": 2037, "name": "wrap_fx_proxy", "filename": 15}, {"line": 2149, "name": "wrap_fx_proxy_cls", "filename": 15}, {"line": 2709, "name": "wrap_to_fake_tensor_and_record", "filename": 15}, {"line": 1574, "name": "wrap_fake_exception", "filename": 16}, {"line": 2710, "name": "<lambda>", "filename": 15}, {"line": 2238, "name": "from_tensor", "filename": 17}, {"line": 375, "name": "from_real_tensor", "filename": 17}, {"line": 1660, "name": "__call__", "filename": 18}, {"line": 1401, "name": "meta_tensor", "filename": 18}, {"line": 1078, "name": "view_from_base", "filename": 18}, {"line": 1564, "name": "_check", "filename": 19}, {"line": 1527, "name": "_check_with", "filename": 19}, {"line": 1010, "name": "expect_true", "filename": 20}, {"line": 465, "name": "expect_true", "filename": 21}, {"line": 449, "name": "guard_bool", "filename": 21}, {"line": 262, "name": "wrapper", "filename": 22}, {"line": 5122, "name": "evaluate_expr", "filename": 20}, {"line": 5282, "name": "_evaluate_expr", "filename": 20}, {"line": 4927, "name": "_maybe_guard_rel", "filename": 20}, {"line": 5469, "name": "_refine_ranges", "filename": 20}, {"line": 4842, "name": "_set_replacement", "filename": 20}, {"line": 1122, "name": "trace_structured", "filename": 23}], "user_stack": [{"line": 116, "name": "jagged_forward", "filename": 9}]}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.870589 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 24, "id": 19, "source": "L['va']._values"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:02.915135 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {}}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "4648de156539fb867f4b2a53d203ecd3"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_qu_: "f32[256, s5, 384][1152*s5, 1152, 1]cuda:0", s5: "Sym(s5)", L_ke_: "f32[256, s8, 384][1152*s8, 1152, 1]cuda:0", s8: "Sym(s8)", L_va_: "f32[256, s11, 384][1152*s11, 1152, 1]cuda:0", s11: "Sym(s11)"):
	        l_qu_ = L_qu_
	        l_ke_ = L_ke_
	        l_va_ = L_va_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten: "f32[256, s5, 6, 64][1152*s5, 1152, 64, 1]cuda:0" = l_qu_.unflatten(-1, [6, 64]);  l_qu_ = None
	        queries: "f32[256, 6, s5, 64][1152*s5, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_1: "f32[256, s8, 6, 64][1152*s8, 1152, 64, 1]cuda:0" = l_ke_.unflatten(-1, [6, 64]);  l_ke_ = None
	        keys: "f32[256, 6, s8, 64][1152*s8, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        unflatten_2: "f32[256, s11, 6, 64][1152*s11, 1152, 64, 1]cuda:0" = l_va_.unflatten(-1, [6, 64]);  l_va_ = None
	        values: "f32[256, 6, s11, 64][1152*s11, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        context_vec: "f32[256, 6, s11, 64][384*s11, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries, keys, values, dropout_p = False, is_causal = False);  queries = keys = values = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        transpose_3: "f32[256, s11, 6, 64][384*s11, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
	        context_vec_1: "f32[256, s11, 384][384*s11, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
	        return (context_vec_1,)
	        
V0303 09:10:02.915648 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "d5e5975069bfe92aa66e46a0356ea7c8"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993002915572.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.915836 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "45e4676978d7005d1cc30cfaf6165b52"}
	{
	"name": "backend_compile",
	"ts": 1740993002915572.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:02.948552 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "b118850d782a5ab3d6d83e42632080a5"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993002948481.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.233735 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "0784a7960c2119ac7f31f3bf16f25ac6"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 384][1152, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s11)"; primals_6: "f32[s1, 384][1152, 1]cuda:0"; primals_7: "i64[257][1]cuda:0"; primals_8: "f32[s3, 0][1, 1]cuda:0"; primals_9: "f32[s4, 0][1, 1]cuda:0"; primals_10: "Sym(s11)"; primals_11: "f32[s1, 384][1152, 1]cuda:0"; primals_12: "i64[257][1]cuda:0"; primals_13: "f32[s3, 0][1, 1]cuda:0"; primals_14: "f32[s4, 0][1, 1]cuda:0"; primals_15: "Sym(s11)"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cpu"; tangents_4: "f32[s4, 0][1, 1]cpu"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_1, [sym_size_int, 6, 64]);  primals_1 = None
	        alias: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
	        permute: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias, [1, 0, 2]);  alias = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_6, [sym_size_int, 6, 64]);  primals_6 = None
	        alias_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_1, [1, 0, 2]);  alias_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_11, [sym_size_int, 6, 64]);  primals_11 = None
	        alias_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_2, [1, 0, 2]);  alias_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute);  permute = None
	        permute_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
	        alias_4: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
	        alias_5: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_12, torch.int32)
	        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_12, torch.int32)
	        alias_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
	        alias_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
	        alias_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_8, 0);  alias_8 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_9, 0);  alias_9 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_10, 0);  alias_10 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0);  primals_4 = None
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_1: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_2: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_3: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        alias_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem)
	        alias_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_11);  alias_11 = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem, 0);  getitem = None
	        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_3, 0);  primals_3 = None
	        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        alias_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
	        permute_6: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_13, [1, 0, 2]);  alias_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        alias_14: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
	        permute_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_14, [1, 0, 2]);  alias_14 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_7, [sym_size_int, 384]);  permute_7 = None
	        view_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [sym_size_int, 6, 64]);  tangents_1 = None
	        alias_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_4);  view_4 = None
	        permute_8: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_15, [1, 0, 2]);  alias_15 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        alias_16: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_8);  permute_8 = None
	        permute_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
	        alias_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_9);  permute_9 = None
	        unsqueeze_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_17, 0);  alias_17 = None
	        alias_18: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_12);  alias_12 = None
	        alias_19: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_3, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_19, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_1, 0.0, getitem_2, getitem_3, 0, False);  unsqueeze_3 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_19 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_1 = getitem_2 = getitem_3 = None
	        getitem_6: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_7: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_8: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_8, 0);  getitem_8 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_7, 0);  getitem_7 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_6, 0);  getitem_6 = None
	        alias_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
	        permute_10: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_20, [1, 0, 2]);  alias_20 = None
	        alias_21: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
	        permute_11: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_21, [1, 0, 2]);  alias_21 = None
	        alias_22: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
	        permute_12: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_22, [1, 0, 2]);  alias_22 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_23: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_10);  permute_10 = None
	        permute_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_23, [1, 0, 2]);  alias_23 = None
	        view_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_13, [sym_size_int, 384]);  permute_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_24: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_11);  permute_11 = None
	        permute_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
	        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_14, [sym_size_int, 384]);  permute_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        alias_25: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_12);  permute_12 = None
	        permute_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_15, [sym_size_int, 384]);  permute_15 = sym_size_int = None
	        return pytree.tree_unflatten([view_3, primals_12, full, full_1, view_7, primals_12, primals_13, primals_14, None, view_6, primals_12, primals_13, primals_14, None, view_5, primals_12, primals_13, primals_14, None], self._out_spec)
	        
V0303 09:10:03.269878 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "b07dd904394054e8327a86bb1821843f"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][1152, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s11)", primals_6: "f32[s1, 384][1152, 1]cuda:0", primals_7: "i64[257][1]cuda:0", primals_8: "f32[s3, 0][1, 1]cuda:0", primals_9: "f32[s4, 0][1, 1]cuda:0", primals_10: "Sym(s11)", primals_11: "f32[s1, 384][1152, 1]cuda:0", primals_12: "i64[257][1]cuda:0", primals_13: "f32[s3, 0][1, 1]cuda:0", primals_14: "f32[s4, 0][1, 1]cuda:0", primals_15: "Sym(s11)"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_1, [sym_size_int, 6, 64]);  primals_1 = None
	        permute: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_6, [sym_size_int, 6, 64]);  primals_6 = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(primals_11, [sym_size_int, 6, 64]);  primals_11 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute, [1, 0, 2]);  permute = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_12, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_3, 0);  permute_3 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0);  primals_4 = None
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_1: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_2: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_3: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem, 0)
	        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_3, 0);  primals_3 = None
	        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        permute_6: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_6, [1, 0, 2]);  permute_6 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_7, [sym_size_int, 384]);  permute_7 = None
	        return (view_3, primals_12, full, full_1, primals_12, primals_13, primals_14, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem, getitem_1, getitem_2, getitem_3, sym_size_int, sym_size_int_1)
	        
V0303 09:10:03.272174 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "1594b6b60e82066731e4e123b8f0761a"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_12: "i64[257][1]cuda:0", primals_13: "f32[s3, 0][1, 1]cuda:0", primals_14: "f32[s4, 0][1, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_1: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_2: "i64[][]cuda:0", getitem_3: "i64[][]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cpu", tangents_4: "f32[s4, 0][1, 1]cpu"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        view_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(tangents_1, [sym_size_int, 6, 64]);  tangents_1 = None
	        permute_8: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [1, 0, 2]);  view_4 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_8, [1, 0, 2]);  permute_8 = None
	        unsqueeze_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_9, 0);  permute_9 = None
	        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_3, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_1, 0.0, getitem_2, getitem_3, 0, False);  unsqueeze_3 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem = convert_element_type = sym_size_int_1 = getitem_1 = getitem_2 = getitem_3 = None
	        getitem_6: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
	        getitem_7: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
	        getitem_8: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
	        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_8, 0);  getitem_8 = None
	        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_7, 0);  getitem_7 = None
	        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_6, 0);  getitem_6 = None
	        permute_10: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
	        permute_11: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
	        permute_12: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0, 2]);  permute_10 = None
	        view_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_13, [sym_size_int, 384]);  permute_13 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0, 2]);  permute_11 = None
	        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_14, [sym_size_int, 384]);  permute_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        permute_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_12, [1, 0, 2]);  permute_12 = None
	        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_15, [sym_size_int, 384]);  permute_15 = sym_size_int = None
	        return (view_7, primals_12, primals_13, primals_14, None, view_6, primals_12, primals_13, primals_14, None, view_5, primals_12, primals_13, primals_14, None)
	        
V0303 09:10:03.272603 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "7d3cfc2bd8fc76f1cf69f2a6fc50423e"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993003272386.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.272914 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "ef6e45f8ccea4a1c163bc3781ec0f634"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993003272854.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.273091 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "2be6ffd537ccb809fc327763ee001e0a"}
	{
	"name": "inductor_compile",
	"ts": 1740993003272854.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.284454 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "5a9a81b29d4f65ea25e4e2414b3f8dfa"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15):
	        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
	        view = torch.ops.aten.view.default(primals_1, [sym_size_int, 6, 64]);  primals_1 = None
	        permute = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        view_1 = torch.ops.aten.view.default(primals_6, [sym_size_int, 6, 64]);  primals_6 = None
	        permute_1 = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        view_2 = torch.ops.aten.view.default(primals_11, [sym_size_int, 6, 64]);  primals_11 = None
	        permute_2 = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        permute_3 = torch.ops.aten.permute.default(permute, [1, 0, 2]);  permute = None
	        permute_4 = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5 = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        convert_element_type = torch.ops.prims.convert_element_type.default(primals_12, torch.int32)
	        unsqueeze = torch.ops.aten.unsqueeze.default(permute_3, 0);  permute_3 = None
	        unsqueeze_1 = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_2 = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        sym_size_int_1 = torch.ops.aten.sym_size.int(primals_4, 0);  primals_4 = None
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem = _efficient_attention_forward[0]
	        getitem_1 = _efficient_attention_forward[1]
	        getitem_2 = _efficient_attention_forward[2]
	        getitem_3 = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze = torch.ops.aten.squeeze.dim(getitem, 0)
	        sym_size_int_2 = torch.ops.aten.sym_size.int(primals_3, 0);  primals_3 = None
	        full = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1 = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        permute_6 = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        permute_7 = torch.ops.aten.permute.default(permute_6, [1, 0, 2]);  permute_6 = None
	        view_3 = torch.ops.aten.view.default(permute_7, [sym_size_int, 384]);  permute_7 = None
	        return (view_3, primals_12, full, full_1, primals_12, primals_13, primals_14, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem, getitem_1, getitem_2, getitem_3, sym_size_int, sym_size_int_1)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 4608*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s1, 384), (1152, 1), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j1)  # primals_5
	    reader.tensor(buf0, (s1, 384), (1152, 1), storage_offset=384, is_leaf=True)  # primals_6
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_7
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_8
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_9
	    reader.symint(j1)  # primals_10
	    reader.tensor(buf0, (s1, 384), (1152, 1), storage_offset=768, is_leaf=True)  # primals_11
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_12
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_13
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_14
	    reader.symint(j1)  # primals_15
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:10:03.305476 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "db238b8c0baae0302cb3f058adbb18db"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][1152, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s11)", primals_6: "f32[s1, 384][1152, 1]cuda:0", primals_7: "i64[257][1]cuda:0", primals_8: "f32[s3, 0][1, 1]cuda:0", primals_9: "f32[s4, 0][1, 1]cuda:0", primals_10: "Sym(s11)", primals_11: "f32[s1, 384][1152, 1]cuda:0", primals_12: "i64[257][1]cuda:0", primals_13: "f32[s3, 0][1, 1]cuda:0", primals_14: "f32[s4, 0][1, 1]cuda:0", primals_15: "Sym(s11)"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(primals_1, [sym_size_int, 6, 64]);  primals_1 = None
	        permute: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view, [1, 0, 2]);  view = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(primals_6, [sym_size_int, 6, 64]);  primals_6 = None
	        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [1, 0, 2]);  view_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
	        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.reshape.default(primals_11, [sym_size_int, 6, 64]);  primals_11 = None
	        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [1, 0, 2]);  view_2 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
	        permute_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute, [1, 0, 2]);  permute = None
	        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_1, [1, 0, 2]);  permute_1 = None
	        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_2, [1, 0, 2]);  permute_2 = None
	        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_12, torch.int32)
	        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_3, 0);  permute_3 = None
	        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_4, 0);  permute_4 = None
	        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_5, 0);  permute_5 = None
	        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0);  primals_4 = None
	        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
	        getitem: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
	        getitem_1: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
	        getitem_2: "i64[][]cuda:0" = _efficient_attention_forward[2]
	        getitem_3: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
	        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem, 0)
	        sym_size_int_2: "Sym(s3)" = torch.ops.aten.sym_size.int(primals_3, 0);  primals_3 = None
	        full: "f32[s3, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_2, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False);  sym_size_int_2 = None
	        full_1: "f32[s4, 0][1, 1]cpu" = torch.ops.aten.full.default([sym_size_int_1, 0], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
	        permute_6: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
	        permute_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_6, [1, 0, 2]);  permute_6 = None
	        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.reshape.default(permute_7, [sym_size_int, 384]);  permute_7 = None
	        return (view_3, primals_12, full, full_1, primals_12, primals_13, primals_14, convert_element_type, unsqueeze, unsqueeze_1, unsqueeze_2, getitem, getitem_1, getitem_2, getitem_3, sym_size_int, sym_size_int_1)
	        
V0303 09:10:03.306179 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "410a2a803688fc9cc855c2d4beb0cc68"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993003306113.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.327100 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "e535f66657e895c42729591cb0ac3702"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993003327048.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.327847 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "fb1c712571d13f2a6627a1bbad7a19af"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993003327782.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.328033 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "36793b42b2c6a9bfad4c6f7485308827"}
	{
	"name": "code_gen",
	"ts": 1740993003327782.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.328924 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "5d350b1cebaf9fe0f26b110f2690d92d"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993003328862.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.332752 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "a86400e94183e028539b5e149401ef12"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993003332702.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.332992 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "7db728c4f95cf428a6e50ed57b52221f"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993003332931.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.458141 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "a3caa174f2ef2398f769d7021b1061b8"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993003457883.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.458748 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "3dab2ab45c5c7e781038868b80c1255f"}
	{
	"name": "code_gen",
	"ts": 1740993003458684.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.458979 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "9e972048bcc5ad0d71fd0cab3cf7a20b"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993003458927.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.459757 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "de7a866c65e5305c17f99335c83ad951"}
	{
	"name": "inductor_compile",
	"ts": 1740993003459700.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.459969 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "290f6c33cc2d2657b817f79b3b7835a2"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993003459922.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.460730 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "d675b25f334eea28aa028acb05d5c7d5"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993003460676.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.464296 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "874cc2d288a1a7b0d66a4595c1f14da7"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993003464241.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.464816 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "0644cc3dc675cbf79cc8672208cdabee"}
	{
	"name": "backend_compile",
	"ts": 1740993003464763.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.465028 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "9431752fae6d359f991167f1a43f557c"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993003464985.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.465776 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "bb666ae4df3f2c40c4d1209a017c7eab"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993003465724.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.465987 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0, "has_payload": "47c0a616c0e71df3204089ad71a08e6c"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993003465945.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 10,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.466341 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "10/0", "frame_key": "25", "co_name": "jagged_forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py", "co_firstlineno": 113, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740993002.716754, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmphfibk_np/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmphfibk_np/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmphfibk_np', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.7494363784790039, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 10, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.485401 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/torch/nn/modules/container.py", 24]}
V0303 09:10:03.485594 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py", 25]}
V0303 09:10:03.485741 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 321, "name": "_predict", "filename": 4}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 182, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 131, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 81, "name": "forward", "filename": 8}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 250, "name": "forward", "filename": 24}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 34, "name": "forward", "filename": 25}]}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.486009 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "24d9776290fbad0d20fbd0fd128fa779"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993003485920.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.486190 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "038f71f808f8c09a60ae91258ff30fbb"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993003485920.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.488931 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 26, "size": 6291456}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.489252 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 26, "size": 6291456}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.489556 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [4096, 384], "is_leaf": true, "stride": [384, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb92af70ae0>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.489737 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 26, "size": 2056}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.489981 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "1", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f41324f0>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.490156 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 26, "size": 0}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.490379 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [9, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132400>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.490544 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 26, "size": 0}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.490762 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [77, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132540>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.491361 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j1", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j1", 384, 1], "storage": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb92af70450>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.491532 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 26, "id": 4, "source": "L['x']"}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.505596 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 26, "size": 1572864}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.505881 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b12e00>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.506045 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 26, "id": 5, "source": "L['self']._modules['mlp']._modules['0']._parameters['weight']"}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.523586 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 26, "size": 1572864}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.523855 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 7, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [384, 1024], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1024, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba18b123b0>", "describer_id": 26}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.524017 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 26, "id": 7, "source": "L['self']._modules['mlp']._modules['3']._parameters['weight']"}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:03.530970 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_modules_mlp_modules_0_parameters_weight_": [1024, 384], "l_self_modules_mlp_modules_3_parameters_weight_": [384, 1024]}}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "5281cf9ef48847817813c8c55b3181ae"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0"):
	        l_x_ = L_x_
	        l_self_modules_mlp_modules_0_parameters_weight_ = L_self_modules_mlp_modules_0_parameters_weight_
	        l_self_modules_mlp_modules_3_parameters_weight_ = L_self_modules_mlp_modules_3_parameters_weight_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        input_1: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(l_x_, l_self_modules_mlp_modules_0_parameters_weight_, None);  l_x_ = l_self_modules_mlp_modules_0_parameters_weight_ = None
	        input_2: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_1, inplace = False);  input_1 = None
	        input_3: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_2, 0.3, True, False);  input_2 = None
	        input_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_3, l_self_modules_mlp_modules_3_parameters_weight_, None);  input_3 = l_self_modules_mlp_modules_3_parameters_weight_ = None
	        return (input_4,)
	        
V0303 09:10:03.531443 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "fbc934ebbc881b52367add56df22215c"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993003531380.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.531624 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "520df730c18b8f70547c93eea19b29aa"}
	{
	"name": "backend_compile",
	"ts": 1740993003531380.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.539347 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "dc8b2d0a708588a756bd75ffb84d37eb"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993003539291.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.665266 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "a7e6a21e029bcb868d140c8a9643a8e4"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[1024, 384][384, 1]cuda:0"; primals_7: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
	        mul: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sigmoid);  sigmoid = None
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        rand: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
	        gt: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
	        mul_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul);  mul = None
	        mul_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1, 1.4285714285714286);  mul_1 = None
	        permute_1: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_2, permute_1);  permute_1 = None
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(tangents_1, primals_7);  primals_7 = None
	        permute_2: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(tangents_1, [1, 0]);  tangents_1 = None
	        mm_3: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_2, mul_2);  permute_2 = mul_2 = None
	        convert_element_type: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, 1.4285714285714286);  convert_element_type = None
	        mul_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, mul_3);  mm_2 = mul_3 = None
	        clone: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_4, memory_format = torch.contiguous_format);  mul_4 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
	        full: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
	        mul_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sub);  mm = sub = None
	        add: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_5, 1);  mul_5 = None
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add);  sigmoid_1 = add = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone, mul_6);  clone = mul_6 = None
	        mm_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_7, primals_6);  primals_6 = None
	        permute_4: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_7, [1, 0]);  mul_7 = None
	        mm_5: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_4, primals_1);  permute_4 = primals_1 = None
	        return pytree.tree_unflatten([mm_1, primals_2, primals_3, primals_4, mm_4, tangents_2, tangents_3, tangents_4, None, mm_5, mm_3], self._out_spec)
	        
V0303 09:10:03.693201 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "653f773aeaa3af7d99868588d19faacb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[1024, 384][384, 1]cuda:0", primals_7: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
	        mul: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sigmoid);  sigmoid = None
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[1][1]cuda:0" = torch.ops.prims.inductor_seeds.default(1, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0);  inductor_seeds_default = None
	        inductor_random_default: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul);  mul = None
	        mul_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1, 1.4285714285714286);  mul_1 = None
	        permute_1: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_2, permute_1);  permute_1 = None
	        return (mm_1, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, mm, gt, mul_2, sym_size_int)
	        
V0303 09:10:03.694484 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "769c31554eec876b3df2f60ce357a97b"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_6: "f32[1024, 384][384, 1]cuda:0", primals_7: "f32[384, 1024][1024, 1]cuda:0", mm: "f32[s1, 1024][1024, 1]cuda:0", gt: "b8[s1, 1024][1024, 1]cuda:0", mul_2: "f32[s1, 1024][1024, 1]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cuda:0", tangents_4: "f32[s4, 0][1, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(tangents_1, primals_7);  primals_7 = None
	        permute_2: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(tangents_1, [1, 0]);  tangents_1 = None
	        mm_3: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_2, mul_2);  permute_2 = mul_2 = None
	        convert_element_type: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
	        mul_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, 1.4285714285714286);  convert_element_type = None
	        mul_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, mul_3);  mm_2 = mul_3 = None
	        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
	        full: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
	        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
	        mul_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sub);  mm = sub = None
	        add: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_5, 1);  mul_5 = None
	        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add);  sigmoid_1 = add = None
	        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, mul_6);  mul_4 = mul_6 = None
	        mm_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_7, primals_6);  primals_6 = None
	        permute_4: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_7, [1, 0]);  mul_7 = None
	        mm_5: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_4, primals_1);  permute_4 = primals_1 = None
	        return (mm_4, tangents_2, tangents_3, tangents_4, None, mm_5, mm_3)
	        
V0303 09:10:03.694924 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "8df953d759c043f70f8bcf4ae63fdb1a"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993003694680.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.695223 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "340ca87a0e6478e8a3ef95809dbd11ca"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993003695161.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.695472 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "fde7738b19cdf0850084502442ff09fd"}
	{
	"name": "inductor_compile",
	"ts": 1740993003695161.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.702814 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "d985d11158861c9ac664299aa72de9ed"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7):
	        permute = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        sigmoid = torch.ops.aten.sigmoid.default(mm)
	        mul = torch.ops.aten.mul.Tensor(mm, sigmoid);  sigmoid = None
	        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
	        inductor_seeds_default = torch.ops.prims.inductor_seeds.default(1, device(type='cuda', index=0))
	        inductor_lookup_seed_default = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0);  inductor_seeds_default = None
	        inductor_random_default = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        gt = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_1 = torch.ops.aten.mul.Tensor(gt, mul);  mul = None
	        mul_2 = torch.ops.aten.mul.Tensor(mul_1, 1.4285714285714286);  mul_1 = None
	        permute_1 = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1 = torch.ops.aten.mm.default(mul_2, permute_1);  permute_1 = None
	        return (mm_1, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, mm, gt, mul_2, sym_size_int)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s1, 384), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j1)  # primals_5
	    buf4 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (1024, 384), is_leaf=True)  # primals_6
	    buf5 = reader.storage(None, 1572864, device=device(type='cuda', index=0))
	    reader.tensor(buf5, (384, 1024), is_leaf=True)  # primals_7
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:10:03.716059 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "653f773aeaa3af7d99868588d19faacb"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[1024, 384][384, 1]cuda:0", primals_7: "f32[384, 1024][1024, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        permute: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
	        mul: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sigmoid);  sigmoid = None
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        
	        # No stacktrace found for following nodes
	        inductor_seeds_default: "i64[1][1]cuda:0" = torch.ops.prims.inductor_seeds.default(1, device(type='cuda', index=0))
	        inductor_lookup_seed_default: "i64[][]cuda:0" = torch.ops.prims.inductor_lookup_seed.default(inductor_seeds_default, 0);  inductor_seeds_default = None
	        inductor_random_default: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.inductor_random.default([sym_size_int, 1024], inductor_lookup_seed_default, 'rand');  inductor_lookup_seed_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
	        gt: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(inductor_random_default, 0.3);  inductor_random_default = None
	        mul_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul);  mul = None
	        mul_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1, 1.4285714285714286);  mul_1 = None
	        permute_1: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
	        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_2, permute_1);  permute_1 = None
	        return (mm_1, primals_2, primals_3, primals_4, primals_1, primals_6, primals_7, mm, gt, mul_2, sym_size_int)
	        
V0303 09:10:03.716660 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "cc027873d3695437445a5a9451185fda"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993003716594.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.724773 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "37b6eac23221e1a42c32f75cbdae249c"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993003724702.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.725161 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "0d681d7e64a51ff72bc9f7fb554e6e29"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993003725100.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.725339 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "ccfe1f6ab994e0b6b7bbe1a3231bea7f"}
	{
	"name": "code_gen",
	"ts": 1740993003725100.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.726117 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "3480d2de688529af061154d5854a3bf1"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993003726053.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.734272 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "e5aa95745fab96110fdce9da8647f105"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993003734222.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:03.734510 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "173e3d2fac18e46e89927a3481125f8a"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993003734450.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.007357 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "275675ce908baf9ce3781d87753684f8"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993004007092.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.007965 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "6e2b5c39b3e25c520496d325aaf6eaf8"}
	{
	"name": "code_gen",
	"ts": 1740993004007902.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.008538 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "d5cb57434cd4d6cc4df5a925f68f942b"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993004008137.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.009303 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "1ef79dfeaa3df5c6a35bcc42ed2e1ded"}
	{
	"name": "inductor_compile",
	"ts": 1740993004009249.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.009512 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "486296a62e27fed1374406f151a57566"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993004009469.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.010278 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "0e431ed3c028fd9ad3c37633589e248c"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993004010226.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.013332 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "cc8d3e252662872ad7fc71c3b0ff6b92"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993004013277.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.013772 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "5741333775f45ed3f3b3c6450d980903"}
	{
	"name": "backend_compile",
	"ts": 1740993004013723.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.013972 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "9a271b19352e027bef1de14f41986f8e"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993004013931.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.014816 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "b5b6b06036600bef745bad263a9bce00"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993004014755.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.015020 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0, "has_payload": "0665545296185578b2a8cc92496d2157"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993004014978.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 11,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.015362 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "11/0", "frame_key": "26", "co_name": "forward", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py", "co_firstlineno": 34, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740993003.4859083, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpr_0g_4zt/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpr_0g_4zt/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpr_0g_4zt', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.5293021202087402, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 11, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.142166 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "torch_dynamo_resume_in_forward_at_426", "filename": 4}]}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.142489 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "2c29315db637b1a3df139b1bd7685217"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993004142394.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.142674 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "2c8acfdde405c5d728d157a97a4c0616"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993004142394.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.145843 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 28, "size": 1966080}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.146148 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 28, "size": 1966080}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.146410 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1280, 384], "is_leaf": true, "stride": [384, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb930530b80>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.146659 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 28, "size": 2056}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.146888 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "2", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132450>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.147053 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 28, "size": 0}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.147268 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [5, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f411cef0>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.147432 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 28, "size": 0}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.147653 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [5, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f411c0e0>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.148267 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j2", 384], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["384*j2", 384, 1], "storage": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb930530c70>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.148437 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 28, "id": 4, "source": "L['___stack0']"}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.160474 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 5, "describer_id": 28, "size": 393216}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.160751 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 5, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, 384], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [384, 1], "storage": 5, "view_func": "<built-in method _view_func_unsafe of Parameter object at 0x7fba01ad5770>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.160915 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 28, "id": 5, "source": "L['self']._modules['out_proj']._parameters['weight']"}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.254114 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 6, "describer_id": 28, "size": 8192}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.254440 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 14, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "stride": [4, 1], "storage": 6, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b40e0>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.254673 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 13, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [256, 4], "is_leaf": true, "is_view": true, "stride": [4, 1], "storage": 6, "base": 14, "creation_meta": "CreationMeta.NO_GRAD_MODE", "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9fc4b4270>", "describer_id": 28}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.254853 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 28, "id": 13, "source": "L['batch'][2]"}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:04.292584 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_self_modules_out_proj_parameters_weight_": [256, 384], "l_batch_2_": [256, 4], "values": ["s1", 256], "rearrange": [256, "(s1//256)", 256], "getitem": [256, "(s1//256) - 1", 256], "logits": ["256*((s1//256)) - 256", 256], "target": [1024], "cross_entropy": [1024], "rearrange_1": [256, 4], "sum_1": [256], "loss": []}}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "65ef6e1f109e0e422706ec389faa0258"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_stack0_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_out_proj_parameters_weight_: "f32[256, 384][384, 1]cuda:0", L_batch_2_: "i64[256, 4][4, 1]cuda:0"):
	        l_stack0_ = L_stack0_
	        l_self_modules_out_proj_parameters_weight_ = L_self_modules_out_proj_parameters_weight_
	        l_batch_2_ = L_batch_2_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
	        predict_out: "f32[256, s0, 256][256*s0, 256, 1]cuda:0" = torch._C._nn.linear(l_stack0_, l_self_modules_out_proj_parameters_weight_, None);  l_stack0_ = l_self_modules_out_proj_parameters_weight_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/utils.py:67 in jagged_to_flattened_tensor, code: return x.values()
	        values: "f32[s1, 256][256, 1]cuda:0" = predict_out.values();  predict_out = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        rearrange: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = einops_einops_rearrange(values, '(b n) d -> b n d', b = 256);  values = None
	        getitem: "f32[256, (s1//256) - 1, 256][256*((s1//256)), 256, 1]cuda:0" = rearrange[(slice(None, None, None), slice(None, -1, None), slice(None, None, None))];  rearrange = None
	        logits: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = getitem.flatten(end_dim = 1);  getitem = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
	        target: "i64[1024][1]cuda:0" = l_batch_2_.flatten(end_dim = 1);  l_batch_2_ = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        cross_entropy: "f32[1024][1]cuda:0" = torch.nn.functional.cross_entropy(logits, target, reduction = 'none', ignore_index = -1);  target = None
	        rearrange_1: "f32[256, 4][4, 1]cuda:0" = einops_einops_rearrange(cross_entropy, '(b n) -> b n', b = 256);  cross_entropy = None
	        sum_1: "f32[256][1]cuda:0" = rearrange_1.sum(axis = 1);  rearrange_1 = None
	        loss: "f32[][]cuda:0" = sum_1.mean();  sum_1 = None
	        return (loss, logits)
	        
V0303 09:10:04.293058 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "731ba95f5cd7c39a328a4fe81dc4dc02"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993004292991.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.293240 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "72a69a8671fb798bf3f871292ee57893"}
	{
	"name": "backend_compile",
	"ts": 1740993004292991.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.302355 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "8f935fcbd722df3a7c8ec6389a5799de"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993004302296.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.559974 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "e0e307e0e8b3c88a461313a80d5a12f3"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[256, 384][384, 1]cuda:0"; primals_7: "i64[256, 4][4, 1]cuda:0"; tangents_1: "f32[][]cuda:0"; tangents_2: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
	        permute: "f32[384, 256][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm: "f32[s1, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/utils.py:67 in jagged_to_flattened_tensor, code: return x.values()
	        alias: "f32[s1, 256][256, 1]cuda:0" = torch.ops.aten.alias.default(mm);  mm = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        floordiv: "Sym((s1//256))" = sym_size_int // 256
	        view: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.view.default(alias, [256, floordiv, 256]);  alias = floordiv = None
	        sym_size_int_1: "Sym((s1//256))" = torch.ops.aten.sym_size.int(view, 1)
	        slice_1: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view, 0, 0, 9223372036854775807);  view = None
	        slice_2: "f32[256, (s1//256) - 1, 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, -1);  slice_1 = None
	        sym_size_int_2: "Sym((s1//256) - 1)" = torch.ops.aten.sym_size.int(slice_2, 1)
	        slice_3: "f32[256, (s1//256) - 1, 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(slice_2, 2, 0, 9223372036854775807);  slice_2 = None
	        mul_14: "Sym(256*((s1//256)) - 256)" = 256 * sym_size_int_2
	        clone: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.clone.default(slice_3, memory_format = torch.contiguous_format);  slice_3 = None
	        view_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone, [mul_14, 256]);  clone = mul_14 = None
	        sym_size_int_3: "Sym(256*((s1//256)) - 256)" = torch.ops.aten.sym_size.int(view_1, 0)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
	        view_2: "i64[1024][1]cuda:0" = torch.ops.aten.view.default(primals_7, [1024]);  primals_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        amax: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.amax.default(view_1, [1], True)
	        sub_6: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_1, amax);  amax = None
	        exp: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(sub_6)
	        sum_1: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
	        log: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.log.default(sum_1);  sum_1 = None
	        sub_7: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = log = None
	        alias_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.alias.default(sub_7)
	        alias_2: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
	        ne_9: "b8[1024][1]cuda:0" = torch.ops.aten.ne.Scalar(view_2, -1)
	        scalar_tensor: "i64[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
	        where: "i64[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, view_2, scalar_tensor);  ne_9 = scalar_tensor = None
	        unsqueeze: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(where, 1);  where = None
	        gather: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.gather.default(sub_7, 1, unsqueeze);  sub_7 = unsqueeze = None
	        squeeze: "f32[1024][1]cuda:0" = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
	        neg: "f32[1024][1]cuda:0" = torch.ops.aten.neg.default(squeeze);  squeeze = None
	        ne_10: "b8[1024][1]cuda:0" = torch.ops.aten.ne.Scalar(view_2, -1)
	        scalar_tensor_1: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_1: "f32[1024][1]cuda:0" = torch.ops.aten.where.self(ne_10, neg, scalar_tensor_1);  ne_10 = neg = scalar_tensor_1 = None
	        view_3: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.view.default(where_1, [256, 4]);  where_1 = None
	        sum_2: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_3, [1]);  view_3 = None
	        mean: "f32[][]cuda:0" = torch.ops.aten.mean.default(sum_2);  sum_2 = None
	        expand: "f32[256][0]cuda:0" = torch.ops.aten.expand.default(tangents_1, [256]);  tangents_1 = None
	        div: "f32[256][1]cuda:0" = torch.ops.aten.div.Scalar(expand, 256);  expand = None
	        unsqueeze_1: "f32[256, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(div, 1);  div = None
	        expand_1: "f32[256, 4][1, 0]cuda:0" = torch.ops.aten.expand.default(unsqueeze_1, [256, 4]);  unsqueeze_1 = None
	        clone_1: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None
	        view_4: "f32[1024][1]cuda:0" = torch.ops.aten.view.default(clone_1, [1024]);  clone_1 = None
	        unsqueeze_2: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(view_2, 1);  view_2 = None
	        ne_11: "b8[1024, 1][1, 1]cuda:0" = torch.ops.aten.ne.Scalar(unsqueeze_2, -1)
	        scalar_tensor_2: "i64[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
	        where_2: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.where.self(ne_11, unsqueeze_2, scalar_tensor_2);  ne_11 = scalar_tensor_2 = None
	        full_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_3 = None
	        scatter: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.scatter.value(full_1, 1, where_2, -1.0);  full_1 = where_2 = None
	        unsqueeze_3: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(view_4, 1);  view_4 = None
	        ne_12: "b8[1024, 1][1, 1]cuda:0" = torch.ops.aten.ne.Scalar(unsqueeze_2, -1);  unsqueeze_2 = None
	        scalar_tensor_3: "f32[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
	        where_3: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.where.self(ne_12, unsqueeze_3, scalar_tensor_3);  ne_12 = unsqueeze_3 = scalar_tensor_3 = None
	        mul_40: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
	        alias_3: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
	        alias_4: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.alias.default(alias_3);  alias_3 = None
	        exp_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(alias_4);  alias_4 = None
	        sum_3: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_40, [1], True)
	        mul_41: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(exp_1, sum_3);  exp_1 = sum_3 = None
	        sub_10: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_40, mul_41);  mul_40 = mul_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        add_29: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_2, sub_10);  tangents_2 = sub_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        view_5: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.view.default(add_29, [256, sym_size_int_2, 256]);  add_29 = None
	        full_2: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.full.default([256, sym_size_int_2, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_2 = None
	        slice_scatter: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_2, view_5, 2, 0, 9223372036854775807);  full_2 = view_5 = None
	        full_3: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.full.default([256, sym_size_int_1, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        slice_scatter_1: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, slice_scatter, 1, 0, -1);  full_3 = slice_scatter = None
	        full_4: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.full.default([256, sym_size_int_1, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_1 = None
	        slice_scatter_2: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_4, slice_scatter_1, 0, 0, 9223372036854775807);  full_4 = slice_scatter_1 = None
	        view_6: "f32[256*((s1//256)), 256][256, 1]cuda:0" = torch.ops.aten.view.default(slice_scatter_2, [sym_size_int, 256]);  slice_scatter_2 = sym_size_int = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
	        mm_1: "f32[256*((s1//256)), 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_6, primals_6);  primals_6 = None
	        permute_1: "f32[256, 256*((s1//256))][1, 256]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0]);  view_6 = None
	        mm_2: "f32[256, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_1, primals_1);  permute_1 = primals_1 = None
	        return pytree.tree_unflatten([mean, view_1, mm_1, primals_2, primals_3, primals_4, None, mm_2, None], self._out_spec)
	        
V0303 09:10:04.605401 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "c4eac957da748ba54a79687824c6c726"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[256, 384][384, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
	        permute: "f32[384, 256][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm: "f32[s1, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        floordiv: "Sym((s1//256))" = sym_size_int // 256
	        view: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.view.default(mm, [256, floordiv, 256]);  mm = floordiv = None
	        sym_size_int_1: "Sym((s1//256))" = torch.ops.aten.sym_size.int(view, 1)
	        slice_2: "f32[256, (s1//256) - 1, 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view, 1, 0, -1);  view = None
	        sym_size_int_2: "Sym((s1//256) - 1)" = torch.ops.aten.sym_size.int(slice_2, 1)
	        mul_14: "Sym(256*((s1//256)) - 256)" = 256 * sym_size_int_2
	        clone: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.clone.default(slice_2, memory_format = torch.contiguous_format);  slice_2 = None
	        view_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone, [mul_14, 256]);  clone = mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
	        view_2: "i64[1024][1]cuda:0" = torch.ops.aten.view.default(primals_7, [1024])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        amax: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.amax.default(view_1, [1], True)
	        sub_6: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_1, amax)
	        exp: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(sub_6)
	        sum_1: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
	        log: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.log.default(sum_1);  sum_1 = None
	        sub_7: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = None
	        ne_9: "b8[1024][1]cuda:0" = torch.ops.aten.ne.Scalar(view_2, -1)
	        full_default: "i64[][]cuda:0" = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where: "i64[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, view_2, full_default);  view_2 = full_default = None
	        unsqueeze: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(where, 1);  where = None
	        gather: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.gather.default(sub_7, 1, unsqueeze);  sub_7 = unsqueeze = None
	        squeeze: "f32[1024][1]cuda:0" = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
	        neg: "f32[1024][1]cuda:0" = torch.ops.aten.neg.default(squeeze);  squeeze = None
	        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where_1: "f32[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, neg, full_default_1);  ne_9 = neg = full_default_1 = None
	        view_3: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.view.default(where_1, [256, 4]);  where_1 = None
	        sum_2: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_3, [1]);  view_3 = None
	        mean: "f32[][]cuda:0" = torch.ops.aten.mean.default(sum_2);  sum_2 = None
	        return (mean, view_1, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, view_1, amax, log, sym_size_int, sym_size_int_1, sym_size_int_2)
	        
V0303 09:10:04.610207 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "4c5a9b2bc25b1a0abfbc15c5c31203cd"}
	class GraphModule(torch.nn.Module):
	    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym((s1//256))", sym_size_int_2: "Sym((s1//256) - 1)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_6: "f32[256, 384][384, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0", view_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0", amax: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0", log: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0", tangents_1: "f32[][]cuda:0", tangents_2: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        expand: "f32[256][0]cuda:0" = torch.ops.aten.expand.default(tangents_1, [256]);  tangents_1 = None
	        div: "f32[256][1]cuda:0" = torch.ops.aten.div.Scalar(expand, 256);  expand = None
	        unsqueeze_1: "f32[256, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(div, 1);  div = None
	        expand_1: "f32[256, 4][1, 0]cuda:0" = torch.ops.aten.expand.default(unsqueeze_1, [256, 4]);  unsqueeze_1 = None
	        clone_1: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None
	        view_4: "f32[1024][1]cuda:0" = torch.ops.aten.view.default(clone_1, [1024]);  clone_1 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
	        view_2: "i64[1024][1]cuda:0" = torch.ops.aten.view.default(primals_7, [1024]);  primals_7 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        unsqueeze_2: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(view_2, 1);  view_2 = None
	        ne_11: "b8[1024, 1][1, 1]cuda:0" = torch.ops.aten.ne.Scalar(unsqueeze_2, -1)
	        full_default: "i64[][]cuda:0" = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where_2: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.where.self(ne_11, unsqueeze_2, full_default);  unsqueeze_2 = full_default = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        sym_size_int_3: "Sym(256*((s1//256)) - 256)" = torch.ops.aten.sym_size.int(view_1, 0)
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        full_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_3 = None
	        scatter: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.scatter.value(full_1, 1, where_2, -1.0);  full_1 = where_2 = None
	        unsqueeze_3: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(view_4, 1);  view_4 = None
	        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where_3: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.where.self(ne_11, unsqueeze_3, full_default_1);  ne_11 = unsqueeze_3 = full_default_1 = None
	        mul_40: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
	        sub_6: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_1, amax);  view_1 = amax = None
	        sub_7: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = log = None
	        exp_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(sub_7);  sub_7 = None
	        sum_3: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_40, [1], True)
	        mul_41: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(exp_1, sum_3);  exp_1 = sum_3 = None
	        sub_10: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_40, mul_41);  mul_40 = mul_41 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        add_29: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_2, sub_10);  tangents_2 = sub_10 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        view_5: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.view.default(add_29, [256, sym_size_int_2, 256]);  add_29 = sym_size_int_2 = None
	        full_3: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.full.default([256, sym_size_int_1, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_1 = None
	        slice_scatter_1: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, view_5, 1, 0, -1);  full_3 = view_5 = None
	        view_6: "f32[256*((s1//256)), 256][256, 1]cuda:0" = torch.ops.aten.view.default(slice_scatter_1, [sym_size_int, 256]);  slice_scatter_1 = sym_size_int = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
	        mm_1: "f32[256*((s1//256)), 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_6, primals_6);  primals_6 = None
	        permute_1: "f32[256, 256*((s1//256))][1, 256]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0]);  view_6 = None
	        mm_2: "f32[256, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_1, primals_1);  permute_1 = primals_1 = None
	        return (mm_1, primals_2, primals_3, primals_4, None, mm_2, None)
	        
V0303 09:10:04.610629 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "ec5669ddabe19925fcb461837de11c9d"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993004610418.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.611766 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "927fa92003d19f0e21bd993d09eb99ea"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993004610873.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.611960 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "451e6017689831159c616bdb0119ed19"}
	{
	"name": "inductor_compile",
	"ts": 1740993004610873.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.621051 12578 torch/_inductor/compile_fx.py:742] {"artifact": {"name": "fx_graph_runnable", "encoding": "string"}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "e570ea2f134816005c532620bf70dc32"}
	
	import torch
	from torch import tensor, device
	import torch.fx as fx
	from torch._dynamo.testing import rand_strided
	from math import inf
	import torch._inductor.inductor_prims
	
	import torch._dynamo.config
	import torch._inductor.config
	import torch._functorch.config
	import torch.fx.experimental._config
	torch._dynamo.config.suppress_errors = True
	
	torch._functorch.config.unlift_effect_tokens = True
	
	
	
	isolate_fails_code_str = None
	
	
	
	# torch version: 2.5.1+cu124
	# torch cuda version: 12.4
	# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
	
	
	# CUDA Info: 
	# nvcc: NVIDIA (R) Cuda compiler driver 
	# Copyright (c) 2005-2024 NVIDIA Corporation 
	# Built on Thu_Mar_28_02:18:24_PDT_2024 
	# Cuda compilation tools, release 12.4, V12.4.131 
	# Build cuda_12.4.r12.4/compiler.34097967_0 
	
	# GPU Hardware Info: 
	# NVIDIA L4 : 1 
	
	
	from torch.nn import *
	class Repro(torch.nn.Module):
	    def __init__(self) -> None:
	        super().__init__()
	
	    
	    
	    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7):
	        permute = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
	        floordiv = sym_size_int // 256
	        view = torch.ops.aten.view.default(mm, [256, floordiv, 256]);  mm = floordiv = None
	        sym_size_int_1 = torch.ops.aten.sym_size.int(view, 1)
	        slice_2 = torch.ops.aten.slice.Tensor(view, 1, 0, -1);  view = None
	        sym_size_int_2 = torch.ops.aten.sym_size.int(slice_2, 1)
	        mul_14 = 256 * sym_size_int_2
	        clone = torch.ops.aten.clone.default(slice_2, memory_format = torch.contiguous_format);  slice_2 = None
	        view_1 = torch.ops.aten.view.default(clone, [mul_14, 256]);  clone = mul_14 = None
	        view_2 = torch.ops.aten.view.default(primals_7, [1024])
	        amax = torch.ops.aten.amax.default(view_1, [1], True)
	        sub_6 = torch.ops.aten.sub.Tensor(view_1, amax)
	        exp = torch.ops.aten.exp.default(sub_6)
	        sum_1 = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
	        log = torch.ops.aten.log.default(sum_1);  sum_1 = None
	        sub_7 = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = None
	        ne_9 = torch.ops.aten.ne.Scalar(view_2, -1)
	        full_default = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where = torch.ops.aten.where.self(ne_9, view_2, full_default);  view_2 = full_default = None
	        unsqueeze = torch.ops.aten.unsqueeze.default(where, 1);  where = None
	        gather = torch.ops.aten.gather.default(sub_7, 1, unsqueeze);  sub_7 = unsqueeze = None
	        squeeze = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
	        neg = torch.ops.aten.neg.default(squeeze);  squeeze = None
	        full_default_1 = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where_1 = torch.ops.aten.where.self(ne_9, neg, full_default_1);  ne_9 = neg = full_default_1 = None
	        view_3 = torch.ops.aten.view.default(where_1, [256, 4]);  where_1 = None
	        sum_2 = torch.ops.aten.sum.dim_IntList(view_3, [1]);  view_3 = None
	        mean = torch.ops.aten.mean.default(sum_2);  sum_2 = None
	        return (mean, view_1, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, view_1, amax, log, sym_size_int, sym_size_int_1, sym_size_int_2)
	        
	def load_args(reader):
	    buf0 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
	    reader.tensor(buf0, (s1, 384), is_leaf=True)  # primals_1
	    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
	    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
	    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
	    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
	    reader.symint(j2)  # primals_5
	    buf4 = reader.storage(None, 393216, device=device(type='cuda', index=0))
	    reader.tensor(buf4, (256, 384), is_leaf=True)  # primals_6
	    buf5 = reader.storage(None, 8192, device=device(type='cuda', index=0), dtype_hint=torch.int64)
	    reader.tensor(buf5, (256, 4), dtype=torch.int64, is_leaf=True)  # primals_7
	load_args._version = 0
	mod = Repro()
	if __name__ == '__main__':
	    from torch._dynamo.repro.after_aot import run_repro
	    with torch.no_grad():
	        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # To run it separately, do 
	        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
	        # mod(*args)
V0303 09:10:04.645832 12578 torch/_inductor/compile_fx.py:801] {"inductor_post_grad_graph": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "16fd84f769da0259801c88379b489948"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[256, 384][384, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0"):
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
	        permute: "f32[384, 256][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
	        mm: "f32[s1, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
	        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
	        floordiv: "Sym((s1//256))" = sym_size_int // 256
	        view: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.reshape.default(mm, [256, floordiv, 256]);  mm = floordiv = None
	        sym_size_int_1: "Sym((s1//256))" = torch.ops.aten.sym_size.int(view, 1)
	        slice_2: "f32[256, (s1//256) - 1, 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view, 1, 0, -1);  view = None
	        sym_size_int_2: "Sym((s1//256) - 1)" = torch.ops.aten.sym_size.int(slice_2, 1)
	        mul_14: "Sym(256*((s1//256)) - 256)" = 256 * sym_size_int_2
	        clone: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.clone.default(slice_2, memory_format = torch.contiguous_format);  slice_2 = None
	        view_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.reshape.default(clone, [mul_14, 256]);  clone = mul_14 = None
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
	        view_2: "i64[1024][1]cuda:0" = torch.ops.aten.reshape.default(primals_7, [1024])
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
	        amax: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.amax.default(view_1, [1], True)
	        sub_6: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_1, amax)
	        exp: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(sub_6)
	        sum_1: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
	        log: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.log.default(sum_1);  sum_1 = None
	        sub_7: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = None
	        ne_9: "b8[1024][1]cuda:0" = torch.ops.aten.ne.Scalar(view_2, -1)
	        full_default: "i64[][]cuda:0" = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where: "i64[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, view_2, full_default);  view_2 = full_default = None
	        unsqueeze: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(where, 1);  where = None
	        gather: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.gather.default(sub_7, 1, unsqueeze);  sub_7 = unsqueeze = None
	        squeeze: "f32[1024][1]cuda:0" = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
	        neg: "f32[1024][1]cuda:0" = torch.ops.aten.neg.default(squeeze);  squeeze = None
	        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
	        where_1: "f32[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, neg, full_default_1);  ne_9 = neg = full_default_1 = None
	        view_3: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.reshape.default(where_1, [256, 4]);  where_1 = None
	        sum_2: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_3, [1]);  view_3 = None
	        mean: "f32[][]cuda:0" = torch.ops.aten.mean.default(sum_2);  sum_2 = None
	        return (mean, view_1, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, view_1, amax, log, sym_size_int, sym_size_int_1, sym_size_int_2)
	        
V0303 09:10:04.646536 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "0dd1b7cd562333ec631d4e211dc00c20"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993004646470.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.736772 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "5dfdb9a47bfbb7bcbbc316ec15ef7f94"}
	{
	"name": "GraphLowering.run",
	"ts": 1740993004736715.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.737171 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "dc2d7a633879e1f613aafadac8b1707d"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993004737109.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.737347 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "0abc310bab1fd8c06746d8ba98411276"}
	{
	"name": "code_gen",
	"ts": 1740993004737109.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.738121 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "a1997d9b916d35db5465243b0f1b961c"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993004738059.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.952140 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "8d40e8c8ce065ab6ab67ba349e7d0800"}
	{
	"name": "Scheduler.__init__",
	"ts": 1740993004952079.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:04.952409 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "3b3a09444b67755ffc84c6714f4ef6e8"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993004952343.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.181989 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "48f4317a649bb56747b448c1a9519442"}
	{
	"name": "Scheduler.codegen",
	"ts": 1740993005181681.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.182596 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "1c441ac3e50c5228751927ef080ccadf"}
	{
	"name": "code_gen",
	"ts": 1740993005182533.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.182812 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "4bfae110ddd0bf82840fb503ccb5ef39"}
	{
	"name": "GraphLowering.compile_to_module",
	"ts": 1740993005182767.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.183586 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "7db66954bef3e869d1e5af34d74b998a"}
	{
	"name": "inductor_compile",
	"ts": 1740993005183532.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.183789 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "49fc702d7b2a43049ec33f9fd86cb8a4"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005183748.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.184539 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "bfa62adc4238e6a763392be4e4e0db86"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005184490.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.187921 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "4bfcb85e6f3b8c24438f53fc08dd8919"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005187867.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.188495 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "b2aaf990a8cbf9c7cdabba4547385a7a"}
	{
	"name": "backend_compile",
	"ts": 1740993005188445.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.188692 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "10d64c131360a7182232481821a6bbf1"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005188651.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.189464 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "cbb3730e95aa9e4d92afc60309f291c7"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005189415.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.189655 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0, "has_payload": "7ad18d92bf2db6721c38c9d45a9b8990"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005189616.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.190010 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "12/0", "frame_key": "27", "co_name": "torch_dynamo_resume_in_forward_at_426", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/model.py", "co_firstlineno": 426, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": null, "shape_env_guard_count": null, "graph_op_count": null, "graph_node_count": null, "graph_input_count": null, "start_time": 1740993004.1423833, "entire_frame_compile_time_s": null, "backend_compile_time_s": null, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": "BackendCompilerFailed", "fail_reason": "backend='inductor' raised:\nCalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmpoea99y4v/main.c', '-O3', '-shared', '-fPIC', '-o', '/tmp/tmpoea99y4v/cuda_utils.cpython-39-x86_64-linux-gnu.so', '-lcuda', '-L/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-L/lib', '-I/home/ec2-user/.local/lib/python3.9/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpoea99y4v', '-I/usr/include/python3.9']' returned non-zero exit status 1.", "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 1.0474727153778076, "has_guarded_code": false, "possibly_missed_reinplacing_opportunities": null}, "frame_id": 12, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.195181 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/code/RQ-VAE-Recommender/modules/utils.py", 26]}
V0303 09:10:05.195404 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 66, "name": "jagged_to_flattened_tensor", "filename": 26}]}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.195706 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "d886111feb981d2bedd7710248868f22"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005195604.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.195926 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "fc7468be2a63840abe893f74cc0fad73"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005195604.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.199148 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 30, "size": 1310720}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.199532 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 1, "describer_id": 30, "size": 1310720}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.199872 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1280, 256], "is_leaf": true, "stride": [256, 1], "storage": 1, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb93845f860>", "describer_id": 30}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.200094 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 2, "describer_id": 30, "size": 2056}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.200389 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 1, "ndim": 1, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [257], "is_leaf": true, "nested_int": "2", "stride": [1], "storage": 2, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f4132450>", "describer_id": 30}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.200630 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 3, "describer_id": 30, "size": 0}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.200953 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 2, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [5, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 3, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f411cef0>", "describer_id": 30}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.201205 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 4, "describer_id": 30, "size": 0}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.201528 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cpu')", "size": [5, 0], "dynamo_dynamic_indices": [0], "is_leaf": true, "stride": [1, 1], "storage": 4, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9f411c0e0>", "describer_id": 30}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.202411 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 4, "ndim": 3, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [256, "j2", 256], "layout": "torch.jagged", "requires_grad": true, "is_nested": true, "is_traceable_wrapper_subclass": true, "stride": ["256*j2", 256, 1], "storage": 0, "attrs": {"_values": 0, "_offsets": 1, "_min_seqlen_tensor": 2, "_max_seqlen_tensor": 3}, "ctx": "{'requires_grad': True, 'ragged_idx': 1}", "type": "<class 'torch.nested._internal.nested_tensor.NestedTensor'>", "view_func": "<built-in method _view_func_unsafe of NestedTensor object at 0x7fb9305654a0>", "describer_id": 30}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.202641 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 30, "id": 4, "source": "L['x']"}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.220704 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"values": ["s1", 256]}}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "e44715a9c688a920c7477a23def1daa1"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_x_: "f32[256, s0, 256][256*s0, 256, 1]cuda:0", s0: "Sym(s0)"):
	        l_x_ = L_x_
	        
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/utils.py:67 in jagged_to_flattened_tensor, code: return x.values()
	        values: "f32[s1, 256][256, 1]cuda:0" = l_x_.values();  l_x_ = None
	        return (values,)
	        
V0303 09:10:05.221165 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "5b2220fc46b2fe8940431b2f07b7a179"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005221096.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.221343 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "bd4368eccc5ccbcc09071dd33130ede6"}
	{
	"name": "backend_compile",
	"ts": 1740993005221096.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.227657 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "dcc2b2c95e32f7542ba5d91f562e8409"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005227600.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.263113 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352] {"aot_joint_graph": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "3abbd1a681c065e3b2b41340b504ef52"}
	class joint_fn(torch.nn.Module):
	    def forward(self, primals, tangents):
	        primals_1: "f32[s1, 256][256, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; tangents_1: "f32[s1, 256][256, 1]cuda:0"; 
	    
	        primals_1, primals_2, primals_3, primals_4, primals_5, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
	         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/utils.py:67 in jagged_to_flattened_tensor, code: return x.values()
	        alias: "f32[s1, 256][256, 1]cuda:0" = torch.ops.aten.alias.default(primals_1);  primals_1 = None
	        return pytree.tree_unflatten([alias, tangents_1, primals_2, primals_3, primals_4, None], self._out_spec)
	        
V0303 09:10:05.268131 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:545] {"aot_forward_graph": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "8d0fd7ca945257b500dd44a1acee4d2e"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_1: "f32[s1, 256][256, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)"):
	        return (primals_1, primals_2, primals_3, primals_4)
	        
V0303 09:10:05.268553 12578 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:551] {"aot_backward_graph": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "3a22490f72ee660e99112e6c4cdb88d8"}
	class GraphModule(torch.nn.Module):
	    def forward(self, primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", tangents_1: "f32[s1, 256][256, 1]cuda:0"):
	        return (tangents_1, primals_2, primals_3, primals_4, None)
	        
V0303 09:10:05.268941 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "f482a65a317f43bde64403a2d5d63f44"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005268735.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.269249 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "635b6811912ea5364d7b46bb89fb8fa3"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005269185.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.269457 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "87312e28c21408377e9c2e30ca88165b"}
	{
	"name": "inductor_compile",
	"ts": 1740993005269185.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.270217 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "fde6005882e140037b54a668e46bea55"}
	{
	"name": "inductor_compile",
	"ts": 1740993005270169.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.270412 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "50418df76c1ebf093ae3c8aeeab81368"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005270371.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.270789 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "5be95b97a58603e2cc6a33f115aa3b1c"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005270743.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.273067 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "1953bb5fd959dd1a1baad60f2f17fecb"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005273018.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.273390 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "5acb4268634692fbb9669c68c8fe9195"}
	{
	"name": "backend_compile",
	"ts": 1740993005273345.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.273577 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "14fc043b1106a3ffe82527c7bd78b752"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005273538.5,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.282424 12578 torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "804a9bc5e091578071ba3f46fbab6973"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)
	| | +- TYPE_MATCH: ___check_type_id(L['x'], 94485813910080)                    
	| | +- LAMBDA_GUARD: ___check_metadata_140433249317504_c13/0                     
	| | +- TENSOR_MATCH: check_tensor(L['x'], NestedTensor, DispatchKeySet(CUDA, NestedTensorCUDA, BackendSelect, Python, ADInplaceOrView, AutogradCUDA, AutogradNestedTensor, PythonTLSSnapshot), torch.float32, device=0, requires_grad=True, size=[256, None, 256], stride=[None, 256, 1])
	| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False         
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['x'], L['x']._values, L['x']._offsets, L['x']._max_seqlen_tensor, L['x']._min_seqlen_tensor)
	| | +- GuardManager: source=L['x']._values, accessed_by=GetAttrGuardAccessor(_values)
	| | | +- TENSOR_MATCH: check_tensor(L['x']._values, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[None, 256], stride=[256, 1])
	| | | +- NO_HASATTR: hasattr(L['x']._values, '_dynamo_dynamic_indices') == False 
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['x']._offsets, accessed_by=GetAttrGuardAccessor(_offsets)
	| | | +- TENSOR_MATCH: check_tensor(L['x']._offsets, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.int64, device=0, requires_grad=False, size=[257], stride=[1])
	| | | +- NO_HASATTR: hasattr(L['x']._offsets, '_dynamo_dynamic_indices') == False
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['x']._max_seqlen_tensor, accessed_by=GetAttrGuardAccessor(_max_seqlen_tensor)
	| | | +- TENSOR_MATCH: check_tensor(L['x']._max_seqlen_tensor, Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[None, 0], stride=[1, 1])
	| | | +- DYNAMIC_INDICES: ((L['x']._max_seqlen_tensor._dynamo_dynamic_indices.issubset({0})) if hasattr(L['x']._max_seqlen_tensor, '_dynamo_dynamic_indices') else True)
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['x']._min_seqlen_tensor, accessed_by=GetAttrGuardAccessor(_min_seqlen_tensor)
	| | | +- TENSOR_MATCH: check_tensor(L['x']._min_seqlen_tensor, Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[None, 0], stride=[1, 1])
	| | | +- DYNAMIC_INDICES: ((L['x']._min_seqlen_tensor._dynamo_dynamic_indices.issubset({0})) if hasattr(L['x']._min_seqlen_tensor, '_dynamo_dynamic_indices') else True)
	| | | +- NO_TENSOR_ALIASING
	| | +- GuardManager: source=L['x'].__tensor_flatten__()[0], accessed_by=PythonLambdaGuardAccessor
	| | | +- EQUALS_MATCH: L['x'].__tensor_flatten__()[0] == ['_values', '_offsets', '_min_seqlen_tensor', '_max_seqlen_tensor']
	+- LAMBDA_GUARD: L['x'].stride()[0] == 256*L['x'].size()[1]                    # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x']._values.size()[0]                                 # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x']._min_seqlen_tensor.size()[0]                      # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 2 <= L['x']._max_seqlen_tensor.size()[0]                      # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0303 09:10:05.282907 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "be3f8df75311807bdd3fb3697de26197"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005282860.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.283098 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "500fd61265696be0cfeed887a41e1d07"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005283059.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 0,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.283343 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "13/0", "frame_key": "28", "co_name": "jagged_to_flattened_tensor", "co_filename": "/home/ec2-user/code/RQ-VAE-Recommender/modules/utils.py", "co_firstlineno": 66, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 13, "shape_env_guard_count": 0, "graph_op_count": 1, "graph_node_count": 4, "graph_input_count": 2, "start_time": 1740993005.1955922, "entire_frame_compile_time_s": 0.0871741771697998, "backend_compile_time_s": 0.05219674110412598, "inductor_compile_time_s": 0.0009164810180664062, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.284109 12578 torch/_logging/structured.py:22] {"str": ["/home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py", 27]}
V0303 09:10:05.284268 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 536, "name": "rearrange", "filename": 27}]}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.284464 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "f6a1ac5d3ae21d46a80be4c35e58271d"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005284409.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.284629 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "4e46b269e8975b36eade1b4501907506"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005284409.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.286367 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 32, "size": 1310720}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.286631 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1280, 256], "requires_grad": true, "stride": [256, 1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb93060f720>", "describer_id": 32}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.286778 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 32, "id": 0, "source": "L['tensor']"}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.314676 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_tensor_": ["s0", 256], "reduce": [256, "(s0//256)", 256]}}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "6b83318d3b26e27c736ff3e1a3307f73"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_tensor_: "f32[s0, 256][256, 1]cuda:0"):
	        l_tensor_ = L_tensor_
	        
	         # File: /home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py:591 in rearrange, code: return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
	        reduce: "f32[256, (s0//256), 256][256*((s0//256)), 256, 1]cuda:0" = einops_einops_reduce(l_tensor_, '(b n) d -> b n d', reduction = 'rearrange', b = 256);  l_tensor_ = None
	        return (reduce,)
	        
V0303 09:10:05.315122 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "7dc043f03cac8786cafaa1fa104a5763"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005315062.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.315301 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "9b818f342d0e20fb3a737f9dfbbedc7b"}
	{
	"name": "backend_compile",
	"ts": 1740993005315062.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.317139 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "df7bb371782d17105bc0faeccaea4915"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005317080.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.339031 12578 torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "f2dc881dacde1c9c9cad4db7cfb1ebe8"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s0)", arg1_1: "f32[s0, 256][256, 1]cuda:0"):
	         # File: /home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py:591 in rearrange, code: return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
	        floordiv: "Sym((s0//256))" = arg0_1 // 256;  arg0_1 = None
	        view: "f32[256, (s0//256), 256][256*((s0//256)), 256, 1]cuda:0" = torch.ops.aten.view.default(arg1_1, [256, floordiv, 256]);  arg1_1 = floordiv = None
	        return (view,)
	        
V0303 09:10:05.339422 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "3b967ed78b3d704ed9a2ee00e19e1589"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005339230.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.341335 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "aabe61d364563ce4b7a1bd7273065c0c"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005341271.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.341516 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "191c74b1b40b3c743d609ead3acde924"}
	{
	"name": "inductor_compile",
	"ts": 1740993005341271.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.353372 12578 torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_ec2-user/um/cumshwfhfvlku5su2xo37dd6cjr2lmjlif4buluw63uu6fkt2s4i.py"}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "d02c00f1b7c27de99965583c47c5c29d"}
	# AOT ID: ['13_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1 = args
	    args.clear()
	    s0 = arg0_1
	    assert_size_stride(arg1_1, (s0, 256), (256, 1))
	    return (reinterpret_tensor(arg1_1, (256, (s0 // 256), 256), (256*(s0 // 256), 256, 1), 0), )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 1280
	    arg1_1 = rand_strided((1280, 256), (256, 1), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([arg0_1, arg1_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0303 09:10:05.353755 12578 torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "40a2d658261bbd87e9ad8dcc1e330875"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1740993005353551.2,
	"args": {
	"key": "f6savurhdgzvw7thfcvn2pdmu3i7dfeyqmftigfzktywhajql5il",
	"components": [
	"[lxtwf4eenft4vycy4pglr6qpzmy7iyzqklckpld2f5hfa5kxknc] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    floordiv = arg0_1 // 256;  arg0_1 = None\n    view = torch.ops.aten.view.default(arg1_1, [256, floordiv, 256]);  arg1_1 = floordiv = None\n    return (view,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[dv53yapbdkm2bjys4rqgpvewhfpe44w6p7jxlcfac3k5lix2jus] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s0, 256]), stride=(256, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[shairiav2uczfvaju735p26onlcujumrymj663kroiuyzs6i55u] fx_kwargs[user_visible_outputs]: {'view': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[7as26aeta7rzhgm2mxh4el36kupf55fr27327kzc2fsdiy3nexy] cuda_matmul_settings: (True, True, True)",
	"[xwojfm2wgtzvv6osq2suz26ozpu7itordb6qnfesrcd6rhlb5jc] torch_version: <bytes>",
	"[us57dzw33utjij5kbyy6wyd2by7wknkpqqzhfk4aav6qz5t74bl] system_info[device]: {'name': 'NVIDIA L4'}",
	"[kkjugxdvgihrm5q2qsprwyxsgxutzunecpz57gaifglfjwiz2nr] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-357ddbf0295262789c6e9d4dedcc296082a0374db4d46e4bfd9e033c0eebfefb-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[lx5lhxotgs6siijuq2slk6w4phtynlj37dhglrk4es64ctg6r4h] system_info[hash]: 6fcc6fdc24b0f3f99e8a929624734d67627e680b3fb39d1060ab79bebd873583",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 24701413,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0303 09:10:05.354111 12578 torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "1902c9e9332125095e56012382c7d2e6"}
	{"key": "f6savurhdgzvw7thfcvn2pdmu3i7dfeyqmftigfzktywhajql5il", "components": ["[lxtwf4eenft4vycy4pglr6qpzmy7iyzqklckpld2f5hfa5kxknc] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    floordiv = arg0_1 // 256;  arg0_1 = None\n    view = torch.ops.aten.view.default(arg1_1, [256, floordiv, 256]);  arg1_1 = floordiv = None\n    return (view,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[dv53yapbdkm2bjys4rqgpvewhfpe44w6p7jxlcfac3k5lix2jus] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s0, 256]), stride=(256, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[shairiav2uczfvaju735p26onlcujumrymj663kroiuyzs6i55u] fx_kwargs[user_visible_outputs]: {'view': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[7as26aeta7rzhgm2mxh4el36kupf55fr27327kzc2fsdiy3nexy] cuda_matmul_settings: (True, True, True)", "[xwojfm2wgtzvv6osq2suz26ozpu7itordb6qnfesrcd6rhlb5jc] torch_version: <bytes>", "[us57dzw33utjij5kbyy6wyd2by7wknkpqqzhfk4aav6qz5t74bl] system_info[device]: {'name': 'NVIDIA L4'}", "[kkjugxdvgihrm5q2qsprwyxsgxutzunecpz57gaifglfjwiz2nr] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-357ddbf0295262789c6e9d4dedcc296082a0374db4d46e4bfd9e033c0eebfefb-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[lx5lhxotgs6siijuq2slk6w4phtynlj37dhglrk4es64ctg6r4h] system_info[hash]: 6fcc6fdc24b0f3f99e8a929624734d67627e680b3fb39d1060ab79bebd873583", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 24701413, "cache_state": "hit"}
V0303 09:10:05.354498 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "fc0b395587dfe6cc0d464278da20d83b"}
	{
	"name": "inductor_compile",
	"ts": 1740993005354451.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.354682 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "0d1cc962daeaf2b91f1140d390ae27df"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005354644.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.355108 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "fef90590af3c35eceb1a39c1e6b5a63a"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005355061.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.357290 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "dd6678a2446009a4e557209d0a2965b9"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005357240.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.357587 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "68da4c42f98871d19f41455ec024b903"}
	{
	"name": "backend_compile",
	"ts": 1740993005357543.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.357809 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "b831cea0e45413ba0fa9a3b65250a669"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005357766.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.377299 12578 torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "e4e731c070aa591eda3e44b9c764c7e9"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['tensor'], accessed_by=DictGetItemGuardAccessor(tensor)
	| | +- TENSOR_MATCH: check_tensor(L['tensor'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=True, size=[None, 256], stride=[256, 1])
	| | +- NO_HASATTR: hasattr(L['tensor'], '_dynamo_dynamic_indices') == False    
	| +- GuardManager: source=L['pattern'], accessed_by=DictGetItemGuardAccessor(pattern)
	| | +- EQUALS_MATCH: L['pattern'] == '(b n) d -> b n d'                          
	| +- GuardManager: source=L['axes_lengths'], accessed_by=DictGetItemGuardAccessor(axes_lengths)
	| | +- DICT_LENGTH: len(L['axes_lengths']) == 1                                 
	| | +- GuardManager: source=L['axes_lengths']['b'], accessed_by=DictGetItemGuardAccessor(b)
	| | | +- EQUALS_MATCH: L['axes_lengths']['b'] == 256                               
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['reduce'], accessed_by=DictGetItemGuardAccessor(reduce)
	| | | +- ID_MATCH: ___check_obj_id(G['reduce'], 140438156473248)               
	+- LAMBDA_GUARD: Eq(256*L['tensor'].size()[0], 65536*((L['tensor'].size()[0]//256)))  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne(L['tensor'].size()[0], 256)                                # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Eq(Mod(L['tensor'].size()[0], 256), 0)                        # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne((L['tensor'].size()[0]//256), 1)                           # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 256 <= L['tensor'].size()[0]                                  # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0303 09:10:05.377578 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "fd0b58760cd0107db11ff6dba7a263b2"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005377532.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.377763 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0, "has_payload": "c4e33b349ec66ef154e92676c739a4ed"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005377725.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 1,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.378011 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "14/0", "frame_key": "29", "co_name": "rearrange", "co_filename": "/home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py", "co_firstlineno": 536, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 11, "shape_env_guard_count": 8, "graph_op_count": 1, "graph_node_count": 4, "graph_input_count": 2, "start_time": 1740993005.2844026, "entire_frame_compile_time_s": 0.09307599067687988, "backend_compile_time_s": 0.042433977127075195, "inductor_compile_time_s": 0.013121366500854492, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 14, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.390663 12578 torch/_dynamo/guards.py:2817] {"artifact": {"name": "recompile_reasons", "encoding": "json"}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "7599673dc829234c1aabdbbad2ef532e"}
	[
	"14/0: tensor 'L['tensor']' rank mismatch. expected 2, actual 1"
	]
V0303 09:10:05.391042 12578 torch/_dynamo/convert_frame.py:894] {"dynamo_start": {"stack": [{"line": 296, "name": "<module>", "filename": 1}, {"line": 1582, "name": "gin_wrapper", "filename": 2}, {"line": 208, "name": "train", "filename": 1}, {"line": 1736, "name": "_wrapped_call_impl", "filename": 3}, {"line": 1747, "name": "_call_impl", "filename": 3}, {"line": 465, "name": "_fn", "filename": 5}, {"line": 426, "name": "forward", "filename": 4}, {"line": 536, "name": "rearrange", "filename": 27}]}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0}
V0303 09:10:05.391297 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "58110f7520ca62c0b411f4a6867aeb7e"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005391214.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.391470 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "93f9b89ca9415fa18174db6abe7588ab"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005391214.5,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.393154 12578 torch/_subclasses/meta_utils.py:204] {"describe_storage": {"id": 0, "describer_id": 34, "size": 4096}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0}
V0303 09:10:05.393427 12578 torch/_subclasses/meta_utils.py:417] {"describe_tensor": {"id": 0, "ndim": 1, "dtype": "torch.float32", "device": "device(type='cuda', index=0)", "size": [1024], "requires_grad": true, "stride": [1], "storage": 0, "view_func": "<built-in method _view_func_unsafe of Tensor object at 0x7fb9383ce630>", "describer_id": 34}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0}
V0303 09:10:05.393574 12578 torch/_subclasses/meta_utils.py:1640] {"describe_source": {"describer_id": 34, "id": 0, "source": "L['tensor']"}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0}
V0303 09:10:05.401957 12578 torch/_dynamo/output_graph.py:1346] {"dynamo_output_graph": {"sizes": {"l_tensor_": ["s0"], "reduce": [256, "(s0//256)"]}}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "165b2986c914285135e5fb3ef0db489c"}
	class GraphModule(torch.nn.Module):
	    def forward(self, s0: "Sym(s0)", L_tensor_: "f32[s0][1]cuda:0"):
	        l_tensor_ = L_tensor_
	        
	         # File: /home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py:591 in rearrange, code: return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
	        reduce: "f32[256, (s0//256)][(s0//256), 1]cuda:0" = einops_einops_reduce(l_tensor_, '(b n) -> b n', reduction = 'rearrange', b = 256);  l_tensor_ = None
	        return (reduce,)
	        
V0303 09:10:05.402370 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "265bc2df9e928cf3088452d71bc88cbf"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005402314.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.402546 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "97dd2840b3cb83f219b1888875132951"}
	{
	"name": "backend_compile",
	"ts": 1740993005402314.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.404291 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "826d291211329291cfb257ec329b35fb"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005404236.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.422812 12578 torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:215] {"aot_forward_graph": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "c844cf71dc318ae33a92fbf0281a6cd7"}
	class <lambda>(torch.nn.Module):
	    def forward(self, arg0_1: "Sym(s0)", arg1_1: "f32[s0][1]cuda:0"):
	         # File: /home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py:591 in rearrange, code: return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
	        floordiv: "Sym((s0//256))" = arg0_1 // 256;  arg0_1 = None
	        view: "f32[256, (s0//256)][(s0//256), 1]cuda:0" = torch.ops.aten.view.default(arg1_1, [256, floordiv]);  arg1_1 = floordiv = None
	        return (view,)
	        
V0303 09:10:05.423213 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "b11fbafc1e4b6c058d893e0eb3463e5a"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005423016.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.424862 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "7d2672e4bdcb36f08f2cfd999b11540b"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005424795.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.425049 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "169a70f3a106b3b6df04f18a67537781"}
	{
	"name": "inductor_compile",
	"ts": 1740993005424795.0,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.431338 12578 torch/_inductor/codecache.py:1132] {"inductor_output_code": {"filename": "/tmp/torchinductor_ec2-user/6d/c6doyjjm3uec56gtmsfpvnlk64qzvhy7xkejiduqyeyk23qnq7s6.py"}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "b359a9803e97fff01665793df3238071"}
	# AOT ID: ['14_inference']
	from ctypes import c_void_p, c_long, c_int
	import torch
	import math
	import random
	import os
	import tempfile
	from math import inf, nan
	from torch._inductor.hooks import run_intermediate_hooks
	from torch._inductor.utils import maybe_profile
	from torch._inductor.codegen.memory_planning import _align as align
	from torch import device, empty_strided
	from torch._inductor.async_compile import AsyncCompile
	from torch._inductor.select_algorithm import extern_kernels
	from torch._inductor.codegen.multi_kernel import MultiKernelCall
	
	aten = torch.ops.aten
	inductor_ops = torch.ops.inductor
	_quantized = torch.ops._quantized
	assert_size_stride = torch._C._dynamo.guards.assert_size_stride
	empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
	empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
	empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
	reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
	alloc_from_pool = torch.ops.inductor._alloc_from_pool
	async_compile = AsyncCompile()
	
	
	async_compile.wait(globals())
	del async_compile
	
	def call(args):
	    arg0_1, arg1_1 = args
	    args.clear()
	    s0 = arg0_1
	    assert_size_stride(arg1_1, (s0, ), (1, ))
	    return (reinterpret_tensor(arg1_1, (256, (s0 // 256)), ((s0 // 256), 1), 0), )
	
	
	def benchmark_compiled_module(times=10, repeat=10):
	    from torch._dynamo.testing import rand_strided
	    from torch._inductor.utils import print_performance
	    arg0_1 = 1024
	    arg1_1 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
	    fn = lambda: call([arg0_1, arg1_1])
	    return print_performance(fn, times=times, repeat=repeat)
	
	
	if __name__ == "__main__":
	    from torch._inductor.wrapper_benchmark import compiled_module_main
	    compiled_module_main('None', benchmark_compiled_module)
	
V0303 09:10:05.431708 12578 torch/_dynamo/utils.py:985] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "173554c70fe67234782024235a5dee66"}
	{
	"name": "fx_graph_cache_hit",
	"ts": 1740993005431506.2,
	"args": {
	"key": "fa7guolcok7dbcnd6oek6diuyfqkwptdefovoput53fu3eimg5cq",
	"components": [
	"[mf63g6esyiimasnn73t5mwljxc36iynhqn35vbkuk7kjf3dwsrt] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    floordiv = arg0_1 // 256;  arg0_1 = None\n    view = torch.ops.aten.view.default(arg1_1, [256, floordiv]);  arg1_1 = floordiv = None\n    return (view,)\n    \n# To see more debug info, please use `graph_module.print_readable()`",
	"[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)",
	"[2i3kxpp2jveltceczzk4ahtnitw4ob2qyb4s2pv4mi4se236u5z] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s0]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False",
	"[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []",
	"[shairiav2uczfvaju735p26onlcujumrymj663kroiuyzs6i55u] fx_kwargs[user_visible_outputs]: {'view': None}",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1",
	"[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)",
	"[7as26aeta7rzhgm2mxh4el36kupf55fr27327kzc2fsdiy3nexy] cuda_matmul_settings: (True, True, True)",
	"[xwojfm2wgtzvv6osq2suz26ozpu7itordb6qnfesrcd6rhlb5jc] torch_version: <bytes>",
	"[us57dzw33utjij5kbyy6wyd2by7wknkpqqzhfk4aav6qz5t74bl] system_info[device]: {'name': 'NVIDIA L4'}",
	"[kkjugxdvgihrm5q2qsprwyxsgxutzunecpz57gaifglfjwiz2nr] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-357ddbf0295262789c6e9d4dedcc296082a0374db4d46e4bfd9e033c0eebfefb-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}",
	"[lx5lhxotgs6siijuq2slk6w4phtynlj37dhglrk4es64ctg6r4h] system_info[hash]: 6fcc6fdc24b0f3f99e8a929624734d67627e680b3fb39d1060ab79bebd873583",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False",
	"[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT",
	"[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True",
	"[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False",
	"[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16",
	"[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None",
	"[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False",
	"[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None",
	"[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None",
	"[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None",
	"[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton",
	"[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False",
	"[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None",
	"[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False",
	"[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False",
	"[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False",
	"[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019",
	"[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True",
	"[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25",
	"[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True",
	"[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10",
	"[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False",
	"[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False",
	"[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP",
	"[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False",
	"[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0",
	"[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0",
	"[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0",
	"[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1",
	"[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False",
	"[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates",
	"[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128",
	"[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None",
	"[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None",
	"[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None",
	"[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8",
	"[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30",
	"[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False",
	"[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True",
	"[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None",
	"[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']",
	"[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True",
	"[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False",
	"[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False",
	"[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False",
	"[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None",
	"[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2",
	"[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256",
	"[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True",
	"[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False",
	"[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192",
	"[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False",
	"[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False",
	"[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False",
	"[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"
	],
	"time_saved_ns": 12704991,
	"cache_state": "hit"
	},
	"ph": "i",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0,
	"s": "p"
	}
V0303 09:10:05.432040 12578 torch/_inductor/codecache.py:1379] {"artifact": {"name": "fx_graph_cache_hash", "encoding": "json"}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "9f8ee090242f16515c235e512e1e5509"}
	{"key": "fa7guolcok7dbcnd6oek6diuyfqkwptdefovoput53fu3eimg5cq", "components": ["[mf63g6esyiimasnn73t5mwljxc36iynhqn35vbkuk7kjf3dwsrt] gm: <lambda>()\n\n\n\ndef forward(self, arg0_1, arg1_1):\n    floordiv = arg0_1 // 256;  arg0_1 = None\n    view = torch.ops.aten.view.default(arg1_1, [256, floordiv]);  arg1_1 = floordiv = None\n    return (view,)\n    \n# To see more debug info, please use `graph_module.print_readable()`", "[u24tscj53t2munm2n4d6s3uftdqr65er4geui7trbayjlc3je4q] example_inputs[0]: ('s0',)", "[2i3kxpp2jveltceczzk4ahtnitw4ob2qyb4s2pv4mi4se236u5z] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([s0]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=None, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[aot_mode]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False", "[xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []", "[shairiav2uczfvaju735p26onlcujumrymj663kroiuyzs6i55u] fx_kwargs[user_visible_outputs]: {'view': None}", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[0]: 1", "[du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)", "[7as26aeta7rzhgm2mxh4el36kupf55fr27327kzc2fsdiy3nexy] cuda_matmul_settings: (True, True, True)", "[xwojfm2wgtzvv6osq2suz26ozpu7itordb6qnfesrcd6rhlb5jc] torch_version: <bytes>", "[us57dzw33utjij5kbyy6wyd2by7wknkpqqzhfk4aav6qz5t74bl] system_info[device]: {'name': 'NVIDIA L4'}", "[kkjugxdvgihrm5q2qsprwyxsgxutzunecpz57gaifglfjwiz2nr] system_info[version]: {'triton': '3.1.0dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-835d4fc33500e1accafc5c5e00f4f73d87432c114860c04b68849bf6f942b8e5-dc767c8fadcf23ea82d79e257c37d44077eae7f681cf967565fd43e9c017937b-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-20b017e9c4d858ab05e783f77df50b86c6d6eee5d79f3f4b158562b4a54f8443-f44338a31e0534290b08653050804c3fabbde403a6d3004ae04f0c28495f0802-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-a979896b9c0acfd41dd953b90bdc4b10968f7c0b45a286eae3f829aaddb2bb55-da771298f7bc45d24a61f35ef51742304421df1ab49d50bf1fc510dd5a46ea4b-357ddbf0295262789c6e9d4dedcc296082a0374db4d46e4bfd9e033c0eebfefb-71330f394e584b0df29595d49f6ac8ac0c5503db9147090dc58ad888cebac7be-f24adfd52383f7866791ebaa5d45a5d2cc826e56ee2fd285f438e85d201fe643-a34be0d3ae4b3ac9aede195cfda42f8a0a097b2bc9642fb59673ce6b3b607f10-36130a37af1b19a0dec569aa08d30b00c74c8f02b6b632999d86dea169146792-36d42f0429aae027cb985b53b9abc616fae4dad9e0ea03953e1e9fb46d0fb9a0-e5d2cb724c08d0ef4130f3ba858d22cf21f834bfd970a5388aa6ad2a6bab91f9', 'cuda': '12.4'}", "[lx5lhxotgs6siijuq2slk6w4phtynlj37dhglrk4es64ctg6r4h] system_info[hash]: 6fcc6fdc24b0f3f99e8a929624734d67627e680b3fb39d1060ab79bebd873583", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[abi_compatible]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[allow_stack_allocation]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_dump_consts_bin]: False", "[ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: ", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: ", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT", "[jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True", "[b4ha3ravs3qv237q65hpfqegbnoww7tf2ahcbu2i7xo6te5spqs] inductor_config[c_shim_version]: 2", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False", "[c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_floating_point_contract_flag]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16", "[g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None", "[sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False", "[bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None", "[tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None", "[lwkz5chtpji756gurqw4foijfi7zfgljtnn5nmnvdi2skpt4mgh] inductor_config[cuda.cutlass_op_denylist_regex]: pingpong", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.generate_test_runner]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None", "[caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton", "[vzzema5ityqj2wepdmkulue7q5pcevdr5h27oxxutf35d4tjume] inductor_config[custom_op_default_layout_constraint]: flexible_layout", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_auto_functionalized_v2]: False", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: ", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False", "[lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None", "[zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False", "[ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False", "[wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False", "[k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019", "[svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True", "[5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25", "[yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True", "[j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10", "[4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False", "[uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False", "[2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP", "[jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False", "[bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0", "[iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0", "[pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0", "[aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1", "[jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False", "[x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates", "[v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128", "[dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_post_pass]: None", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[post_grad_custom_pre_pass]: None", "[4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None", "[gtkv35cxmtt6tr556buxi277a67g25mjojnv32dc4bjvc7bwscw] inductor_config[pre_grad_fusion_options]: {'batch_linear': {}, 'batch_linear_lhs': {}, 'batch_layernorm': {}, 'batch_tanh': {}, 'batch_relu': {}, 'batch_sigmoid': {}}", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None", "[v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: ", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8", "[rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30", "[lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False", "[ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True", "[h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None", "[oartxnko2l7d67tzwwm2otcumaut3n4wwcfgz3o377hmcveu5ft] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx940', 'gfx941', 'gfx942']", "[klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_at_compile_time]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True", "[tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False", "[ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False", "[yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False", "[tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None", "[pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2", "[fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256", "[vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True", "[ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_kernel_names]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False", "[wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192", "[yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_minimal_arrayref_interface]: False", "[cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False", "[esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False", "[hofygoznqmna6yvgsc6itdddi4hxftssgegh6wquixg2yng3a3z] inductor_config[worker_start_method]: subprocess"], "time_saved_ns": 12704991, "cache_state": "hit"}
V0303 09:10:05.432403 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "c8a41fa9f5d5b83ce4af497a90812a28"}
	{
	"name": "inductor_compile",
	"ts": 1740993005432357.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.432586 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "b825b40272df3cab4e4052997993ca55"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005432548.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.432954 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "d4fbda0d4d687313c0d7fcfca2e4d8d6"}
	{
	"name": "compile_fx.<locals>.fw_compiler_base",
	"ts": 1740993005432909.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.435123 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "3c26a734518dd54766b8f4099398a913"}
	{
	"name": "create_aot_dispatcher_function",
	"ts": 1740993005435074.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.435421 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "31524cd3cf7acaa14b5552d483390c9b"}
	{
	"name": "backend_compile",
	"ts": 1740993005435377.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.435603 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "bf94d62c1ef8484407b960f0c4774d6a"}
	{
	"name": "OutputGraph.call_user_compiler",
	"ts": 1740993005435564.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.447175 12578 torch/_dynamo/guards.py:2277] {"dynamo_cpp_guards_str": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "11f5fcd5fbe9cf3d980f23210624f707"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['tensor'], accessed_by=DictGetItemGuardAccessor(tensor)
	| | +- TENSOR_MATCH: check_tensor(L['tensor'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=True, size=[None], stride=[1])
	| | +- NO_HASATTR: hasattr(L['tensor'], '_dynamo_dynamic_indices') == False    
	| +- GuardManager: source=L['pattern'], accessed_by=DictGetItemGuardAccessor(pattern)
	| | +- EQUALS_MATCH: L['pattern'] == '(b n) -> b n'                              
	| +- GuardManager: source=L['axes_lengths'], accessed_by=DictGetItemGuardAccessor(axes_lengths)
	| | +- DICT_LENGTH: len(L['axes_lengths']) == 1                                 
	| | +- GuardManager: source=L['axes_lengths']['b'], accessed_by=DictGetItemGuardAccessor(b)
	| | | +- EQUALS_MATCH: L['axes_lengths']['b'] == 256                               
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor
	| | +- GuardManager: source=G['reduce'], accessed_by=DictGetItemGuardAccessor(reduce)
	| | | +- ID_MATCH: ___check_obj_id(G['reduce'], 140438156473248)               
	+- LAMBDA_GUARD: Eq(L['tensor'].size()[0], 256*((L['tensor'].size()[0]//256)))  # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: Ne((L['tensor'].size()[0]//256), 1)                           # _dynamo/output_graph.py:463 in init_ambient_guards
	+- LAMBDA_GUARD: 256 <= L['tensor'].size()[0]                                  # _dynamo/output_graph.py:463 in init_ambient_guards
	
V0303 09:10:05.447451 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "0202ff0fec04b72ff7b61e2e25b599c6"}
	{
	"name": "entire_frame_compile",
	"ts": 1740993005447404.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.447638 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0, "has_payload": "7030cd74add06f184be8931cd56d69f0"}
	{
	"name": "_compile.compile_inner",
	"ts": 1740993005447597.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.447880 12578 torch/_dynamo/utils.py:810] {"compilation_metrics": {"compile_id": "14/1", "frame_key": "30", "co_name": "rearrange", "co_filename": "/home/ec2-user/.local/lib/python3.9/site-packages/einops/einops.py", "co_firstlineno": 536, "cache_size": 1, "accumulated_cache_size": 1, "guard_count": 11, "shape_env_guard_count": 6, "graph_op_count": 1, "graph_node_count": 4, "graph_input_count": 2, "start_time": 1740993005.391205, "entire_frame_compile_time_s": 0.056122779846191406, "backend_compile_time_s": 0.033019304275512695, "inductor_compile_time_s": 0.007496356964111328, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "possibly_missed_reinplacing_opportunities": 0}, "frame_id": 14, "frame_compile_id": 1, "attempt": 0}
V0303 09:10:05.455028 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "a56f1a581e65c117b4a1b4c368994404"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1740993005454956.2,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.455365 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "bf2bddd5504d5ce74de65d6f04e81a37"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005455309.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.455548 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "39a97a35f523e56010791c69f0f96cfd"}
	{
	"name": "inductor_compile",
	"ts": 1740993005455309.8,
	"args": null,
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.456008 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "9312e12b7233c9af2431a2f0cab77e55"}
	{
	"name": "inductor_compile",
	"ts": 1740993005455965.0,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.456206 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "cf7bfea13d69fd32e96bff4d494fd516"}
	{
	"name": "compile_fx_inner",
	"ts": 1740993005456167.2,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0303 09:10:05.456384 12578 torch/_dynamo/utils.py:810] {"bwd_compilation_metrics": {"compile_id": "13/0", "inductor_compile_time_s": 0.0006103515625, "code_gen_time_s": null, "fail_type": null, "fail_reason": null}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0}
V0303 09:10:05.456589 12578 torch/_dynamo/utils.py:953] {"chromium_event": {}, "frame_id": 13, "frame_compile_id": 0, "attempt": 0, "has_payload": "e53a1020590331c2cc24225590cf0563"}
	{
	"name": "compile_fx.<locals>.bw_compiler",
	"ts": 1740993005456547.8,
	"args": {
	"cache_stats": {
	"fxgraph_cache_hit": 2,
	"fxgraph_cache_miss": 12,
	"fxgraph_cache_bypass": 0
	}
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
