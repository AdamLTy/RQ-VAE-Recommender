class GraphModule(torch.nn.Module):
    def forward(self, primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_5: "Sym(s0)", primals_6: "f32[256, 384][384, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0"):
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
        permute: "f32[384, 256][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
        mm: "f32[s1, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
        floordiv: "Sym((s1//256))" = sym_size_int // 256
        view: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.reshape.default(mm, [256, floordiv, 256]);  mm = floordiv = None
        sym_size_int_1: "Sym((s1//256))" = torch.ops.aten.sym_size.int(view, 1)
        slice_2: "f32[256, (s1//256) - 1, 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view, 1, 0, -1);  view = None
        sym_size_int_2: "Sym((s1//256) - 1)" = torch.ops.aten.sym_size.int(slice_2, 1)
        mul_14: "Sym(256*((s1//256)) - 256)" = 256 * sym_size_int_2
        clone: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.clone.default(slice_2, memory_format = torch.contiguous_format);  slice_2 = None
        view_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.reshape.default(clone, [mul_14, 256]);  clone = mul_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
        view_2: "i64[1024][1]cuda:0" = torch.ops.aten.reshape.default(primals_7, [1024])
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
        amax: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.amax.default(view_1, [1], True)
        sub_6: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_1, amax)
        exp: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(sub_6)
        sum_1: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
        log: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.log.default(sum_1);  sum_1 = None
        sub_7: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = None
        ne_9: "b8[1024][1]cuda:0" = torch.ops.aten.ne.Scalar(view_2, -1)
        full_default: "i64[][]cuda:0" = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where: "i64[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, view_2, full_default);  view_2 = full_default = None
        unsqueeze: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(where, 1);  where = None
        gather: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.gather.default(sub_7, 1, unsqueeze);  sub_7 = unsqueeze = None
        squeeze: "f32[1024][1]cuda:0" = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
        neg: "f32[1024][1]cuda:0" = torch.ops.aten.neg.default(squeeze);  squeeze = None
        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_1: "f32[1024][1]cuda:0" = torch.ops.aten.where.self(ne_9, neg, full_default_1);  ne_9 = neg = full_default_1 = None
        view_3: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.reshape.default(where_1, [256, 4]);  where_1 = None
        sum_2: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_3, [1]);  view_3 = None
        mean: "f32[][]cuda:0" = torch.ops.aten.mean.default(sum_2);  sum_2 = None
        return (mean, view_1, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, view_1, amax, log, sym_size_int, sym_size_int_1, sym_size_int_2)
        