class GraphModule(torch.nn.Module):
    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym((s1//256))", sym_size_int_2: "Sym((s1//256) - 1)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_2: "i64[257][1]cuda:0", primals_3: "f32[s3, 0][1, 1]cuda:0", primals_4: "f32[s4, 0][1, 1]cuda:0", primals_6: "f32[256, 384][384, 1]cuda:0", primals_7: "i64[256, 4][4, 1]cuda:0", view_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0", amax: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0", log: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0", tangents_1: "f32[][]cuda:0", tangents_2: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0"):
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
        expand: "f32[256][0]cuda:0" = torch.ops.aten.expand.default(tangents_1, [256]);  tangents_1 = None
        div: "f32[256][1]cuda:0" = torch.ops.aten.div.Scalar(expand, 256);  expand = None
        unsqueeze_1: "f32[256, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(div, 1);  div = None
        expand_1: "f32[256, 4][1, 0]cuda:0" = torch.ops.aten.expand.default(unsqueeze_1, [256, 4]);  unsqueeze_1 = None
        clone_1: "f32[256, 4][4, 1]cuda:0" = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None
        view_4: "f32[1024][1]cuda:0" = torch.ops.aten.view.default(clone_1, [1024]);  clone_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:432 in torch_dynamo_resume_in_forward_at_426, code: target = batch.sem_ids_fut.flatten(end_dim=1)
        view_2: "i64[1024][1]cuda:0" = torch.ops.aten.view.default(primals_7, [1024]);  primals_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
        unsqueeze_2: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(view_2, 1);  view_2 = None
        ne_11: "b8[1024, 1][1, 1]cuda:0" = torch.ops.aten.ne.Scalar(unsqueeze_2, -1)
        full_default: "i64[][]cuda:0" = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_2: "i64[1024, 1][1, 1]cuda:0" = torch.ops.aten.where.self(ne_11, unsqueeze_2, full_default);  unsqueeze_2 = full_default = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
        sym_size_int_3: "Sym(256*((s1//256)) - 256)" = torch.ops.aten.sym_size.int(view_1, 0)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
        full_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_3, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_3 = None
        scatter: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.scatter.value(full_1, 1, where_2, -1.0);  full_1 = where_2 = None
        unsqueeze_3: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(view_4, 1);  view_4 = None
        full_default_1: "f32[][]cuda:0" = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_3: "f32[1024, 1][1, 1]cuda:0" = torch.ops.aten.where.self(ne_11, unsqueeze_3, full_default_1);  ne_11 = unsqueeze_3 = full_default_1 = None
        mul_40: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
        sub_6: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_1, amax);  view_1 = amax = None
        sub_7: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = log = None
        exp_1: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(sub_7);  sub_7 = None
        sum_3: "f32[256*((s1//256)) - 256, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_40, [1], True)
        mul_41: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(exp_1, sum_3);  exp_1 = sum_3 = None
        sub_10: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_40, mul_41);  mul_40 = mul_41 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:433 in torch_dynamo_resume_in_forward_at_426, code: loss = rearrange(F.cross_entropy(logits, target, reduction="none", ignore_index=-1), "(b n) -> b n", b=B).sum(axis=1).mean()
        add_29: "f32[256*((s1//256)) - 256, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_2, sub_10);  tangents_2 = sub_10 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:431 in torch_dynamo_resume_in_forward_at_426, code: logits = rearrange(jagged_to_flattened_tensor(predict_out), "(b n) d -> b n d", b=B)[:,:-1,:].flatten(end_dim=1)
        view_5: "f32[256, (s1//256) - 1, 256][256*((s1//256)) - 256, 256, 1]cuda:0" = torch.ops.aten.view.default(add_29, [256, sym_size_int_2, 256]);  add_29 = sym_size_int_2 = None
        full_3: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.full.default([256, sym_size_int_1, 256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_1 = None
        slice_scatter_1: "f32[256, (s1//256), 256][256*((s1//256)), 256, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, view_5, 1, 0, -1);  full_3 = view_5 = None
        view_6: "f32[256*((s1//256)), 256][256, 1]cuda:0" = torch.ops.aten.view.default(slice_scatter_1, [sym_size_int, 256]);  slice_scatter_1 = sym_size_int = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:429 in torch_dynamo_resume_in_forward_at_426, code: predict_out = self.out_proj(trnsf_out)
        mm_1: "f32[256*((s1//256)), 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_6, primals_6);  primals_6 = None
        permute_1: "f32[256, 256*((s1//256))][1, 256]cuda:0" = torch.ops.aten.permute.default(view_6, [1, 0]);  view_6 = None
        mm_2: "f32[256, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_1, primals_1);  permute_1 = primals_1 = None
        return (mm_1, primals_2, primals_3, primals_4, None, mm_2, None)
        