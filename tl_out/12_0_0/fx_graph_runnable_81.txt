
import torch
from torch import tensor, device
import torch.fx as fx
from torch._dynamo.testing import rand_strided
from math import inf
import torch._inductor.inductor_prims

import torch._dynamo.config
import torch._inductor.config
import torch._functorch.config
import torch.fx.experimental._config
torch._dynamo.config.suppress_errors = True

torch._functorch.config.unlift_effect_tokens = True



isolate_fails_code_str = None



# torch version: 2.5.1+cu124
# torch cuda version: 12.4
# torch git version: a8d6afb511a69687bbb2b7e88a3cf67917e1697e


# CUDA Info: 
# nvcc: NVIDIA (R) Cuda compiler driver 
# Copyright (c) 2005-2024 NVIDIA Corporation 
# Built on Thu_Mar_28_02:18:24_PDT_2024 
# Cuda compilation tools, release 12.4, V12.4.131 
# Build cuda_12.4.r12.4/compiler.34097967_0 

# GPU Hardware Info: 
# NVIDIA L4 : 1 


from torch.nn import *
class Repro(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()

    
    
    def forward(self, primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7):
        permute = torch.ops.aten.permute.default(primals_6, [1, 0])
        mm = torch.ops.aten.mm.default(primals_1, permute);  permute = None
        sym_size_int = torch.ops.aten.sym_size.int(primals_1, 0)
        floordiv = sym_size_int // 256
        view = torch.ops.aten.view.default(mm, [256, floordiv, 256]);  mm = floordiv = None
        sym_size_int_1 = torch.ops.aten.sym_size.int(view, 1)
        slice_2 = torch.ops.aten.slice.Tensor(view, 1, 0, -1);  view = None
        sym_size_int_2 = torch.ops.aten.sym_size.int(slice_2, 1)
        mul_14 = 256 * sym_size_int_2
        clone = torch.ops.aten.clone.default(slice_2, memory_format = torch.contiguous_format);  slice_2 = None
        view_1 = torch.ops.aten.view.default(clone, [mul_14, 256]);  clone = mul_14 = None
        view_2 = torch.ops.aten.view.default(primals_7, [1024])
        amax = torch.ops.aten.amax.default(view_1, [1], True)
        sub_6 = torch.ops.aten.sub.Tensor(view_1, amax)
        exp = torch.ops.aten.exp.default(sub_6)
        sum_1 = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
        log = torch.ops.aten.log.default(sum_1);  sum_1 = None
        sub_7 = torch.ops.aten.sub.Tensor(sub_6, log);  sub_6 = None
        ne_9 = torch.ops.aten.ne.Scalar(view_2, -1)
        full_default = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where = torch.ops.aten.where.self(ne_9, view_2, full_default);  view_2 = full_default = None
        unsqueeze = torch.ops.aten.unsqueeze.default(where, 1);  where = None
        gather = torch.ops.aten.gather.default(sub_7, 1, unsqueeze);  sub_7 = unsqueeze = None
        squeeze = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
        neg = torch.ops.aten.neg.default(squeeze);  squeeze = None
        full_default_1 = torch.ops.aten.full.default([], 0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_1 = torch.ops.aten.where.self(ne_9, neg, full_default_1);  ne_9 = neg = full_default_1 = None
        view_3 = torch.ops.aten.view.default(where_1, [256, 4]);  where_1 = None
        sum_2 = torch.ops.aten.sum.dim_IntList(view_3, [1]);  view_3 = None
        mean = torch.ops.aten.mean.default(sum_2);  sum_2 = None
        return (mean, view_1, primals_1, primals_2, primals_3, primals_4, primals_6, primals_7, view_1, amax, log, sym_size_int, sym_size_int_1, sym_size_int_2)
        
def load_args(reader):
    buf0 = reader.storage(None, 1536*s1, device=device(type='cuda', index=0))
    reader.tensor(buf0, (s1, 384), is_leaf=True)  # primals_1
    buf1 = reader.storage(None, 2056, device=device(type='cuda', index=0), dtype_hint=torch.int64)
    reader.tensor(buf1, (257,), dtype=torch.int64, is_leaf=True)  # primals_2
    buf2 = reader.storage(None, 0, device=device(type='cuda', index=0))
    reader.tensor(buf2, (s3, 0), is_leaf=True)  # primals_3
    buf3 = reader.storage(None, 0, device=device(type='cuda', index=0))
    reader.tensor(buf3, (s4, 0), is_leaf=True)  # primals_4
    reader.symint(j2)  # primals_5
    buf4 = reader.storage(None, 393216, device=device(type='cuda', index=0))
    reader.tensor(buf4, (256, 384), is_leaf=True)  # primals_6
    buf5 = reader.storage(None, 8192, device=device(type='cuda', index=0), dtype_hint=torch.int64)
    reader.tensor(buf5, (256, 4), dtype=torch.int64, is_leaf=True)  # primals_7
load_args._version = 0
mod = Repro()
if __name__ == '__main__':
    from torch._dynamo.repro.after_aot import run_repro
    with torch.no_grad():
        run_repro(mod, load_args, accuracy=False, command='run', save_dir=None, tracing_mode='symbolic', check_str=None)
        # To run it separately, do 
        # mod, args = run_repro(mod, load_args, accuracy=False, command='get_args', save_dir=None, tracing_mode='symbolic', check_str=None)
        # mod(*args)