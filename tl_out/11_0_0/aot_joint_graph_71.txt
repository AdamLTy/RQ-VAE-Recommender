class joint_fn(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[1024, 384][384, 1]cuda:0"; primals_7: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
    
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        permute: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_6, [1, 0])
        mm: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(primals_1, permute);  permute = None
        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
        mul: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sigmoid);  sigmoid = None
        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
        rand: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
        mul_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul);  mul = None
        mul_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1, 1.4285714285714286);  mul_1 = None
        permute_1: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_2, permute_1);  permute_1 = None
        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(tangents_1, primals_7);  primals_7 = None
        permute_2: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(tangents_1, [1, 0]);  tangents_1 = None
        mm_3: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_2, mul_2);  permute_2 = mul_2 = None
        convert_element_type: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
        mul_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type, 1.4285714285714286);  convert_element_type = None
        mul_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, mul_3);  mm_2 = mul_3 = None
        clone: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_4, memory_format = torch.contiguous_format);  mul_4 = None
        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm)
        full: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
        mul_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm, sub);  mm = sub = None
        add: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_5, 1);  mul_5 = None
        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add);  sigmoid_1 = add = None
        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone, mul_6);  clone = mul_6 = None
        mm_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_7, primals_6);  primals_6 = None
        permute_4: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_7, [1, 0]);  mul_7 = None
        mm_5: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_4, primals_1);  permute_4 = primals_1 = None
        return pytree.tree_unflatten([mm_1, primals_2, primals_3, primals_4, mm_4, tangents_2, tangents_3, tangents_4, None, mm_5, mm_3], self._out_spec)
        