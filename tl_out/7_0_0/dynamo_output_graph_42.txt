class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0", L_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_
        l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
        l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
        l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_
        l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_
        l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
        l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
        l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_
        l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_
        l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
        l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
        l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = L_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_
        l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = L_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_
        l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
        l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1.pow(2)
        mean: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        add: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean + 1e-06;  mean = None
        rsqrt: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        mul: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1 * rsqrt;  float_1 = rsqrt = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul.type_as(l_x_);  mul = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output * l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_;  output = l_self_modules_layers_modules_0_modules_attn_norm_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        dropout: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_1, 0.3, True, False);  mul_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        linear: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout, l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_, None);  dropout = l_self_modules_layers_modules_0_modules_attention_modules_qkv_parameters_weight_ = None
        chunk = linear.chunk(3, dim = -1);  linear = None
        queries: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[0]
        keys: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[1]
        values: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[2];  chunk = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries.unflatten(-1, [6, 64]);  queries = None
        queries_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_1: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys.unflatten(-1, [6, 64]);  keys = None
        keys_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_2: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values.unflatten(-1, [6, 64]);  values = None
        values_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        context_vec: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_1, keys_1, values_1, dropout_p = False, is_causal = False);  queries_1 = keys_1 = values_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        transpose_3: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
        context_vec_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        context_vec_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_1, l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_, None);  context_vec_1 = l_self_modules_layers_modules_0_modules_attention_modules_proj_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        attn_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_ + context_vec_2;  l_x_ = context_vec_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2.pow(2)
        mean_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_2.mean(-1, keepdim = True);  pow_2 = None
        add_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_1 + 1e-06;  mean_1 = None
        rsqrt_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_2);  add_2 = None
        mul_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2 * rsqrt_1;  float_2 = rsqrt_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_2.type_as(attn_out);  mul_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        input_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_1 * l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_;  output_1 = l_self_modules_layers_modules_0_modules_ff_modules_0_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        input_2: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_1, l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_1 = l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
        input_3: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_2, inplace = False);  input_2 = None
        input_4: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_3, 0.3, True, False);  input_3 = None
        input_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_4, l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_4 = l_self_modules_layers_modules_0_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        input_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_5, 0.3, True, False);  input_5 = None
        proj_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out + input_6;  attn_out = input_6 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_3.pow(2)
        mean_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_3.mean(-1, keepdim = True);  pow_3 = None
        add_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_2 + 1e-06;  mean_2 = None
        rsqrt_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_4);  add_4 = None
        mul_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_3 * rsqrt_2;  float_3 = rsqrt_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_4.type_as(proj_out);  mul_4 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_2 * l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_;  output_2 = l_self_modules_layers_modules_1_modules_attn_norm_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        dropout_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_5, 0.3, True, False);  mul_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        linear_4: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_3, l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_, None);  dropout_3 = l_self_modules_layers_modules_1_modules_attention_modules_qkv_parameters_weight_ = None
        chunk_1 = linear_4.chunk(3, dim = -1);  linear_4 = None
        queries_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[0]
        keys_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[1]
        values_2: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_1[2];  chunk_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_3: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_2.unflatten(-1, [6, 64]);  queries_2 = None
        queries_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_3.transpose(1, 2);  unflatten_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_4: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_2.unflatten(-1, [6, 64]);  keys_2 = None
        keys_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_4.transpose(1, 2);  unflatten_4 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_5: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_2.unflatten(-1, [6, 64]);  values_2 = None
        values_3: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_5.transpose(1, 2);  unflatten_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        context_vec_3: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_3, keys_3, values_3, dropout_p = False, is_causal = False);  queries_3 = keys_3 = values_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        transpose_7: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_3.transpose(1, 2);  context_vec_3 = None
        context_vec_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_7.flatten(-2);  transpose_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        context_vec_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_4, l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_, None);  context_vec_4 = l_self_modules_layers_modules_1_modules_attention_modules_proj_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        attn_out_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out + context_vec_5;  proj_out = context_vec_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_1.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_4.pow(2)
        mean_3: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_4.mean(-1, keepdim = True);  pow_4 = None
        add_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_3 + 1e-06;  mean_3 = None
        rsqrt_3: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_6);  add_6 = None
        mul_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_4 * rsqrt_3;  float_4 = rsqrt_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_6.type_as(attn_out_1);  mul_6 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        input_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_3 * l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_;  output_3 = l_self_modules_layers_modules_1_modules_ff_modules_0_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        input_8: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_7, l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_7 = l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
        input_9: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_8, inplace = False);  input_8 = None
        input_10: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_9, 0.3, True, False);  input_9 = None
        input_11: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_10, l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_10 = l_self_modules_layers_modules_1_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        input_12: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_11, 0.3, True, False);  input_11 = None
        proj_out_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_1 + input_12;  attn_out_1 = input_12 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_1.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_5.pow(2)
        mean_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_5.mean(-1, keepdim = True);  pow_5 = None
        add_8: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_4 + 1e-06;  mean_4 = None
        rsqrt_4: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_8);  add_8 = None
        mul_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_5 * rsqrt_4;  float_5 = rsqrt_4 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_4: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_8.type_as(proj_out_1);  mul_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_9: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_4 * l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_;  output_4 = l_self_modules_layers_modules_2_modules_attn_norm_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        dropout_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_9, 0.3, True, False);  mul_9 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        linear_8: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_6, l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_, None);  dropout_6 = l_self_modules_layers_modules_2_modules_attention_modules_qkv_parameters_weight_ = None
        chunk_2 = linear_8.chunk(3, dim = -1);  linear_8 = None
        queries_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[0]
        keys_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[1]
        values_4: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_2[2];  chunk_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_6: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_4.unflatten(-1, [6, 64]);  queries_4 = None
        queries_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_6.transpose(1, 2);  unflatten_6 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_7: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_4.unflatten(-1, [6, 64]);  keys_4 = None
        keys_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_7.transpose(1, 2);  unflatten_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_8: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_4.unflatten(-1, [6, 64]);  values_4 = None
        values_5: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_8.transpose(1, 2);  unflatten_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        context_vec_6: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_5, keys_5, values_5, dropout_p = False, is_causal = False);  queries_5 = keys_5 = values_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        transpose_11: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_6.transpose(1, 2);  context_vec_6 = None
        context_vec_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_11.flatten(-2);  transpose_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        context_vec_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_7, l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_, None);  context_vec_7 = l_self_modules_layers_modules_2_modules_attention_modules_proj_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        attn_out_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_1 + context_vec_8;  proj_out_1 = context_vec_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_2.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_6.pow(2)
        mean_5: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_6.mean(-1, keepdim = True);  pow_6 = None
        add_10: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_5 + 1e-06;  mean_5 = None
        rsqrt_5: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_10);  add_10 = None
        mul_10: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_6 * rsqrt_5;  float_6 = rsqrt_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_10.type_as(attn_out_2);  mul_10 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        input_13: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_5 * l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_;  output_5 = l_self_modules_layers_modules_2_modules_ff_modules_0_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        input_14: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_13, l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_13 = l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
        input_15: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_14, inplace = False);  input_14 = None
        input_16: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_15, 0.3, True, False);  input_15 = None
        input_17: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_16, l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_16 = l_self_modules_layers_modules_2_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        input_18: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_17, 0.3, True, False);  input_17 = None
        proj_out_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_2 + input_18;  attn_out_2 = input_18 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_2.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_7.pow(2)
        mean_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_7.mean(-1, keepdim = True);  pow_7 = None
        add_12: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_6 + 1e-06;  mean_6 = None
        rsqrt_6: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_12);  add_12 = None
        mul_12: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_7 * rsqrt_6;  float_7 = rsqrt_6 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_12.type_as(proj_out_2);  mul_12 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_13: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_6 * l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_;  output_6 = l_self_modules_layers_modules_3_modules_attn_norm_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        dropout_9: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_13, 0.3, True, False);  mul_13 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        linear_12: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout_9, l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_, None);  dropout_9 = l_self_modules_layers_modules_3_modules_attention_modules_qkv_parameters_weight_ = None
        chunk_3 = linear_12.chunk(3, dim = -1);  linear_12 = None
        queries_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[0]
        keys_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[1]
        values_6: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk_3[2];  chunk_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_9: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries_6.unflatten(-1, [6, 64]);  queries_6 = None
        queries_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_9.transpose(1, 2);  unflatten_9 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_10: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys_6.unflatten(-1, [6, 64]);  keys_6 = None
        keys_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_10.transpose(1, 2);  unflatten_10 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_11: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values_6.unflatten(-1, [6, 64]);  values_6 = None
        values_7: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_11.transpose(1, 2);  unflatten_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        context_vec_9: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_7, keys_7, values_7, dropout_p = False, is_causal = False);  queries_7 = keys_7 = values_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        transpose_15: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec_9.transpose(1, 2);  context_vec_9 = None
        context_vec_10: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_15.flatten(-2);  transpose_15 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        context_vec_11: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_10, l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_, None);  context_vec_10 = l_self_modules_layers_modules_3_modules_attention_modules_proj_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        attn_out_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = proj_out_2 + context_vec_11;  proj_out_2 = context_vec_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_3.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_8: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_8.pow(2)
        mean_7: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_8.mean(-1, keepdim = True);  pow_8 = None
        add_14: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_7 + 1e-06;  mean_7 = None
        rsqrt_7: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_14);  add_14 = None
        mul_14: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_8 * rsqrt_7;  float_8 = rsqrt_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_7: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_14.type_as(attn_out_3);  mul_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        input_19: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_7 * l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_;  output_7 = l_self_modules_layers_modules_3_modules_ff_modules_0_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        input_20: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_19, l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_19 = l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
        input_21: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_20, inplace = False);  input_20 = None
        input_22: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_21, 0.3, True, False);  input_21 = None
        input_23: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_22, l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_22 = l_self_modules_layers_modules_3_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        input_24: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_23, 0.3, True, False);  input_23 = None
        proj_out_3: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out_3 + input_24;  attn_out_3 = input_24 = None
        return (proj_out_3,)
        