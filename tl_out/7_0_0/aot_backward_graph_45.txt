class GraphModule(torch.nn.Module):
    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", primals_12: "f32[384][1]cuda:0", primals_13: "f32[1152, 384][384, 1]cuda:0", primals_14: "f32[384, 384][384, 1]cuda:0", primals_15: "f32[384][1]cuda:0", primals_16: "f32[1024, 384][384, 1]cuda:0", primals_17: "f32[384, 1024][1024, 1]cuda:0", primals_18: "f32[384][1]cuda:0", primals_19: "f32[1152, 384][384, 1]cuda:0", primals_20: "f32[384, 384][384, 1]cuda:0", primals_21: "f32[384][1]cuda:0", primals_22: "f32[1024, 384][384, 1]cuda:0", primals_23: "f32[384, 1024][1024, 1]cuda:0", primals_24: "f32[384][1]cuda:0", primals_25: "f32[1152, 384][384, 1]cuda:0", primals_26: "f32[384, 384][384, 1]cuda:0", primals_27: "f32[384][1]cuda:0", primals_28: "f32[1024, 384][384, 1]cuda:0", primals_29: "f32[384, 1024][1024, 1]cuda:0", rsqrt: "f32[s1, 1][1, 1]cuda:0", gt: "b8[s1, 384][384, 1]cuda:0", mul_3: "f32[s1, 384][384, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_5: "i64[][]cuda:0", getitem_6: "i64[][]cuda:0", mm_1: "f32[s1, 384][384, 1]cuda:0", rsqrt_1: "f32[s1, 1][1, 1]cuda:0", mul_5: "f32[s1, 384][384, 1]cuda:0", mm_2: "f32[s1, 1024][1024, 1]cuda:0", gt_1: "b8[s1, 1024][1024, 1]cuda:0", mul_8: "f32[s1, 1024][1024, 1]cuda:0", gt_2: "b8[s1, 384][384, 1]cuda:0", add_3: "f32[s1, 384][384, 1]cuda:0", rsqrt_2: "f32[s1, 1][1, 1]cuda:0", gt_3: "b8[s1, 384][384, 1]cuda:0", mul_14: "f32[s1, 384][384, 1]cuda:0", convert_element_type_2: "i32[257][1]cuda:0", unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_14: "i64[][]cuda:0", getitem_15: "i64[][]cuda:0", add_5: "f32[s1, 384][384, 1]cuda:0", rsqrt_3: "f32[s1, 1][1, 1]cuda:0", mul_16: "f32[s1, 384][384, 1]cuda:0", mm_6: "f32[s1, 1024][1024, 1]cuda:0", gt_4: "b8[s1, 1024][1024, 1]cuda:0", mul_19: "f32[s1, 1024][1024, 1]cuda:0", gt_5: "b8[s1, 384][384, 1]cuda:0", add_7: "f32[s1, 384][384, 1]cuda:0", rsqrt_4: "f32[s1, 1][1, 1]cuda:0", gt_6: "b8[s1, 384][384, 1]cuda:0", mul_25: "f32[s1, 384][384, 1]cuda:0", convert_element_type_4: "i32[257][1]cuda:0", unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_23: "i64[][]cuda:0", getitem_24: "i64[][]cuda:0", add_9: "f32[s1, 384][384, 1]cuda:0", rsqrt_5: "f32[s1, 1][1, 1]cuda:0", mul_27: "f32[s1, 384][384, 1]cuda:0", mm_10: "f32[s1, 1024][1024, 1]cuda:0", gt_7: "b8[s1, 1024][1024, 1]cuda:0", mul_30: "f32[s1, 1024][1024, 1]cuda:0", gt_8: "b8[s1, 384][384, 1]cuda:0", add_11: "f32[s1, 384][384, 1]cuda:0", rsqrt_6: "f32[s1, 1][1, 1]cuda:0", gt_9: "b8[s1, 384][384, 1]cuda:0", mul_36: "f32[s1, 384][384, 1]cuda:0", convert_element_type_6: "i32[257][1]cuda:0", unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_32: "i64[][]cuda:0", getitem_33: "i64[][]cuda:0", add_13: "f32[s1, 384][384, 1]cuda:0", rsqrt_7: "f32[s1, 1][1, 1]cuda:0", mul_38: "f32[s1, 384][384, 1]cuda:0", mm_14: "f32[s1, 1024][1024, 1]cuda:0", gt_10: "b8[s1, 1024][1024, 1]cuda:0", mul_41: "f32[s1, 1024][1024, 1]cuda:0", gt_11: "b8[s1, 384][384, 1]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cuda:0", tangents_4: "f32[s4, 0][1, 1]cuda:0"):
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_11, torch.float32);  gt_11 = None
        mul_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_8, 1.4285714285714286);  convert_element_type_8 = None
        mul_45: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_44);  mul_44 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_45, primals_29);  primals_29 = None
        permute_48: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_45, [1, 0]);  mul_45 = None
        mm_17: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_48, mul_41);  permute_48 = mul_41 = None
        convert_element_type_9: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_10, torch.float32);  gt_10 = None
        mul_46: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_9, 1.4285714285714286);  convert_element_type_9 = None
        mul_47: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_16, mul_46);  mm_16 = mul_46 = None
        sigmoid_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
        full_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_4)
        mul_48: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sub);  mm_14 = sub = None
        add_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_48, 1);  mul_48 = None
        mul_49: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_4, add_16);  sigmoid_4 = add_16 = None
        mul_50: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_47, mul_49);  mul_47 = mul_49 = None
        mm_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_50, primals_28);  primals_28 = None
        permute_50: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_50, [1, 0]);  mul_50 = None
        mm_19: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_50, mul_38);  permute_50 = mul_38 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, mul_37);  mul_37 = None
        mul_52: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, primals_27);  mm_18 = primals_27 = None
        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_51, [0], True);  mul_51 = None
        unsqueeze_12: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
        view_16: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_12, [384]);  unsqueeze_12 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_53: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, add_13)
        mul_54: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, rsqrt_7);  mul_52 = None
        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_53, [1], True);  mul_53 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_54);  tangents_1 = mul_54 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_9: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_7, 3);  rsqrt_7 = None
        mul_55: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_56: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, pow_9);  mul_55 = pow_9 = None
        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_56, [-1, 384]);  mul_56 = None
        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
        pow_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 1.0);  add_13 = None
        mul_57: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_10, 2.0);  pow_10 = None
        mul_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_57);  div = mul_57 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_17, mul_58);  add_17 = mul_58 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_18, primals_26);  primals_26 = None
        permute_51: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_18, [1, 0])
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0)
        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_43, [1, 0, 2]);  permute_43 = None
        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_21: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_51, view_15);  permute_51 = view_15 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [sym_size_int, 6, 64]);  mm_20 = None
        permute_52: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_17, [1, 0, 2]);  view_17 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_53: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_52, [1, 0, 2]);  permute_52 = None
        unsqueeze_13: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_53, 0);  permute_53 = None
        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_13, unsqueeze_9, unsqueeze_10, unsqueeze_11, None, getitem_30, convert_element_type_6, convert_element_type_6, sym_size_int_1, sym_size_int_1, getitem_31, 0.0, getitem_32, getitem_33, 0, False);  unsqueeze_13 = unsqueeze_9 = unsqueeze_10 = unsqueeze_11 = getitem_30 = convert_element_type_6 = getitem_31 = getitem_32 = getitem_33 = None
        getitem_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
        getitem_37: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
        getitem_38: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
        squeeze_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_38, 0);  getitem_38 = None
        squeeze_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_37, 0);  getitem_37 = None
        squeeze_6: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_36, 0);  getitem_36 = None
        permute_54: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_4, [1, 0, 2]);  squeeze_4 = None
        permute_55: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_5, [1, 0, 2]);  squeeze_5 = None
        permute_56: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_6, [1, 0, 2]);  squeeze_6 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_57: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_54, [1, 0, 2]);  permute_54 = None
        view_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_57, [sym_size_int, 384]);  permute_57 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_55, [1, 0, 2]);  permute_55 = None
        view_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_58, [sym_size_int, 384]);  permute_58 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_59: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_56, [1, 0, 2]);  permute_56 = None
        view_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_59, [sym_size_int, 384]);  permute_59 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        split_4 = torch.ops.aten.split.Tensor(full_9, 384, 1)
        getitem_40: "f32[s1, 384][1152, 1]cuda:0" = split_4[0];  split_4 = None
        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_20);  view_20 = None
        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy, 1, 0, 384);  copy = None
        split_7 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
        getitem_50: "f32[s1, 384][1152, 1]cuda:0" = split_7[1];  split_7 = None
        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_50, view_19);  getitem_50 = view_19 = None
        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
        split_10 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
        getitem_60: "f32[s1, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_60, view_18);  getitem_60 = view_18 = None
        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
        mm_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_25);  primals_25 = None
        permute_61: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
        mm_23: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_61, mul_36);  permute_61 = mul_36 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_9, torch.float32);  gt_9 = None
        mul_59: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_10, 1.4285714285714286);  convert_element_type_10 = None
        mul_60: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_22, mul_59);  mm_22 = mul_59 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_61: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_60, mul_33);  mul_33 = None
        mul_62: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_60, primals_24);  mul_60 = primals_24 = None
        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_61, [0], True);  mul_61 = None
        unsqueeze_14: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
        view_21: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_14, [384]);  unsqueeze_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_63: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, add_11)
        mul_64: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, rsqrt_6);  mul_62 = None
        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_63, [1], True);  mul_63 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_18, mul_64);  add_18 = mul_64 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_11: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_6, 3);  rsqrt_6 = None
        mul_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
        mul_66: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, pow_11);  mul_65 = pow_11 = None
        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_66, [-1, 384]);  mul_66 = None
        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
        pow_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 1.0);  add_11 = None
        mul_67: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_12, 2.0);  pow_12 = None
        mul_68: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_67);  div_1 = mul_67 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, mul_68);  add_19 = mul_68 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_8, torch.float32);  gt_8 = None
        mul_69: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, 1.4285714285714286);  convert_element_type_11 = None
        mul_70: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_20, mul_69);  mul_69 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_24: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_70, primals_23);  primals_23 = None
        permute_62: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_70, [1, 0]);  mul_70 = None
        mm_25: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_62, mul_30);  permute_62 = mul_30 = None
        convert_element_type_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_7, torch.float32);  gt_7 = None
        mul_71: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, 1.4285714285714286);  convert_element_type_12 = None
        mul_72: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_24, mul_71);  mm_24 = mul_71 = None
        sigmoid_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
        sub_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_5)
        mul_73: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sub_1);  mm_10 = sub_1 = None
        add_21: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_73, 1);  mul_73 = None
        mul_74: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_5, add_21);  sigmoid_5 = add_21 = None
        mul_75: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_72, mul_74);  mul_72 = mul_74 = None
        mm_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_75, primals_22);  primals_22 = None
        permute_64: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_75, [1, 0]);  mul_75 = None
        mm_27: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_64, mul_27);  permute_64 = mul_27 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_76: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, mul_26);  mul_26 = None
        mul_77: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, primals_21);  mm_26 = primals_21 = None
        sum_5: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_76, [0], True);  mul_76 = None
        unsqueeze_15: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_5, 0);  sum_5 = None
        view_22: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_15, [384]);  unsqueeze_15 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, add_9)
        mul_79: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, rsqrt_5);  mul_77 = None
        sum_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_78, [1], True);  mul_78 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_20, mul_79);  add_20 = mul_79 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_13: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_5, 3);  rsqrt_5 = None
        mul_80: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_6, -0.5);  sum_6 = None
        mul_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, pow_13);  mul_80 = pow_13 = None
        expand_2: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_81, [-1, 384]);  mul_81 = None
        div_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_2, 384);  expand_2 = None
        pow_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 1.0);  add_9 = None
        mul_82: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_14, 2.0);  pow_14 = None
        mul_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_2, mul_82);  div_2 = mul_82 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_22, mul_83);  add_22 = mul_83 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_23, primals_20);  primals_20 = None
        permute_65: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_23, [1, 0])
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0)
        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0, 2]);  permute_31 = None
        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_29: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_65, view_11);  permute_65 = view_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_28, [sym_size_int, 6, 64]);  mm_28 = None
        permute_66: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_23, [1, 0, 2]);  view_23 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_67: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_66, [1, 0, 2]);  permute_66 = None
        unsqueeze_16: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_67, 0);  permute_67 = None
        _efficient_attention_backward_1 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_16, unsqueeze_6, unsqueeze_7, unsqueeze_8, None, getitem_21, convert_element_type_4, convert_element_type_4, sym_size_int_1, sym_size_int_1, getitem_22, 0.0, getitem_23, getitem_24, 0, False);  unsqueeze_16 = unsqueeze_6 = unsqueeze_7 = unsqueeze_8 = getitem_21 = convert_element_type_4 = getitem_22 = getitem_23 = getitem_24 = None
        getitem_67: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[0]
        getitem_68: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[1]
        getitem_69: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[2];  _efficient_attention_backward_1 = None
        squeeze_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_69, 0);  getitem_69 = None
        squeeze_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_68, 0);  getitem_68 = None
        squeeze_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_67, 0);  getitem_67 = None
        permute_68: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_7, [1, 0, 2]);  squeeze_7 = None
        permute_69: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_8, [1, 0, 2]);  squeeze_8 = None
        permute_70: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_9, [1, 0, 2]);  squeeze_9 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_71: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_68, [1, 0, 2]);  permute_68 = None
        view_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_71, [sym_size_int, 384]);  permute_71 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_72: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_69, [1, 0, 2]);  permute_69 = None
        view_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_72, [sym_size_int, 384]);  permute_72 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_73: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_70, [1, 0, 2]);  permute_70 = None
        view_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_73, [sym_size_int, 384]);  permute_73 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        copy_3: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_26);  view_26 = None
        slice_scatter_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy_3, 1, 0, 384);  copy_3 = None
        split_16 = torch.ops.aten.split.Tensor(slice_scatter_3, 384, 1)
        getitem_81: "f32[s1, 384][1152, 1]cuda:0" = split_16[1];  split_16 = None
        copy_4: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_81, view_25);  getitem_81 = view_25 = None
        slice_scatter_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, copy_4, 1, 384, 768);  slice_scatter_3 = copy_4 = None
        split_19 = torch.ops.aten.split.Tensor(slice_scatter_4, 384, 1)
        getitem_91: "f32[s1, 384][1152, 1]cuda:0" = split_19[2];  split_19 = None
        copy_5: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_91, view_24);  getitem_91 = view_24 = None
        slice_scatter_5: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_4, copy_5, 1, 768, 1152);  slice_scatter_4 = copy_5 = None
        mm_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_5, primals_19);  primals_19 = None
        permute_75: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_5, [1, 0]);  slice_scatter_5 = None
        mm_31: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_75, mul_25);  permute_75 = mul_25 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_6, torch.float32);  gt_6 = None
        mul_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_13, 1.4285714285714286);  convert_element_type_13 = None
        mul_85: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_30, mul_84);  mm_30 = mul_84 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_86: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, mul_22);  mul_22 = None
        mul_87: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_85, primals_18);  mul_85 = primals_18 = None
        sum_7: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_86, [0], True);  mul_86 = None
        unsqueeze_17: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_7, 0);  sum_7 = None
        view_27: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_17, [384]);  unsqueeze_17 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_88: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, add_7)
        mul_89: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, rsqrt_4);  mul_87 = None
        sum_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_88, [1], True);  mul_88 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, mul_89);  add_23 = mul_89 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_15: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_4, 3);  rsqrt_4 = None
        mul_90: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_8, -0.5);  sum_8 = None
        mul_91: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_90, pow_15);  mul_90 = pow_15 = None
        expand_3: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_91, [-1, 384]);  mul_91 = None
        div_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_3, 384);  expand_3 = None
        pow_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 1.0);  add_7 = None
        mul_92: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_16, 2.0);  pow_16 = None
        mul_93: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_3, mul_92);  div_3 = mul_92 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_24, mul_93);  add_24 = mul_93 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_5, torch.float32);  gt_5 = None
        mul_94: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_14, 1.4285714285714286);  convert_element_type_14 = None
        mul_95: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, mul_94);  mul_94 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_32: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_95, primals_17);  primals_17 = None
        permute_76: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_95, [1, 0]);  mul_95 = None
        mm_33: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_76, mul_19);  permute_76 = mul_19 = None
        convert_element_type_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_4, torch.float32);  gt_4 = None
        mul_96: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_15, 1.4285714285714286);  convert_element_type_15 = None
        mul_97: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_32, mul_96);  mm_32 = mul_96 = None
        sigmoid_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
        sub_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_6)
        mul_98: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sub_2);  mm_6 = sub_2 = None
        add_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_98, 1);  mul_98 = None
        mul_99: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_6, add_26);  sigmoid_6 = add_26 = None
        mul_100: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_97, mul_99);  mul_97 = mul_99 = None
        mm_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_100, primals_16);  primals_16 = None
        permute_78: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_100, [1, 0]);  mul_100 = None
        mm_35: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_78, mul_16);  permute_78 = mul_16 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_101: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, mul_15);  mul_15 = None
        mul_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, primals_15);  mm_34 = primals_15 = None
        sum_9: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_101, [0], True);  mul_101 = None
        unsqueeze_18: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_9, 0);  sum_9 = None
        view_28: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_18, [384]);  unsqueeze_18 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_103: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, add_5)
        mul_104: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, rsqrt_3);  mul_102 = None
        sum_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_103, [1], True);  mul_103 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_25, mul_104);  add_25 = mul_104 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_17: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_3, 3);  rsqrt_3 = None
        mul_105: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_10, -0.5);  sum_10 = None
        mul_106: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_105, pow_17);  mul_105 = pow_17 = None
        expand_4: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_106, [-1, 384]);  mul_106 = None
        div_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_4, 384);  expand_4 = None
        pow_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 1.0);  add_5 = None
        mul_107: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_18, 2.0);  pow_18 = None
        mul_108: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_4, mul_107);  div_4 = mul_107 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, mul_108);  add_27 = mul_108 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_28, primals_14);  primals_14 = None
        permute_79: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_28, [1, 0])
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0)
        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_37: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_79, view_7);  permute_79 = view_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_29: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_36, [sym_size_int, 6, 64]);  mm_36 = None
        permute_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_29, [1, 0, 2]);  view_29 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_81: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_80, [1, 0, 2]);  permute_80 = None
        unsqueeze_19: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_81, 0);  permute_81 = None
        _efficient_attention_backward_2 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_19, unsqueeze_3, unsqueeze_4, unsqueeze_5, None, getitem_12, convert_element_type_2, convert_element_type_2, sym_size_int_1, sym_size_int_1, getitem_13, 0.0, getitem_14, getitem_15, 0, False);  unsqueeze_19 = unsqueeze_3 = unsqueeze_4 = unsqueeze_5 = getitem_12 = convert_element_type_2 = getitem_13 = getitem_14 = getitem_15 = None
        getitem_98: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[0]
        getitem_99: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[1]
        getitem_100: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[2];  _efficient_attention_backward_2 = None
        squeeze_10: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_100, 0);  getitem_100 = None
        squeeze_11: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_99, 0);  getitem_99 = None
        squeeze_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0);  getitem_98 = None
        permute_82: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_10, [1, 0, 2]);  squeeze_10 = None
        permute_83: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_11, [1, 0, 2]);  squeeze_11 = None
        permute_84: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_12, [1, 0, 2]);  squeeze_12 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_85: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_82, [1, 0, 2]);  permute_82 = None
        view_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_85, [sym_size_int, 384]);  permute_85 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_86: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_83, [1, 0, 2]);  permute_83 = None
        view_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_86, [sym_size_int, 384]);  permute_86 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_87: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_84, [1, 0, 2]);  permute_84 = None
        view_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_87, [sym_size_int, 384]);  permute_87 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        copy_6: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_32);  view_32 = None
        slice_scatter_6: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy_6, 1, 0, 384);  copy_6 = None
        split_25 = torch.ops.aten.split.Tensor(slice_scatter_6, 384, 1)
        getitem_112: "f32[s1, 384][1152, 1]cuda:0" = split_25[1];  split_25 = None
        copy_7: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_112, view_31);  getitem_112 = view_31 = None
        slice_scatter_7: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_6, copy_7, 1, 384, 768);  slice_scatter_6 = copy_7 = None
        split_28 = torch.ops.aten.split.Tensor(slice_scatter_7, 384, 1)
        getitem_122: "f32[s1, 384][1152, 1]cuda:0" = split_28[2];  split_28 = None
        copy_8: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_122, view_30);  getitem_122 = view_30 = None
        slice_scatter_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_7, copy_8, 1, 768, 1152);  slice_scatter_7 = copy_8 = None
        mm_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_8, primals_13);  primals_13 = None
        permute_89: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_8, [1, 0]);  slice_scatter_8 = None
        mm_39: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_89, mul_14);  permute_89 = mul_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_3, torch.float32);  gt_3 = None
        mul_109: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_16, 1.4285714285714286);  convert_element_type_16 = None
        mul_110: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_38, mul_109);  mm_38 = mul_109 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_111: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_110, mul_11);  mul_11 = None
        mul_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_110, primals_12);  mul_110 = primals_12 = None
        sum_11: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_111, [0], True);  mul_111 = None
        unsqueeze_20: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_11, 0);  sum_11 = None
        view_33: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_20, [384]);  unsqueeze_20 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_113: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, add_3)
        mul_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, rsqrt_2);  mul_112 = None
        sum_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_113, [1], True);  mul_113 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_28, mul_114);  add_28 = mul_114 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_2, 3);  rsqrt_2 = None
        mul_115: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_12, -0.5);  sum_12 = None
        mul_116: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, pow_19);  mul_115 = pow_19 = None
        expand_5: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_116, [-1, 384]);  mul_116 = None
        div_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_5, 384);  expand_5 = None
        pow_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 1.0);  add_3 = None
        mul_117: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_20, 2.0);  pow_20 = None
        mul_118: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_5, mul_117);  div_5 = mul_117 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_29, mul_118);  add_29 = mul_118 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
        mul_119: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_17, 1.4285714285714286);  convert_element_type_17 = None
        mul_120: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_30, mul_119);  mul_119 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_120, primals_11);  primals_11 = None
        permute_90: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_120, [1, 0]);  mul_120 = None
        mm_41: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_90, mul_8);  permute_90 = mul_8 = None
        convert_element_type_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
        mul_121: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_18, 1.4285714285714286);  convert_element_type_18 = None
        mul_122: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_40, mul_121);  mm_40 = mul_121 = None
        sigmoid_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
        sub_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_7);  full_8 = None
        mul_123: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub_3);  mm_2 = sub_3 = None
        add_31: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_123, 1);  mul_123 = None
        mul_124: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_7, add_31);  sigmoid_7 = add_31 = None
        mul_125: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_122, mul_124);  mul_122 = mul_124 = None
        mm_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_125, primals_10);  primals_10 = None
        permute_92: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_125, [1, 0]);  mul_125 = None
        mm_43: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_92, mul_5);  permute_92 = mul_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_126: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, mul_4);  mul_4 = None
        mul_127: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, primals_9);  mm_42 = primals_9 = None
        sum_13: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_126, [0], True);  mul_126 = None
        unsqueeze_21: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_13, 0);  sum_13 = None
        view_34: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_21, [384]);  unsqueeze_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_128: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, add_1)
        mul_129: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, rsqrt_1);  mul_127 = None
        sum_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_128, [1], True);  mul_128 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_30, mul_129);  add_30 = mul_129 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_1, 3);  rsqrt_1 = None
        mul_130: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_14, -0.5);  sum_14 = None
        mul_131: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_130, pow_21);  mul_130 = pow_21 = None
        expand_6: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_131, [-1, 384]);  mul_131 = None
        div_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_6, 384);  expand_6 = None
        pow_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
        mul_132: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_22, 2.0);  pow_22 = None
        mul_133: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_6, mul_132);  div_6 = mul_132 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_32, mul_133);  add_32 = mul_133 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_33, primals_8);  primals_8 = None
        permute_93: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_33, [1, 0])
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_45: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_93, view_3);  permute_93 = view_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_35: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_44, [sym_size_int, 6, 64]);  mm_44 = None
        permute_94: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_35, [1, 0, 2]);  view_35 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_95: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_94, [1, 0, 2]);  permute_94 = None
        unsqueeze_22: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_95, 0);  permute_95 = None
        _efficient_attention_backward_3 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_22, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem_3, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_22 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem_3 = convert_element_type = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
        getitem_129: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[0]
        getitem_130: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[1]
        getitem_131: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[2];  _efficient_attention_backward_3 = None
        squeeze_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_131, 0);  getitem_131 = None
        squeeze_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_130, 0);  getitem_130 = None
        squeeze_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_129, 0);  getitem_129 = None
        permute_96: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_13, [1, 0, 2]);  squeeze_13 = None
        permute_97: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_14, [1, 0, 2]);  squeeze_14 = None
        permute_98: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_15, [1, 0, 2]);  squeeze_15 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_99: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_96, [1, 0, 2]);  permute_96 = None
        view_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_99, [sym_size_int, 384]);  permute_99 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_100: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_97, [1, 0, 2]);  permute_97 = None
        view_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_100, [sym_size_int, 384]);  permute_100 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_101: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_98, [1, 0, 2]);  permute_98 = None
        view_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_101, [sym_size_int, 384]);  permute_101 = sym_size_int = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        copy_9: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_40, view_38);  getitem_40 = view_38 = None
        slice_scatter_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy_9, 1, 0, 384);  full_9 = copy_9 = None
        split_34 = torch.ops.aten.split.Tensor(slice_scatter_9, 384, 1)
        getitem_143: "f32[s1, 384][1152, 1]cuda:0" = split_34[1];  split_34 = None
        copy_10: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_143, view_37);  getitem_143 = view_37 = None
        slice_scatter_10: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_9, copy_10, 1, 384, 768);  slice_scatter_9 = copy_10 = None
        split_37 = torch.ops.aten.split.Tensor(slice_scatter_10, 384, 1)
        getitem_153: "f32[s1, 384][1152, 1]cuda:0" = split_37[2];  split_37 = None
        copy_11: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_153, view_36);  getitem_153 = view_36 = None
        slice_scatter_11: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, copy_11, 1, 768, 1152);  slice_scatter_10 = copy_11 = None
        mm_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_11, primals_7);  primals_7 = None
        permute_103: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_11, [1, 0]);  slice_scatter_11 = None
        mm_47: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_103, mul_3);  permute_103 = mul_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
        mul_134: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_19, 1.4285714285714286);  convert_element_type_19 = None
        mul_135: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, mul_134);  mm_46 = mul_134 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_136: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_135, mul);  mul = None
        mul_137: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_135, primals_6);  mul_135 = primals_6 = None
        sum_15: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_136, [0], True);  mul_136 = None
        unsqueeze_23: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_15, 0);  sum_15 = None
        view_39: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_23, [384]);  unsqueeze_23 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_138: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, primals_1)
        mul_139: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, rsqrt);  mul_137 = None
        sum_16: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_138, [1], True);  mul_138 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_33, mul_139);  add_33 = mul_139 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
        mul_140: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_16, -0.5);  sum_16 = None
        mul_141: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_140, pow_23);  mul_140 = pow_23 = None
        expand_7: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_141, [-1, 384]);  mul_141 = None
        div_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_7, 384);  expand_7 = None
        pow_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
        mul_142: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_24, 2.0);  pow_24 = None
        mul_143: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_7, mul_142);  div_7 = mul_142 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_34, mul_143);  add_34 = mul_143 = None
        return (add_35, tangents_2, tangents_3, tangents_4, None, view_39, mm_47, mm_45, view_34, mm_43, mm_41, view_33, mm_39, mm_37, view_28, mm_35, mm_33, view_27, mm_31, mm_29, view_22, mm_27, mm_25, view_21, mm_23, mm_21, view_16, mm_19, mm_17)
        