class joint_fn(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[384][1]cuda:0"; primals_7: "f32[1152, 384][384, 1]cuda:0"; primals_8: "f32[384, 384][384, 1]cuda:0"; primals_9: "f32[384][1]cuda:0"; primals_10: "f32[1024, 384][384, 1]cuda:0"; primals_11: "f32[384, 1024][1024, 1]cuda:0"; primals_12: "f32[384][1]cuda:0"; primals_13: "f32[1152, 384][384, 1]cuda:0"; primals_14: "f32[384, 384][384, 1]cuda:0"; primals_15: "f32[384][1]cuda:0"; primals_16: "f32[1024, 384][384, 1]cuda:0"; primals_17: "f32[384, 1024][1024, 1]cuda:0"; primals_18: "f32[384][1]cuda:0"; primals_19: "f32[1152, 384][384, 1]cuda:0"; primals_20: "f32[384, 384][384, 1]cuda:0"; primals_21: "f32[384][1]cuda:0"; primals_22: "f32[1024, 384][384, 1]cuda:0"; primals_23: "f32[384, 1024][1024, 1]cuda:0"; primals_24: "f32[384][1]cuda:0"; primals_25: "f32[1152, 384][384, 1]cuda:0"; primals_26: "f32[384, 384][384, 1]cuda:0"; primals_27: "f32[384][1]cuda:0"; primals_28: "f32[1024, 384][384, 1]cuda:0"; primals_29: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
    
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
        alias: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
        alias_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
        alias_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
        rand: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
        alias_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
        alias_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
        alias_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_6: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_6, [1, 0, 2]);  alias_6 = None
        alias_7: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_7, [1, 0, 2]);  alias_7 = None
        alias_8: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_8, [1, 0, 2]);  alias_8 = None
        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        alias_11: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
        alias_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
        alias_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_11, 0);  alias_11 = None
        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_12, 0);  alias_12 = None
        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_13, 0);  alias_13 = None
        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
        alias_14: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_3)
        alias_15: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0);  getitem_3 = None
        alias_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        alias_17: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_7);  permute_7 = None
        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_17, [1, 0, 2]);  alias_17 = None
        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  permute_9 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
        alias_18: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_1)
        alias_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
        alias_20: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
        rand_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_1, 0.3);  rand_1 = None
        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        rand_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_2, 0.3);  rand_2 = None
        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  mul_10 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 2)
        mean_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
        add_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_2, 1e-06);  mean_2 = None
        rsqrt_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
        alias_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_2)
        alias_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
        alias_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3, rsqrt_2)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_11, primals_12)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        rand_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_3: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_3, 0.3);  rand_3 = None
        mul_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_3, mul_12);  mul_12 = None
        mul_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, 1.4285714285714286);  mul_13 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        permute_12: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_13, [1, 0])
        mm_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_14, permute_12);  permute_12 = None
        split_1 = torch.ops.aten.split.Tensor(mm_4, 384, 1);  mm_4 = None
        getitem_9: "f32[s1, 384][1152, 1]cuda:0" = split_1[0]
        getitem_10: "f32[s1, 384][1152, 1]cuda:0" = split_1[1]
        getitem_11: "f32[s1, 384][1152, 1]cuda:0" = split_1[2];  split_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_9, [sym_size_int, 6, 64]);  getitem_9 = None
        alias_24: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_4);  view_4 = None
        permute_13: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_10, [sym_size_int, 6, 64]);  getitem_10 = None
        alias_25: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_5);  view_5 = None
        permute_14: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_11, [sym_size_int, 6, 64]);  getitem_11 = None
        alias_26: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_6);  view_6 = None
        permute_15: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_26, [1, 0, 2]);  alias_26 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_13);  permute_13 = None
        permute_16: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_27, [1, 0, 2]);  alias_27 = None
        alias_28: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_14);  permute_14 = None
        permute_17: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_28, [1, 0, 2]);  alias_28 = None
        alias_29: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_15);  permute_15 = None
        permute_18: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_29, [1, 0, 2]);  alias_29 = None
        convert_element_type_2: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        convert_element_type_3: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        alias_32: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_16);  permute_16 = None
        alias_33: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_17);  permute_17 = None
        alias_34: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_18);  permute_18 = None
        unsqueeze_3: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_32, 0);  alias_32 = None
        unsqueeze_4: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_33, 0);  alias_33 = None
        unsqueeze_5: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_34, 0);  alias_34 = None
        _efficient_attention_forward_1 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_3, unsqueeze_4, unsqueeze_5, None, convert_element_type_2, convert_element_type_3, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
        getitem_12: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_1[0]
        getitem_13: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_1[1]
        getitem_14: "i64[][]cuda:0" = _efficient_attention_forward_1[2]
        getitem_15: "i64[][]cuda:0" = _efficient_attention_forward_1[3];  _efficient_attention_forward_1 = None
        alias_35: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_12)
        alias_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_35);  alias_35 = None
        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_12, 0);  getitem_12 = None
        alias_37: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_37, [1, 0, 2]);  alias_37 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        alias_38: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_19);  permute_19 = None
        permute_20: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_38, [1, 0, 2]);  alias_38 = None
        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_20, [sym_size_int, 384]);  permute_20 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        permute_21: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_14, [1, 0])
        mm_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_7, permute_21);  permute_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3, mm_5);  mm_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 2)
        mean_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
        add_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_3, 1e-06);  mean_3 = None
        rsqrt_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
        alias_39: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_3)
        alias_40: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_39);  alias_39 = None
        alias_41: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_40);  alias_40 = None
        mul_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_5, rsqrt_3)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_15)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        permute_22: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_16, [1, 0])
        mm_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_16, permute_22);  permute_22 = None
        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sigmoid_1);  sigmoid_1 = None
        rand_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_4: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_4, 0.3);  rand_4 = None
        mul_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_4, mul_17);  mul_17 = None
        mul_19: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_18, 1.4285714285714286);  mul_18 = None
        permute_23: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_17, [1, 0])
        mm_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_19, permute_23);  permute_23 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        rand_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_5: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_5, 0.3);  rand_5 = None
        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_5, mm_7);  mm_7 = None
        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_20, 1.4285714285714286);  mul_20 = None
        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_21);  mul_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 2)
        mean_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
        add_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_4, 1e-06);  mean_4 = None
        rsqrt_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
        alias_42: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_4)
        alias_43: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_42);  alias_42 = None
        alias_44: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_43);  alias_43 = None
        mul_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_7, rsqrt_4)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, primals_18)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        rand_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_6: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_6, 0.3);  rand_6 = None
        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_6, mul_23);  mul_23 = None
        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_24, 1.4285714285714286);  mul_24 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        permute_24: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_19, [1, 0])
        mm_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_25, permute_24);  permute_24 = None
        split_2 = torch.ops.aten.split.Tensor(mm_8, 384, 1);  mm_8 = None
        getitem_18: "f32[s1, 384][1152, 1]cuda:0" = split_2[0]
        getitem_19: "f32[s1, 384][1152, 1]cuda:0" = split_2[1]
        getitem_20: "f32[s1, 384][1152, 1]cuda:0" = split_2[2];  split_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_8: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_18, [sym_size_int, 6, 64]);  getitem_18 = None
        alias_45: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_8);  view_8 = None
        permute_25: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_45, [1, 0, 2]);  alias_45 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_9: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_19, [sym_size_int, 6, 64]);  getitem_19 = None
        alias_46: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_9);  view_9 = None
        permute_26: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_46, [1, 0, 2]);  alias_46 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_10: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_20, [sym_size_int, 6, 64]);  getitem_20 = None
        alias_47: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_10);  view_10 = None
        permute_27: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_47, [1, 0, 2]);  alias_47 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_48: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_25);  permute_25 = None
        permute_28: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_48, [1, 0, 2]);  alias_48 = None
        alias_49: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_26);  permute_26 = None
        permute_29: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_49, [1, 0, 2]);  alias_49 = None
        alias_50: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_27);  permute_27 = None
        permute_30: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_50, [1, 0, 2]);  alias_50 = None
        convert_element_type_4: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        convert_element_type_5: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        alias_53: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_28);  permute_28 = None
        alias_54: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_29);  permute_29 = None
        alias_55: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_30);  permute_30 = None
        unsqueeze_6: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_53, 0);  alias_53 = None
        unsqueeze_7: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_54, 0);  alias_54 = None
        unsqueeze_8: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_55, 0);  alias_55 = None
        _efficient_attention_forward_2 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_6, unsqueeze_7, unsqueeze_8, None, convert_element_type_4, convert_element_type_5, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
        getitem_21: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_2[0]
        getitem_22: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_2[1]
        getitem_23: "i64[][]cuda:0" = _efficient_attention_forward_2[2]
        getitem_24: "i64[][]cuda:0" = _efficient_attention_forward_2[3];  _efficient_attention_forward_2 = None
        alias_56: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_21)
        alias_57: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_56);  alias_56 = None
        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_21, 0);  getitem_21 = None
        alias_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
        permute_31: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_58, [1, 0, 2]);  alias_58 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        alias_59: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_31);  permute_31 = None
        permute_32: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_59, [1, 0, 2]);  alias_59 = None
        view_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_32, [sym_size_int, 384]);  permute_32 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        permute_33: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_20, [1, 0])
        mm_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_11, permute_33);  permute_33 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mm_9);  mm_9 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 2)
        mean_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
        add_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_5, 1e-06);  mean_5 = None
        rsqrt_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
        alias_60: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_5)
        alias_61: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_60);  alias_60 = None
        alias_62: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_61);  alias_61 = None
        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_9, rsqrt_5)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_26, primals_21)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        permute_34: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_22, [1, 0])
        mm_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_27, permute_34);  permute_34 = None
        sigmoid_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
        mul_28: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sigmoid_2);  sigmoid_2 = None
        rand_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_7: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_7, 0.3);  rand_7 = None
        mul_29: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_7, mul_28);  mul_28 = None
        mul_30: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, 1.4285714285714286);  mul_29 = None
        permute_35: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_23, [1, 0])
        mm_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_30, permute_35);  permute_35 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        rand_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_8: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_8, 0.3);  rand_8 = None
        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_8, mm_11);  mm_11 = None
        mul_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, 1.4285714285714286);  mul_31 = None
        add_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_9, mul_32);  mul_32 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 2)
        mean_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
        add_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_6, 1e-06);  mean_6 = None
        rsqrt_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
        alias_63: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_6)
        alias_64: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_63);  alias_63 = None
        alias_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_64);  alias_64 = None
        mul_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_11, rsqrt_6)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_24)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        rand_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_9: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_9, 0.3);  rand_9 = None
        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_9, mul_34);  mul_34 = None
        mul_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, 1.4285714285714286);  mul_35 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        permute_36: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_25, [1, 0])
        mm_12: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_36, permute_36);  permute_36 = None
        split_3 = torch.ops.aten.split.Tensor(mm_12, 384, 1);  mm_12 = None
        getitem_27: "f32[s1, 384][1152, 1]cuda:0" = split_3[0]
        getitem_28: "f32[s1, 384][1152, 1]cuda:0" = split_3[1]
        getitem_29: "f32[s1, 384][1152, 1]cuda:0" = split_3[2];  split_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_27, [sym_size_int, 6, 64]);  getitem_27 = None
        alias_66: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_12);  view_12 = None
        permute_37: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_66, [1, 0, 2]);  alias_66 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [sym_size_int, 6, 64]);  getitem_28 = None
        alias_67: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_13);  view_13 = None
        permute_38: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_67, [1, 0, 2]);  alias_67 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_14: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_29, [sym_size_int, 6, 64]);  getitem_29 = None
        alias_68: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_14);  view_14 = None
        permute_39: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_68, [1, 0, 2]);  alias_68 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_69: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_37);  permute_37 = None
        permute_40: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_69, [1, 0, 2]);  alias_69 = None
        alias_70: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_38);  permute_38 = None
        permute_41: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_70, [1, 0, 2]);  alias_70 = None
        alias_71: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_39);  permute_39 = None
        permute_42: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_71, [1, 0, 2]);  alias_71 = None
        convert_element_type_6: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        convert_element_type_7: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        alias_74: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_40);  permute_40 = None
        alias_75: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_41);  permute_41 = None
        alias_76: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_42);  permute_42 = None
        unsqueeze_9: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_74, 0);  alias_74 = None
        unsqueeze_10: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_75, 0);  alias_75 = None
        unsqueeze_11: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_76, 0);  alias_76 = None
        _efficient_attention_forward_3 = torch.ops.aten._efficient_attention_forward.default(unsqueeze_9, unsqueeze_10, unsqueeze_11, None, convert_element_type_6, convert_element_type_7, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
        getitem_30: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward_3[0]
        getitem_31: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward_3[1]
        getitem_32: "i64[][]cuda:0" = _efficient_attention_forward_3[2]
        getitem_33: "i64[][]cuda:0" = _efficient_attention_forward_3[3];  _efficient_attention_forward_3 = None
        alias_77: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_30)
        alias_78: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_77);  alias_77 = None
        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_30, 0);  getitem_30 = None
        alias_79: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
        permute_43: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_79, [1, 0, 2]);  alias_79 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        alias_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_43);  permute_43 = None
        permute_44: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_80, [1, 0, 2]);  alias_80 = None
        view_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_44, [sym_size_int, 384]);  permute_44 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        permute_45: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_26, [1, 0])
        mm_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_15, permute_45);  permute_45 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_11, mm_13);  mm_13 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 2)
        mean_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
        add_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_7, 1e-06);  mean_7 = None
        rsqrt_7: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_14);  add_14 = None
        alias_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_7)
        alias_82: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_81);  alias_81 = None
        alias_83: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_82);  alias_82 = None
        mul_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_13, rsqrt_7)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_27)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        permute_46: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_28, [1, 0])
        mm_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_38, permute_46);  permute_46 = None
        sigmoid_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
        mul_39: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sigmoid_3);  sigmoid_3 = None
        rand_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_10: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_10, 0.3);  rand_10 = None
        mul_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_10, mul_39);  mul_39 = None
        mul_41: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_40, 1.4285714285714286);  mul_40 = None
        permute_47: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_29, [1, 0])
        mm_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_41, permute_47);  permute_47 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        rand_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_11: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_11, 0.3);  rand_11 = None
        mul_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_11, mm_15);  mm_15 = None
        mul_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_42, 1.4285714285714286);  mul_42 = None
        add_15: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_13, mul_43);  mul_43 = None
        convert_element_type_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_11, torch.float32);  gt_11 = None
        mul_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_8, 1.4285714285714286);  convert_element_type_8 = None
        mul_45: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_44);  mul_44 = None
        clone: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_45, memory_format = torch.contiguous_format);  mul_45 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone, primals_29);  primals_29 = None
        permute_48: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone, [1, 0]);  clone = None
        mm_17: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_48, mul_41);  permute_48 = mul_41 = None
        convert_element_type_9: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_10, torch.float32);  gt_10 = None
        mul_46: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_9, 1.4285714285714286);  convert_element_type_9 = None
        mul_47: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_16, mul_46);  mm_16 = mul_46 = None
        clone_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_47, memory_format = torch.contiguous_format);  mul_47 = None
        sigmoid_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_14)
        full_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_8, sigmoid_4);  full_8 = None
        mul_48: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_14, sub);  mm_14 = sub = None
        add_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_48, 1);  mul_48 = None
        mul_49: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_4, add_16);  sigmoid_4 = add_16 = None
        mul_50: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_1, mul_49);  clone_1 = mul_49 = None
        mm_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_50, primals_28);  primals_28 = None
        permute_50: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_50, [1, 0]);  mul_50 = None
        mm_19: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_50, mul_38);  permute_50 = mul_38 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_51: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, mul_37);  mul_37 = None
        mul_52: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_18, primals_27);  mm_18 = primals_27 = None
        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_51, [0], True);  mul_51 = None
        unsqueeze_12: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
        view_16: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_12, [384]);  unsqueeze_12 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_53: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, add_13)
        mul_54: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_52, rsqrt_7);  mul_52 = rsqrt_7 = None
        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_53, [1], True);  mul_53 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_54);  tangents_1 = mul_54 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_84: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_83);  alias_83 = None
        alias_85: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_84);  alias_84 = None
        alias_86: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_85);  alias_85 = None
        pow_9: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_86, 3);  alias_86 = None
        mul_55: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_56: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, pow_9);  mul_55 = pow_9 = None
        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_56, [-1, 384]);  mul_56 = None
        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
        pow_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_13, 1.0);  add_13 = None
        mul_57: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_10, 2.0);  pow_10 = None
        mul_58: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_57);  div = mul_57 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_17, mul_58);  add_17 = mul_58 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_18, primals_26);  primals_26 = None
        permute_51: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_18, [1, 0])
        mm_21: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_51, view_15);  permute_51 = view_15 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [sym_size_int, 6, 64]);  mm_20 = None
        alias_87: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_17);  view_17 = None
        permute_52: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_87, [1, 0, 2]);  alias_87 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_88: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_52);  permute_52 = None
        permute_53: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_88, [1, 0, 2]);  alias_88 = None
        alias_89: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_53);  permute_53 = None
        unsqueeze_13: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_89, 0);  alias_89 = None
        alias_90: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_78);  alias_78 = None
        alias_91: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_90);  alias_90 = None
        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_13, unsqueeze_9, unsqueeze_10, unsqueeze_11, None, alias_91, convert_element_type_6, convert_element_type_7, sym_size_int_1, sym_size_int_1, getitem_31, 0.0, getitem_32, getitem_33, 0, False);  unsqueeze_13 = unsqueeze_9 = unsqueeze_10 = unsqueeze_11 = alias_91 = convert_element_type_6 = convert_element_type_7 = getitem_31 = getitem_32 = getitem_33 = None
        getitem_36: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
        getitem_37: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
        getitem_38: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
        squeeze_4: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_38, 0);  getitem_38 = None
        squeeze_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_37, 0);  getitem_37 = None
        squeeze_6: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_36, 0);  getitem_36 = None
        alias_92: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_4);  squeeze_4 = None
        permute_54: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_92, [1, 0, 2]);  alias_92 = None
        alias_93: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_5);  squeeze_5 = None
        permute_55: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_93, [1, 0, 2]);  alias_93 = None
        alias_94: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_6);  squeeze_6 = None
        permute_56: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_94, [1, 0, 2]);  alias_94 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_95: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_54);  permute_54 = None
        permute_57: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_95, [1, 0, 2]);  alias_95 = None
        view_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_57, [sym_size_int, 384]);  permute_57 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_96: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_55);  permute_55 = None
        permute_58: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_96, [1, 0, 2]);  alias_96 = None
        view_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_58, [sym_size_int, 384]);  permute_58 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_97: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_56);  permute_56 = None
        permute_59: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_97, [1, 0, 2]);  alias_97 = None
        view_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_59, [sym_size_int, 384]);  permute_59 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        split_4 = torch.ops.aten.split.Tensor(full_9, 384, 1)
        getitem_40: "f32[s1, 384][1152, 1]cuda:0" = split_4[0];  split_4 = None
        alias_98: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_40);  getitem_40 = None
        alias_99: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_20);  view_20 = None
        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_98, alias_99);  alias_98 = alias_99 = None
        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_9, copy, 1, 0, 384);  full_9 = copy = None
        alias_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_19);  view_19 = None
        split_7 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
        getitem_50: "f32[s1, 384][1152, 1]cuda:0" = split_7[1];  split_7 = None
        alias_103: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_50);  getitem_50 = None
        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_103, alias_102);  alias_103 = alias_102 = None
        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
        alias_106: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_18);  view_18 = None
        split_10 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
        getitem_60: "f32[s1, 384][1152, 1]cuda:0" = split_10[2];  split_10 = None
        alias_107: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_60);  getitem_60 = None
        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_107, alias_106);  alias_107 = alias_106 = None
        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
        mm_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_25);  primals_25 = None
        permute_61: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
        mm_23: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_61, mul_36);  permute_61 = mul_36 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_9, torch.float32);  gt_9 = None
        mul_59: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_10, 1.4285714285714286);  convert_element_type_10 = None
        mul_60: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_22, mul_59);  mm_22 = mul_59 = None
        clone_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_60, memory_format = torch.contiguous_format);  mul_60 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_61: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, mul_33);  mul_33 = None
        mul_62: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, primals_24);  clone_2 = primals_24 = None
        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_61, [0], True);  mul_61 = None
        unsqueeze_14: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
        view_21: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_14, [384]);  unsqueeze_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_63: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, add_11)
        mul_64: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_62, rsqrt_6);  mul_62 = rsqrt_6 = None
        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_63, [1], True);  mul_63 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_18, mul_64);  add_18 = mul_64 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_109: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_65);  alias_65 = None
        alias_110: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_109);  alias_109 = None
        alias_111: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_110);  alias_110 = None
        pow_11: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_111, 3);  alias_111 = None
        mul_65: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
        mul_66: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, pow_11);  mul_65 = pow_11 = None
        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_66, [-1, 384]);  mul_66 = None
        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
        pow_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_11, 1.0);  add_11 = None
        mul_67: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_12, 2.0);  pow_12 = None
        mul_68: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_67);  div_1 = mul_67 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_19, mul_68);  add_19 = mul_68 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_8, torch.float32);  gt_8 = None
        mul_69: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_11, 1.4285714285714286);  convert_element_type_11 = None
        mul_70: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_20, mul_69);  mul_69 = None
        clone_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_70, memory_format = torch.contiguous_format);  mul_70 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_24: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_3, primals_23);  primals_23 = None
        permute_62: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_3, [1, 0]);  clone_3 = None
        mm_25: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_62, mul_30);  permute_62 = mul_30 = None
        convert_element_type_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_7, torch.float32);  gt_7 = None
        mul_71: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_12, 1.4285714285714286);  convert_element_type_12 = None
        mul_72: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_24, mul_71);  mm_24 = mul_71 = None
        clone_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_72, memory_format = torch.contiguous_format);  mul_72 = None
        sigmoid_5: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_10)
        full_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_10, sigmoid_5);  full_10 = None
        mul_73: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, sub_1);  mm_10 = sub_1 = None
        add_21: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_73, 1);  mul_73 = None
        mul_74: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_5, add_21);  sigmoid_5 = add_21 = None
        mul_75: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_4, mul_74);  clone_4 = mul_74 = None
        mm_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_75, primals_22);  primals_22 = None
        permute_64: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_75, [1, 0]);  mul_75 = None
        mm_27: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_64, mul_27);  permute_64 = mul_27 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_76: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, mul_26);  mul_26 = None
        mul_77: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_26, primals_21);  mm_26 = primals_21 = None
        sum_5: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_76, [0], True);  mul_76 = None
        unsqueeze_15: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_5, 0);  sum_5 = None
        view_22: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_15, [384]);  unsqueeze_15 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_78: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, add_9)
        mul_79: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_77, rsqrt_5);  mul_77 = rsqrt_5 = None
        sum_6: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_78, [1], True);  mul_78 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_20, mul_79);  add_20 = mul_79 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_112: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_62);  alias_62 = None
        alias_113: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_112);  alias_112 = None
        alias_114: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_113);  alias_113 = None
        pow_13: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_114, 3);  alias_114 = None
        mul_80: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_6, -0.5);  sum_6 = None
        mul_81: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_80, pow_13);  mul_80 = pow_13 = None
        expand_2: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_81, [-1, 384]);  mul_81 = None
        div_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_2, 384);  expand_2 = None
        pow_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_9, 1.0);  add_9 = None
        mul_82: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_14, 2.0);  pow_14 = None
        mul_83: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_2, mul_82);  div_2 = mul_82 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_23: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_22, mul_83);  add_22 = mul_83 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_23, primals_20);  primals_20 = None
        permute_65: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_23, [1, 0])
        mm_29: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_65, view_11);  permute_65 = view_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_28, [sym_size_int, 6, 64]);  mm_28 = None
        alias_115: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_23);  view_23 = None
        permute_66: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_115, [1, 0, 2]);  alias_115 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_116: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_66);  permute_66 = None
        permute_67: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_116, [1, 0, 2]);  alias_116 = None
        alias_117: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_67);  permute_67 = None
        unsqueeze_16: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_117, 0);  alias_117 = None
        alias_118: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_57);  alias_57 = None
        alias_119: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_118);  alias_118 = None
        _efficient_attention_backward_1 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_16, unsqueeze_6, unsqueeze_7, unsqueeze_8, None, alias_119, convert_element_type_4, convert_element_type_5, sym_size_int_1, sym_size_int_1, getitem_22, 0.0, getitem_23, getitem_24, 0, False);  unsqueeze_16 = unsqueeze_6 = unsqueeze_7 = unsqueeze_8 = alias_119 = convert_element_type_4 = convert_element_type_5 = getitem_22 = getitem_23 = getitem_24 = None
        getitem_67: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[0]
        getitem_68: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[1]
        getitem_69: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_1[2];  _efficient_attention_backward_1 = None
        squeeze_7: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_69, 0);  getitem_69 = None
        squeeze_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_68, 0);  getitem_68 = None
        squeeze_9: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_67, 0);  getitem_67 = None
        alias_120: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_7);  squeeze_7 = None
        permute_68: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_120, [1, 0, 2]);  alias_120 = None
        alias_121: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_8);  squeeze_8 = None
        permute_69: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_121, [1, 0, 2]);  alias_121 = None
        alias_122: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_9);  squeeze_9 = None
        permute_70: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_122, [1, 0, 2]);  alias_122 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_123: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_68);  permute_68 = None
        permute_71: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_123, [1, 0, 2]);  alias_123 = None
        view_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_71, [sym_size_int, 384]);  permute_71 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_124: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_69);  permute_69 = None
        permute_72: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_124, [1, 0, 2]);  alias_124 = None
        view_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_72, [sym_size_int, 384]);  permute_72 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_125: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_70);  permute_70 = None
        permute_73: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_125, [1, 0, 2]);  alias_125 = None
        view_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_73, [sym_size_int, 384]);  permute_73 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_11: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        split_13 = torch.ops.aten.split.Tensor(full_11, 384, 1)
        getitem_71: "f32[s1, 384][1152, 1]cuda:0" = split_13[0];  split_13 = None
        alias_126: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_71);  getitem_71 = None
        alias_127: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_26);  view_26 = None
        copy_3: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_126, alias_127);  alias_126 = alias_127 = None
        slice_scatter_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_11, copy_3, 1, 0, 384);  full_11 = copy_3 = None
        alias_130: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_25);  view_25 = None
        split_16 = torch.ops.aten.split.Tensor(slice_scatter_3, 384, 1)
        getitem_81: "f32[s1, 384][1152, 1]cuda:0" = split_16[1];  split_16 = None
        alias_131: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_81);  getitem_81 = None
        copy_4: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_131, alias_130);  alias_131 = alias_130 = None
        slice_scatter_4: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, copy_4, 1, 384, 768);  slice_scatter_3 = copy_4 = None
        alias_134: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_24);  view_24 = None
        split_19 = torch.ops.aten.split.Tensor(slice_scatter_4, 384, 1)
        getitem_91: "f32[s1, 384][1152, 1]cuda:0" = split_19[2];  split_19 = None
        alias_135: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_91);  getitem_91 = None
        copy_5: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_135, alias_134);  alias_135 = alias_134 = None
        slice_scatter_5: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_4, copy_5, 1, 768, 1152);  slice_scatter_4 = copy_5 = None
        mm_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_5, primals_19);  primals_19 = None
        permute_75: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_5, [1, 0]);  slice_scatter_5 = None
        mm_31: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_75, mul_25);  permute_75 = mul_25 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_13: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_6, torch.float32);  gt_6 = None
        mul_84: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_13, 1.4285714285714286);  convert_element_type_13 = None
        mul_85: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_30, mul_84);  mm_30 = mul_84 = None
        clone_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_85, memory_format = torch.contiguous_format);  mul_85 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_86: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_5, mul_22);  mul_22 = None
        mul_87: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_5, primals_18);  clone_5 = primals_18 = None
        sum_7: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_86, [0], True);  mul_86 = None
        unsqueeze_17: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_7, 0);  sum_7 = None
        view_27: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_17, [384]);  unsqueeze_17 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_88: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, add_7)
        mul_89: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_87, rsqrt_4);  mul_87 = rsqrt_4 = None
        sum_8: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_88, [1], True);  mul_88 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_23, mul_89);  add_23 = mul_89 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_137: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_44);  alias_44 = None
        alias_138: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_137);  alias_137 = None
        alias_139: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_138);  alias_138 = None
        pow_15: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_139, 3);  alias_139 = None
        mul_90: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_8, -0.5);  sum_8 = None
        mul_91: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_90, pow_15);  mul_90 = pow_15 = None
        expand_3: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_91, [-1, 384]);  mul_91 = None
        div_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_3, 384);  expand_3 = None
        pow_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_7, 1.0);  add_7 = None
        mul_92: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_16, 2.0);  pow_16 = None
        mul_93: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_3, mul_92);  div_3 = mul_92 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_24, mul_93);  add_24 = mul_93 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_14: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_5, torch.float32);  gt_5 = None
        mul_94: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_14, 1.4285714285714286);  convert_element_type_14 = None
        mul_95: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_25, mul_94);  mul_94 = None
        clone_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_95, memory_format = torch.contiguous_format);  mul_95 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_32: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_6, primals_17);  primals_17 = None
        permute_76: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_6, [1, 0]);  clone_6 = None
        mm_33: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_76, mul_19);  permute_76 = mul_19 = None
        convert_element_type_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_4, torch.float32);  gt_4 = None
        mul_96: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_15, 1.4285714285714286);  convert_element_type_15 = None
        mul_97: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_32, mul_96);  mm_32 = mul_96 = None
        clone_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_97, memory_format = torch.contiguous_format);  mul_97 = None
        sigmoid_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_6)
        full_12: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_12, sigmoid_6);  full_12 = None
        mul_98: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, sub_2);  mm_6 = sub_2 = None
        add_26: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_98, 1);  mul_98 = None
        mul_99: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_6, add_26);  sigmoid_6 = add_26 = None
        mul_100: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_7, mul_99);  clone_7 = mul_99 = None
        mm_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_100, primals_16);  primals_16 = None
        permute_78: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_100, [1, 0]);  mul_100 = None
        mm_35: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_78, mul_16);  permute_78 = mul_16 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_101: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, mul_15);  mul_15 = None
        mul_102: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_34, primals_15);  mm_34 = primals_15 = None
        sum_9: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_101, [0], True);  mul_101 = None
        unsqueeze_18: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_9, 0);  sum_9 = None
        view_28: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_18, [384]);  unsqueeze_18 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_103: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, add_5)
        mul_104: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_102, rsqrt_3);  mul_102 = rsqrt_3 = None
        sum_10: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_103, [1], True);  mul_103 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_25, mul_104);  add_25 = mul_104 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_140: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_41);  alias_41 = None
        alias_141: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_140);  alias_140 = None
        alias_142: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_141);  alias_141 = None
        pow_17: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_142, 3);  alias_142 = None
        mul_105: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_10, -0.5);  sum_10 = None
        mul_106: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_105, pow_17);  mul_105 = pow_17 = None
        expand_4: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_106, [-1, 384]);  mul_106 = None
        div_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_4, 384);  expand_4 = None
        pow_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_5, 1.0);  add_5 = None
        mul_107: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_18, 2.0);  pow_18 = None
        mul_108: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_4, mul_107);  div_4 = mul_107 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_27, mul_108);  add_27 = mul_108 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_28, primals_14);  primals_14 = None
        permute_79: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_28, [1, 0])
        mm_37: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_79, view_7);  permute_79 = view_7 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_29: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_36, [sym_size_int, 6, 64]);  mm_36 = None
        alias_143: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_29);  view_29 = None
        permute_80: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_143, [1, 0, 2]);  alias_143 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_144: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_80);  permute_80 = None
        permute_81: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_144, [1, 0, 2]);  alias_144 = None
        alias_145: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_81);  permute_81 = None
        unsqueeze_19: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_145, 0);  alias_145 = None
        alias_146: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_36);  alias_36 = None
        alias_147: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_146);  alias_146 = None
        _efficient_attention_backward_2 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_19, unsqueeze_3, unsqueeze_4, unsqueeze_5, None, alias_147, convert_element_type_2, convert_element_type_3, sym_size_int_1, sym_size_int_1, getitem_13, 0.0, getitem_14, getitem_15, 0, False);  unsqueeze_19 = unsqueeze_3 = unsqueeze_4 = unsqueeze_5 = alias_147 = convert_element_type_2 = convert_element_type_3 = getitem_13 = getitem_14 = getitem_15 = None
        getitem_98: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[0]
        getitem_99: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[1]
        getitem_100: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_2[2];  _efficient_attention_backward_2 = None
        squeeze_10: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_100, 0);  getitem_100 = None
        squeeze_11: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_99, 0);  getitem_99 = None
        squeeze_12: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_98, 0);  getitem_98 = None
        alias_148: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_10);  squeeze_10 = None
        permute_82: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_148, [1, 0, 2]);  alias_148 = None
        alias_149: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_11);  squeeze_11 = None
        permute_83: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_149, [1, 0, 2]);  alias_149 = None
        alias_150: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_12);  squeeze_12 = None
        permute_84: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_150, [1, 0, 2]);  alias_150 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_151: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_82);  permute_82 = None
        permute_85: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_151, [1, 0, 2]);  alias_151 = None
        view_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_85, [sym_size_int, 384]);  permute_85 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_152: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_83);  permute_83 = None
        permute_86: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_152, [1, 0, 2]);  alias_152 = None
        view_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_86, [sym_size_int, 384]);  permute_86 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_153: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_84);  permute_84 = None
        permute_87: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_153, [1, 0, 2]);  alias_153 = None
        view_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_87, [sym_size_int, 384]);  permute_87 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_13: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        split_22 = torch.ops.aten.split.Tensor(full_13, 384, 1)
        getitem_102: "f32[s1, 384][1152, 1]cuda:0" = split_22[0];  split_22 = None
        alias_154: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_102);  getitem_102 = None
        alias_155: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_32);  view_32 = None
        copy_6: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_154, alias_155);  alias_154 = alias_155 = None
        slice_scatter_6: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_13, copy_6, 1, 0, 384);  full_13 = copy_6 = None
        alias_158: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_31);  view_31 = None
        split_25 = torch.ops.aten.split.Tensor(slice_scatter_6, 384, 1)
        getitem_112: "f32[s1, 384][1152, 1]cuda:0" = split_25[1];  split_25 = None
        alias_159: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_112);  getitem_112 = None
        copy_7: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_159, alias_158);  alias_159 = alias_158 = None
        slice_scatter_7: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_6, copy_7, 1, 384, 768);  slice_scatter_6 = copy_7 = None
        alias_162: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_30);  view_30 = None
        split_28 = torch.ops.aten.split.Tensor(slice_scatter_7, 384, 1)
        getitem_122: "f32[s1, 384][1152, 1]cuda:0" = split_28[2];  split_28 = None
        alias_163: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_122);  getitem_122 = None
        copy_8: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_163, alias_162);  alias_163 = alias_162 = None
        slice_scatter_8: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_7, copy_8, 1, 768, 1152);  slice_scatter_7 = copy_8 = None
        mm_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_8, primals_13);  primals_13 = None
        permute_89: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_8, [1, 0]);  slice_scatter_8 = None
        mm_39: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_89, mul_14);  permute_89 = mul_14 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_16: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_3, torch.float32);  gt_3 = None
        mul_109: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_16, 1.4285714285714286);  convert_element_type_16 = None
        mul_110: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_38, mul_109);  mm_38 = mul_109 = None
        clone_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_110, memory_format = torch.contiguous_format);  mul_110 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_111: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_8, mul_11);  mul_11 = None
        mul_112: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_8, primals_12);  clone_8 = primals_12 = None
        sum_11: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_111, [0], True);  mul_111 = None
        unsqueeze_20: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_11, 0);  sum_11 = None
        view_33: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_20, [384]);  unsqueeze_20 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_113: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, add_3)
        mul_114: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_112, rsqrt_2);  mul_112 = rsqrt_2 = None
        sum_12: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_113, [1], True);  mul_113 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_28, mul_114);  add_28 = mul_114 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_165: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_23);  alias_23 = None
        alias_166: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_165);  alias_165 = None
        alias_167: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_166);  alias_166 = None
        pow_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_167, 3);  alias_167 = None
        mul_115: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_12, -0.5);  sum_12 = None
        mul_116: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, pow_19);  mul_115 = pow_19 = None
        expand_5: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_116, [-1, 384]);  mul_116 = None
        div_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_5, 384);  expand_5 = None
        pow_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_3, 1.0);  add_3 = None
        mul_117: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_20, 2.0);  pow_20 = None
        mul_118: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_5, mul_117);  div_5 = mul_117 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_29, mul_118);  add_29 = mul_118 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_17: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
        mul_119: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_17, 1.4285714285714286);  convert_element_type_17 = None
        mul_120: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_30, mul_119);  mul_119 = None
        clone_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_120, memory_format = torch.contiguous_format);  mul_120 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_40: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone_9, primals_11);  primals_11 = None
        permute_90: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone_9, [1, 0]);  clone_9 = None
        mm_41: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_90, mul_8);  permute_90 = mul_8 = None
        convert_element_type_18: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
        mul_121: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_18, 1.4285714285714286);  convert_element_type_18 = None
        mul_122: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_40, mul_121);  mm_40 = mul_121 = None
        clone_10: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_122, memory_format = torch.contiguous_format);  mul_122 = None
        sigmoid_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
        full_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_14, sigmoid_7);  full_14 = None
        mul_123: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub_3);  mm_2 = sub_3 = None
        add_31: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_123, 1);  mul_123 = None
        mul_124: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_7, add_31);  sigmoid_7 = add_31 = None
        mul_125: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_10, mul_124);  clone_10 = mul_124 = None
        mm_42: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_125, primals_10);  primals_10 = None
        permute_92: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_125, [1, 0]);  mul_125 = None
        mm_43: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_92, mul_5);  permute_92 = mul_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_126: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, mul_4);  mul_4 = None
        mul_127: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_42, primals_9);  mm_42 = primals_9 = None
        sum_13: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_126, [0], True);  mul_126 = None
        unsqueeze_21: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_13, 0);  sum_13 = None
        view_34: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_21, [384]);  unsqueeze_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_128: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, add_1)
        mul_129: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_127, rsqrt_1);  mul_127 = rsqrt_1 = None
        sum_14: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_128, [1], True);  mul_128 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_32: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_30, mul_129);  add_30 = mul_129 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_168: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
        alias_169: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_168);  alias_168 = None
        alias_170: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_169);  alias_169 = None
        pow_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_170, 3);  alias_170 = None
        mul_130: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_14, -0.5);  sum_14 = None
        mul_131: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_130, pow_21);  mul_130 = pow_21 = None
        expand_6: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_131, [-1, 384]);  mul_131 = None
        div_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_6, 384);  expand_6 = None
        pow_22: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
        mul_132: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_22, 2.0);  pow_22 = None
        mul_133: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_6, mul_132);  div_6 = mul_132 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_33: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_32, mul_133);  add_32 = mul_133 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_44: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_33, primals_8);  primals_8 = None
        permute_93: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_33, [1, 0])
        mm_45: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_93, view_3);  permute_93 = view_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_35: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_44, [sym_size_int, 6, 64]);  mm_44 = None
        alias_171: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_35);  view_35 = None
        permute_94: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_171, [1, 0, 2]);  alias_171 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_172: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_94);  permute_94 = None
        permute_95: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_172, [1, 0, 2]);  alias_172 = None
        alias_173: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_95);  permute_95 = None
        unsqueeze_22: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_173, 0);  alias_173 = None
        alias_174: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
        alias_175: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_174);  alias_174 = None
        _efficient_attention_backward_3 = torch.ops.aten._efficient_attention_backward.default(unsqueeze_22, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_175, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_22 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_175 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
        getitem_129: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[0]
        getitem_130: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[1]
        getitem_131: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward_3[2];  _efficient_attention_backward_3 = None
        squeeze_13: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_131, 0);  getitem_131 = None
        squeeze_14: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_130, 0);  getitem_130 = None
        squeeze_15: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_129, 0);  getitem_129 = None
        alias_176: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_13);  squeeze_13 = None
        permute_96: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_176, [1, 0, 2]);  alias_176 = None
        alias_177: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_14);  squeeze_14 = None
        permute_97: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_177, [1, 0, 2]);  alias_177 = None
        alias_178: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_15);  squeeze_15 = None
        permute_98: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_178, [1, 0, 2]);  alias_178 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_179: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_96);  permute_96 = None
        permute_99: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_179, [1, 0, 2]);  alias_179 = None
        view_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_99, [sym_size_int, 384]);  permute_99 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_180: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_97);  permute_97 = None
        permute_100: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_180, [1, 0, 2]);  alias_180 = None
        view_37: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_100, [sym_size_int, 384]);  permute_100 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_181: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_98);  permute_98 = None
        permute_101: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_181, [1, 0, 2]);  alias_181 = None
        view_38: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_101, [sym_size_int, 384]);  permute_101 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_15: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
        split_31 = torch.ops.aten.split.Tensor(full_15, 384, 1)
        getitem_133: "f32[s1, 384][1152, 1]cuda:0" = split_31[0];  split_31 = None
        alias_182: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_133);  getitem_133 = None
        alias_183: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_38);  view_38 = None
        copy_9: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_182, alias_183);  alias_182 = alias_183 = None
        slice_scatter_9: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_15, copy_9, 1, 0, 384);  full_15 = copy_9 = None
        alias_186: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_37);  view_37 = None
        split_34 = torch.ops.aten.split.Tensor(slice_scatter_9, 384, 1)
        getitem_143: "f32[s1, 384][1152, 1]cuda:0" = split_34[1];  split_34 = None
        alias_187: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_143);  getitem_143 = None
        copy_10: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_187, alias_186);  alias_187 = alias_186 = None
        slice_scatter_10: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_9, copy_10, 1, 384, 768);  slice_scatter_9 = copy_10 = None
        alias_190: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_36);  view_36 = None
        split_37 = torch.ops.aten.split.Tensor(slice_scatter_10, 384, 1)
        getitem_153: "f32[s1, 384][1152, 1]cuda:0" = split_37[2];  split_37 = None
        alias_191: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_153);  getitem_153 = None
        copy_11: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_191, alias_190);  alias_191 = alias_190 = None
        slice_scatter_11: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, copy_11, 1, 768, 1152);  slice_scatter_10 = copy_11 = None
        mm_46: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_11, primals_7);  primals_7 = None
        permute_103: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_11, [1, 0]);  slice_scatter_11 = None
        mm_47: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_103, mul_3);  permute_103 = mul_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
        mul_134: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_19, 1.4285714285714286);  convert_element_type_19 = None
        mul_135: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_46, mul_134);  mm_46 = mul_134 = None
        clone_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_135, memory_format = torch.contiguous_format);  mul_135 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_136: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_11, mul);  mul = None
        mul_137: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_11, primals_6);  clone_11 = primals_6 = None
        sum_15: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_136, [0], True);  mul_136 = None
        unsqueeze_23: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_15, 0);  sum_15 = None
        view_39: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_23, [384]);  unsqueeze_23 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_138: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, primals_1)
        mul_139: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_137, rsqrt);  mul_137 = rsqrt = None
        sum_16: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_138, [1], True);  mul_138 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_33, mul_139);  add_33 = mul_139 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_193: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        alias_194: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_193);  alias_193 = None
        alias_195: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_194);  alias_194 = None
        pow_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_195, 3);  alias_195 = None
        mul_140: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_16, -0.5);  sum_16 = None
        mul_141: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_140, pow_23);  mul_140 = pow_23 = None
        expand_7: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_141, [-1, 384]);  mul_141 = None
        div_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_7, 384);  expand_7 = None
        pow_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
        mul_142: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_24, 2.0);  pow_24 = None
        mul_143: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_7, mul_142);  div_7 = mul_142 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_34, mul_143);  add_34 = mul_143 = None
        return pytree.tree_unflatten([add_15, primals_2, primals_3, primals_4, add_35, tangents_2, tangents_3, tangents_4, None, view_39, mm_47, mm_45, view_34, mm_43, mm_41, view_33, mm_39, mm_37, view_28, mm_35, mm_33, view_27, mm_31, mm_29, view_22, mm_27, mm_25, view_21, mm_23, mm_21, view_16, mm_19, mm_17], self._out_spec)
        