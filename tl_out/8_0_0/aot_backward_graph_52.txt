class GraphModule(torch.nn.Module):
    def forward(self, sym_size_int: "Sym(s1)", sym_size_int_1: "Sym(s4)", primals_1: "f32[s1, 384][384, 1]cuda:0", primals_6: "f32[384][1]cuda:0", primals_7: "f32[1152, 384][384, 1]cuda:0", primals_8: "f32[384, 384][384, 1]cuda:0", primals_9: "f32[384][1]cuda:0", primals_10: "f32[1024, 384][384, 1]cuda:0", primals_11: "f32[384, 1024][1024, 1]cuda:0", rsqrt: "f32[s1, 1][1, 1]cuda:0", gt: "b8[s1, 384][384, 1]cuda:0", mul_3: "f32[s1, 384][384, 1]cuda:0", convert_element_type: "i32[257][1]cuda:0", unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0", getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0", getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0", getitem_5: "i64[][]cuda:0", getitem_6: "i64[][]cuda:0", mm_1: "f32[s1, 384][384, 1]cuda:0", rsqrt_1: "f32[s1, 1][1, 1]cuda:0", mul_5: "f32[s1, 384][384, 1]cuda:0", mm_2: "f32[s1, 1024][1024, 1]cuda:0", gt_1: "b8[s1, 1024][1024, 1]cuda:0", mul_8: "f32[s1, 1024][1024, 1]cuda:0", gt_2: "b8[s1, 384][384, 1]cuda:0", tangents_1: "f32[s1, 384][384, 1]cuda:0", tangents_2: "i64[257][1]cuda:0", tangents_3: "f32[s3, 0][1, 1]cuda:0", tangents_4: "f32[s4, 0][1, 1]cuda:0"):
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        convert_element_type_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, 1.4285714285714286);  convert_element_type_2 = None
        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_11);  mul_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_12, primals_11);  primals_11 = None
        permute_12: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(mul_12, [1, 0]);  mul_12 = None
        mm_5: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_12, mul_8);  permute_12 = mul_8 = None
        convert_element_type_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
        mul_13: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_3, 1.4285714285714286);  convert_element_type_3 = None
        mul_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_4, mul_13);  mm_4 = mul_13 = None
        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
        full_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_2, sigmoid_1);  full_2 = None
        mul_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub);  mm_2 = sub = None
        add_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_15, 1);  mul_15 = None
        mul_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_4);  sigmoid_1 = add_4 = None
        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_14, mul_16);  mul_14 = mul_16 = None
        mm_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_17, primals_10);  primals_10 = None
        permute_14: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_17, [1, 0]);  mul_17 = None
        mm_7: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_14, mul_5);  permute_14 = mul_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, mul_4);  mul_4 = None
        mul_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, primals_9);  mm_6 = primals_9 = None
        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_18, [0], True);  mul_18 = None
        unsqueeze_3: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
        view_4: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_3, [384]);  unsqueeze_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, add_1)
        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, rsqrt_1);  mul_19 = None
        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_20, [1], True);  mul_20 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_21);  tangents_1 = mul_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt_1, 3);  rsqrt_1 = None
        mul_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, pow_3);  mul_22 = pow_3 = None
        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_23, [-1, 384]);  mul_23 = None
        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_4, 2.0);  pow_4 = None
        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_24);  div = mul_24 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_25);  add_5 = mul_25 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_6, primals_8);  primals_8 = None
        permute_15: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_6, [1, 0])
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0)
        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze, [1, 0, 2]);  squeeze = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0, 2]);  permute_7 = None
        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_9: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_15, view_3);  permute_15 = view_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [sym_size_int, 6, 64]);  mm_8 = None
        permute_16: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [1, 0, 2]);  view_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        permute_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_16, [1, 0, 2]);  permute_16 = None
        unsqueeze_4: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(permute_17, 0);  permute_17 = None
        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_4, unsqueeze, unsqueeze_1, unsqueeze_2, None, getitem_3, convert_element_type, convert_element_type, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_4 = unsqueeze = unsqueeze_1 = unsqueeze_2 = getitem_3 = convert_element_type = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
        getitem_9: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
        getitem_10: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
        getitem_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_11, 0);  getitem_11 = None
        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_10, 0);  getitem_10 = None
        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_9, 0);  getitem_9 = None
        permute_18: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_1, [1, 0, 2]);  squeeze_1 = None
        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_2, [1, 0, 2]);  squeeze_2 = None
        permute_20: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(squeeze_3, [1, 0, 2]);  squeeze_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_21: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_18, [1, 0, 2]);  permute_18 = None
        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_21, [sym_size_int, 384]);  permute_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_22: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_19, [1, 0, 2]);  permute_19 = None
        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_22, [sym_size_int, 384]);  permute_22 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        permute_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_20, [1, 0, 2]);  permute_20 = None
        view_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_23, [sym_size_int, 384]);  permute_23 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
        split_1 = torch.ops.aten.split.Tensor(full_3, 384, 1)
        getitem_13: "f32[s1, 384][1152, 1]cuda:0" = split_1[0];  split_1 = None
        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_13, view_8);  getitem_13 = view_8 = None
        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, copy, 1, 0, 384);  full_3 = copy = None
        split_4 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
        getitem_23: "f32[s1, 384][1152, 1]cuda:0" = split_4[1];  split_4 = None
        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_23, view_7);  getitem_23 = view_7 = None
        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
        split_7 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
        getitem_33: "f32[s1, 384][1152, 1]cuda:0" = split_7[2];  split_7 = None
        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(getitem_33, view_6);  getitem_33 = view_6 = None
        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
        mm_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_7);  primals_7 = None
        permute_25: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
        mm_11: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_25, mul_3);  permute_25 = mul_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_4, 1.4285714285714286);  convert_element_type_4 = None
        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, mul_26);  mm_10 = mul_26 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_27, mul);  mul = None
        mul_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_27, primals_6);  mul_27 = primals_6 = None
        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_28, [0], True);  mul_28 = None
        unsqueeze_5: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
        view_9: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_5, [384]);  unsqueeze_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, primals_1)
        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, rsqrt);  mul_29 = None
        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_30, [1], True);  mul_30 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6, mul_31);  add_6 = mul_31 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(rsqrt, 3);  rsqrt = None
        mul_32: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
        mul_33: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_32, pow_5);  mul_32 = pow_5 = None
        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_33, [-1, 384]);  mul_33 = None
        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_6, 2.0);  pow_6 = None
        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_34);  div_1 = mul_34 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mul_35);  add_7 = mul_35 = None
        return (add_8, tangents_2, tangents_3, tangents_4, None, view_9, mm_11, mm_9, view_4, mm_7, mm_5)
        