class joint_fn(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "f32[s1, 384][384, 1]cuda:0"; primals_2: "i64[257][1]cuda:0"; primals_3: "f32[s3, 0][1, 1]cuda:0"; primals_4: "f32[s4, 0][1, 1]cuda:0"; primals_5: "Sym(s0)"; primals_6: "f32[384][1]cuda:0"; primals_7: "f32[1152, 384][384, 1]cuda:0"; primals_8: "f32[384, 384][384, 1]cuda:0"; primals_9: "f32[384][1]cuda:0"; primals_10: "f32[1024, 384][384, 1]cuda:0"; primals_11: "f32[384, 1024][1024, 1]cuda:0"; tangents_1: "f32[s1, 384][384, 1]cuda:0"; tangents_2: "i64[257][1]cuda:0"; tangents_3: "f32[s3, 0][1, 1]cuda:0"; tangents_4: "f32[s4, 0][1, 1]cuda:0"; 
    
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, tangents_1, tangents_2, tangents_3, tangents_4, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 2)
        mean: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
        add: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
        rsqrt: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
        alias: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt)
        alias_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
        alias_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        mul: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(primals_1, rsqrt)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_6)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        sym_size_int: "Sym(s1)" = torch.ops.aten.sym_size.int(primals_1, 0)
        rand: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand, 0.3);  rand = None
        mul_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt, mul_1);  mul_1 = None
        mul_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, 1.4285714285714286);  mul_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        permute: "f32[384, 1152][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_7, [1, 0])
        mm: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.mm.default(mul_3, permute);  permute = None
        split = torch.ops.aten.split.Tensor(mm, 384, 1);  mm = None
        getitem: "f32[s1, 384][1152, 1]cuda:0" = split[0]
        getitem_1: "f32[s1, 384][1152, 1]cuda:0" = split[1]
        getitem_2: "f32[s1, 384][1152, 1]cuda:0" = split[2];  split = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem, [sym_size_int, 6, 64]);  getitem = None
        alias_3: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view);  view = None
        permute_1: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_3, [1, 0, 2]);  alias_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_1: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_1, [sym_size_int, 6, 64]);  getitem_1 = None
        alias_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_1);  view_1 = None
        permute_2: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_4, [1, 0, 2]);  alias_4 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        view_2: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [sym_size_int, 6, 64]);  getitem_2 = None
        alias_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_2);  view_2 = None
        permute_3: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.permute.default(alias_5, [1, 0, 2]);  alias_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_6: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_1);  permute_1 = None
        permute_4: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_6, [1, 0, 2]);  alias_6 = None
        alias_7: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_2);  permute_2 = None
        permute_5: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_7, [1, 0, 2]);  alias_7 = None
        alias_8: "f32[6, s1, 64][64, 1152, 1]cuda:0" = torch.ops.aten.alias.default(permute_3);  permute_3 = None
        permute_6: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_8, [1, 0, 2]);  alias_8 = None
        convert_element_type: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        convert_element_type_1: "i32[257][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_2, torch.int32)
        alias_11: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_4);  permute_4 = None
        alias_12: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_5);  permute_5 = None
        alias_13: "f32[s1, 6, 64][1152, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_6);  permute_6 = None
        unsqueeze: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_11, 0);  alias_11 = None
        unsqueeze_1: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_12, 0);  alias_12 = None
        unsqueeze_2: "f32[1, s1, 6, 64][1152*s1, 1152, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_13, 0);  alias_13 = None
        sym_size_int_1: "Sym(s4)" = torch.ops.aten.sym_size.int(primals_4, 0)
        _efficient_attention_forward = torch.ops.aten._efficient_attention_forward.default(unsqueeze, unsqueeze_1, unsqueeze_2, None, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, 0.0, 0, True)
        getitem_3: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_forward[0]
        getitem_4: "f32[256, 6, 32*CeilToInt(IntTrueDiv(s4, 32))][192*CeilToInt(IntTrueDiv(s4, 32)), 32*CeilToInt(IntTrueDiv(s4, 32)), 1]cuda:0" = _efficient_attention_forward[1]
        getitem_5: "i64[][]cuda:0" = _efficient_attention_forward[2]
        getitem_6: "i64[][]cuda:0" = _efficient_attention_forward[3];  _efficient_attention_forward = None
        alias_14: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(getitem_3)
        alias_15: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
        squeeze: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_3, 0);  getitem_3 = None
        alias_16: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze);  squeeze = None
        permute_7: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_16, [1, 0, 2]);  alias_16 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        alias_17: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_7);  permute_7 = None
        permute_8: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_17, [1, 0, 2]);  alias_17 = None
        view_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_8, [sym_size_int, 384]);  permute_8 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        permute_9: "f32[384, 384][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_8, [1, 0])
        mm_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(view_3, permute_9);  permute_9 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        add_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(primals_1, mm_1);  mm_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 2)
        mean_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
        add_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
        rsqrt_1: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
        alias_18: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_1)
        alias_19: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
        alias_20: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
        mul_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_1, rsqrt_1)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_9)
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        permute_10: "f32[384, 1024][1, 384]cuda:0" = torch.ops.aten.permute.default(primals_10, [1, 0])
        mm_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(mul_5, permute_10);  permute_10 = None
        sigmoid: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
        mul_6: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sigmoid);  sigmoid = None
        rand_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 1024], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_1: "b8[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_1, 0.3);  rand_1 = None
        mul_7: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_1, mul_6);  mul_6 = None
        mul_8: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_7, 1.4285714285714286);  mul_7 = None
        permute_11: "f32[1024, 384][1, 1024]cuda:0" = torch.ops.aten.permute.default(primals_11, [1, 0])
        mm_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_8, permute_11);  permute_11 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        rand_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.rand.default([sym_size_int, 384], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        gt_2: "b8[s1, 384][384, 1]cuda:0" = torch.ops.aten.gt.Scalar(rand_2, 0.3);  rand_2 = None
        mul_9: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(gt_2, mm_3);  mm_3 = None
        mul_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_9, 1.4285714285714286);  mul_9 = None
        add_3: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_1, mul_10);  mul_10 = None
        convert_element_type_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_2, torch.float32);  gt_2 = None
        mul_11: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2, 1.4285714285714286);  convert_element_type_2 = None
        mul_12: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, mul_11);  mul_11 = None
        clone: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_12, memory_format = torch.contiguous_format);  mul_12 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        mm_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(clone, primals_11);  primals_11 = None
        permute_12: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(clone, [1, 0]);  clone = None
        mm_5: "f32[384, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_12, mul_8);  permute_12 = mul_8 = None
        convert_element_type_3: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt_1, torch.float32);  gt_1 = None
        mul_13: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_3, 1.4285714285714286);  convert_element_type_3 = None
        mul_14: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_4, mul_13);  mm_4 = mul_13 = None
        clone_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.clone.default(mul_14, memory_format = torch.contiguous_format);  mul_14 = None
        sigmoid_1: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(mm_2)
        full_2: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1024], 1, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(full_2, sigmoid_1);  full_2 = None
        mul_15: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_2, sub);  mm_2 = sub = None
        add_4: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Scalar(mul_15, 1);  mul_15 = None
        mul_16: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_1, add_4);  sigmoid_1 = add_4 = None
        mul_17: "f32[s1, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_1, mul_16);  clone_1 = mul_16 = None
        mm_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(mul_17, primals_10);  primals_10 = None
        permute_14: "f32[1024, s1][1, 1024]cuda:0" = torch.ops.aten.permute.default(mul_17, [1, 0]);  mul_17 = None
        mm_7: "f32[1024, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_14, mul_5);  permute_14 = mul_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_18: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, mul_4);  mul_4 = None
        mul_19: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_6, primals_9);  mm_6 = primals_9 = None
        sum_1: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_18, [0], True);  mul_18 = None
        unsqueeze_3: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_1, 0);  sum_1 = None
        view_4: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_3, [384]);  unsqueeze_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_20: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, add_1)
        mul_21: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, rsqrt_1);  mul_19 = rsqrt_1 = None
        sum_2: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_20, [1], True);  mul_20 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_5: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, mul_21);  tangents_1 = mul_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_21: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
        alias_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
        alias_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
        pow_3: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_23, 3);  alias_23 = None
        mul_22: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_23: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_22, pow_3);  mul_22 = pow_3 = None
        expand: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_23, [-1, 384]);  mul_23 = None
        div: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand, 384);  expand = None
        pow_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(add_1, 1.0);  add_1 = None
        mul_24: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_4, 2.0);  pow_4 = None
        mul_25: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div, mul_24);  div = mul_24 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_5, mul_25);  add_5 = mul_25 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        mm_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(add_6, primals_8);  primals_8 = None
        permute_15: "f32[384, s1][1, 384]cuda:0" = torch.ops.aten.permute.default(add_6, [1, 0])
        mm_9: "f32[384, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_15, view_3);  permute_15 = view_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        view_5: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_8, [sym_size_int, 6, 64]);  mm_8 = None
        alias_24: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(view_5);  view_5 = None
        permute_16: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_24, [1, 0, 2]);  alias_24 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        alias_25: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_16);  permute_16 = None
        permute_17: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_25, [1, 0, 2]);  alias_25 = None
        alias_26: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(permute_17);  permute_17 = None
        unsqueeze_4: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.unsqueeze.default(alias_26, 0);  alias_26 = None
        alias_27: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
        alias_28: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = torch.ops.aten.alias.default(alias_27);  alias_27 = None
        _efficient_attention_backward = torch.ops.aten._efficient_attention_backward.default(unsqueeze_4, unsqueeze, unsqueeze_1, unsqueeze_2, None, alias_28, convert_element_type, convert_element_type_1, sym_size_int_1, sym_size_int_1, getitem_4, 0.0, getitem_5, getitem_6, 0, False);  unsqueeze_4 = unsqueeze = unsqueeze_1 = unsqueeze_2 = alias_28 = convert_element_type = convert_element_type_1 = sym_size_int_1 = getitem_4 = getitem_5 = getitem_6 = None
        getitem_9: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[0]
        getitem_10: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[1]
        getitem_11: "f32[1, s1, 6, 64][384*s1, 384, 64, 1]cuda:0" = _efficient_attention_backward[2];  _efficient_attention_backward = None
        squeeze_1: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_11, 0);  getitem_11 = None
        squeeze_2: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_10, 0);  getitem_10 = None
        squeeze_3: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.squeeze.dim(getitem_9, 0);  getitem_9 = None
        alias_29: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_1);  squeeze_1 = None
        permute_18: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_29, [1, 0, 2]);  alias_29 = None
        alias_30: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_2);  squeeze_2 = None
        permute_19: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_30, [1, 0, 2]);  alias_30 = None
        alias_31: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.alias.default(squeeze_3);  squeeze_3 = None
        permute_20: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.permute.default(alias_31, [1, 0, 2]);  alias_31 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_32: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_18);  permute_18 = None
        permute_21: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_32, [1, 0, 2]);  alias_32 = None
        view_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_21, [sym_size_int, 384]);  permute_21 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_33: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_19);  permute_19 = None
        permute_22: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_33, [1, 0, 2]);  alias_33 = None
        view_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_22, [sym_size_int, 384]);  permute_22 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        alias_34: "f32[6, s1, 64][64, 384, 1]cuda:0" = torch.ops.aten.alias.default(permute_20);  permute_20 = None
        permute_23: "f32[s1, 6, 64][384, 64, 1]cuda:0" = torch.ops.aten.permute.default(alias_34, [1, 0, 2]);  alias_34 = None
        view_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.view.default(permute_23, [sym_size_int, 384]);  permute_23 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        full_3: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int, 1152], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int = None
        split_1 = torch.ops.aten.split.Tensor(full_3, 384, 1)
        getitem_13: "f32[s1, 384][1152, 1]cuda:0" = split_1[0];  split_1 = None
        alias_35: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_13);  getitem_13 = None
        alias_36: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_8);  view_8 = None
        copy: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_35, alias_36);  alias_35 = alias_36 = None
        slice_scatter: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(full_3, copy, 1, 0, 384);  full_3 = copy = None
        alias_39: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_7);  view_7 = None
        split_4 = torch.ops.aten.split.Tensor(slice_scatter, 384, 1)
        getitem_23: "f32[s1, 384][1152, 1]cuda:0" = split_4[1];  split_4 = None
        alias_40: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_23);  getitem_23 = None
        copy_1: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_40, alias_39);  alias_40 = alias_39 = None
        slice_scatter_1: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, copy_1, 1, 384, 768);  slice_scatter = copy_1 = None
        alias_43: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.alias.default(view_6);  view_6 = None
        split_7 = torch.ops.aten.split.Tensor(slice_scatter_1, 384, 1)
        getitem_33: "f32[s1, 384][1152, 1]cuda:0" = split_7[2];  split_7 = None
        alias_44: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.alias.default(getitem_33);  getitem_33 = None
        copy_2: "f32[s1, 384][1152, 1]cuda:0" = torch.ops.aten.copy.default(alias_44, alias_43);  alias_44 = alias_43 = None
        slice_scatter_2: "f32[s1, 1152][1152, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, copy_2, 1, 768, 1152);  slice_scatter_1 = copy_2 = None
        mm_10: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_2, primals_7);  primals_7 = None
        permute_25: "f32[1152, s1][1, 1152]cuda:0" = torch.ops.aten.permute.default(slice_scatter_2, [1, 0]);  slice_scatter_2 = None
        mm_11: "f32[1152, 384][384, 1]cuda:0" = torch.ops.aten.mm.default(permute_25, mul_3);  permute_25 = mul_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        convert_element_type_4: "f32[s1, 384][384, 1]cuda:0" = torch.ops.prims.convert_element_type.default(gt, torch.float32);  gt = None
        mul_26: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_4, 1.4285714285714286);  convert_element_type_4 = None
        mul_27: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_10, mul_26);  mm_10 = mul_26 = None
        clone_2: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.clone.default(mul_27, memory_format = torch.contiguous_format);  mul_27 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_28: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, mul);  mul = None
        mul_29: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(clone_2, primals_6);  clone_2 = primals_6 = None
        sum_3: "f32[1, 384][384, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_28, [0], True);  mul_28 = None
        unsqueeze_5: "f32[1, 1, 384][384, 384, 1]cuda:0" = torch.ops.aten.unsqueeze.default(sum_3, 0);  sum_3 = None
        view_9: "f32[384][1]cuda:0" = torch.ops.aten.view.default(unsqueeze_5, [384]);  unsqueeze_5 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        mul_30: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, primals_1)
        mul_31: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, rsqrt);  mul_29 = rsqrt = None
        sum_4: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_30, [1], True);  mul_30 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_7: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_6, mul_31);  add_6 = mul_31 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        alias_46: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        alias_47: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_46);  alias_46 = None
        alias_48: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_47);  alias_47 = None
        pow_5: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_48, 3);  alias_48 = None
        mul_32: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_4, -0.5);  sum_4 = None
        mul_33: "f32[s1, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_32, pow_5);  mul_32 = pow_5 = None
        expand_1: "f32[s1, 384][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_33, [-1, 384]);  mul_33 = None
        div_1: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_1, 384);  expand_1 = None
        pow_6: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(primals_1, 1.0);  primals_1 = None
        mul_34: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_6, 2.0);  pow_6 = None
        mul_35: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_1, mul_34);  div_1 = mul_34 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        add_8: "f32[s1, 384][384, 1]cuda:0" = torch.ops.aten.add.Tensor(add_7, mul_35);  add_7 = mul_35 = None
        return pytree.tree_unflatten([add_3, primals_2, primals_3, primals_4, add_8, tangents_2, tangents_3, tangents_4, None, view_9, mm_11, mm_9, view_4, mm_7, mm_5], self._out_spec)
        