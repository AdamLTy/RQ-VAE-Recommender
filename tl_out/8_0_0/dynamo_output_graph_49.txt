class GraphModule(torch.nn.Module):
    def forward(self, L_x_: "f32[256, s0, 384][384*s0, 384, 1]cuda:0", s0: "Sym(s0)", L_self_modules_attn_norm_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_attention_modules_qkv_parameters_weight_: "f32[1152, 384][384, 1]cuda:0", L_self_modules_attention_modules_proj_parameters_weight_: "f32[384, 384][384, 1]cuda:0", L_self_modules_ff_modules_0_parameters_weight_: "f32[384][1]cuda:0", L_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_: "f32[1024, 384][384, 1]cuda:0", L_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_: "f32[384, 1024][1024, 1]cuda:0"):
        l_x_ = L_x_
        l_self_modules_attn_norm_parameters_weight_ = L_self_modules_attn_norm_parameters_weight_
        l_self_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_attention_modules_proj_parameters_weight_ = L_self_modules_attention_modules_proj_parameters_weight_
        l_self_modules_ff_modules_0_parameters_weight_ = L_self_modules_ff_modules_0_parameters_weight_
        l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_
        l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1.pow(2)
        mean: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        add: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean + 1e-06;  mean = None
        rsqrt: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        mul: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_1 * rsqrt;  float_1 = rsqrt = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul.type_as(l_x_);  mul = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        mul_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output * l_self_modules_attn_norm_parameters_weight_;  output = l_self_modules_attn_norm_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        dropout: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(mul_1, 0.3, True, False);  mul_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:200 in forward, code: queries, keys, values = self.qkv(x).chunk(3, dim=-1)
        linear: "f32[256, s0, 1152][1152*s0, 1152, 1]cuda:0" = torch._C._nn.linear(dropout, l_self_modules_attention_modules_qkv_parameters_weight_, None);  dropout = l_self_modules_attention_modules_qkv_parameters_weight_ = None
        chunk = linear.chunk(3, dim = -1);  linear = None
        queries: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[0]
        keys: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[1]
        values: "f32[256, s0, 384][1152*s0, 1152, 1]cuda:0" = chunk[2];  chunk = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:114 in jagged_forward, code: queries = qu.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = queries.unflatten(-1, [6, 64]);  queries = None
        queries_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten.transpose(1, 2);  unflatten = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:115 in jagged_forward, code: keys = ke.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_1: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = keys.unflatten(-1, [6, 64]);  keys = None
        keys_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_1.transpose(1, 2);  unflatten_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:116 in jagged_forward, code: values = va.unflatten(-1, [self.num_heads, self.head_dim]).transpose(1, 2)
        unflatten_2: "f32[256, s0, 6, 64][1152*s0, 1152, 64, 1]cuda:0" = values.unflatten(-1, [6, 64]);  values = None
        values_1: "f32[256, 6, s0, 64][1152*s0, 64, 1152, 1]cuda:0" = unflatten_2.transpose(1, 2);  unflatten_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:120 in jagged_forward, code: context_vec = F.scaled_dot_product_attention(
        context_vec: "f32[256, 6, s0, 64][384*s0, 64, 384, 1]cuda:0" = torch._C._nn.scaled_dot_product_attention(queries_1, keys_1, values_1, dropout_p = False, is_causal = False);  queries_1 = keys_1 = values_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:123 in jagged_forward, code: context_vec = context_vec.transpose(1, 2).flatten(-2)
        transpose_3: "f32[256, s0, 6, 64][384*s0, 384, 64, 1]cuda:0" = context_vec.transpose(1, 2);  context_vec = None
        context_vec_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = transpose_3.flatten(-2);  transpose_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/attention.py:231 in forward, code: context_vec = self.proj(context_vec)
        context_vec_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(context_vec_1, l_self_modules_attention_modules_proj_parameters_weight_, None);  context_vec_1 = l_self_modules_attention_modules_proj_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:76 in forward, code: attn_out = x + self.attention(self.do(self.attn_norm(x)), padding_mask=padding_mask, is_causal=is_causal, jagged=jagged, use_cache=not self.training and self.enable_kv_cache)
        attn_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = l_x_ + context_vec_2;  l_x_ = context_vec_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        float_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out.float()
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:28 in _norm, code: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        pow_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2.pow(2)
        mean_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = pow_2.mean(-1, keepdim = True);  pow_2 = None
        add_2: "f32[256, s0, 1][s0, 1, 1]cuda:0" = mean_1 + 1e-06;  mean_1 = None
        rsqrt_1: "f32[256, s0, 1][s0, 1, 1]cuda:0" = torch.rsqrt(add_2);  add_2 = None
        mul_2: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = float_2 * rsqrt_1;  float_2 = rsqrt_1 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:31 in forward, code: output = self._norm(x.float()).type_as(x)
        output_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = mul_2.type_as(attn_out);  mul_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/normalize.py:32 in forward, code: return output * self.weight
        input_1: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = output_1 * l_self_modules_ff_modules_0_parameters_weight_;  output_1 = l_self_modules_ff_modules_0_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/encoder.py:36 in forward, code: return self.mlp(x)
        input_2: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch._C._nn.linear(input_1, l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_, None);  input_1 = l_self_modules_ff_modules_1_modules_mlp_modules_0_parameters_weight_ = None
        input_3: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.silu(input_2, inplace = False);  input_2 = None
        input_4: "f32[256, s0, 1024][1024*s0, 1024, 1]cuda:0" = torch.nn.functional.dropout(input_3, 0.3, True, False);  input_3 = None
        input_5: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch._C._nn.linear(input_4, l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_, None);  input_4 = l_self_modules_ff_modules_1_modules_mlp_modules_3_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/transformer/model.py:81 in forward, code: proj_out = attn_out + self.ff(attn_out)
        input_6: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = torch.nn.functional.dropout(input_5, 0.3, True, False);  input_5 = None
        proj_out: "f32[256, s0, 384][384*s0, 384, 1]cuda:0" = attn_out + input_6;  attn_out = input_6 = None
        return (proj_out,)
        