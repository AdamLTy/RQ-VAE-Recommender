class GraphModule(torch.nn.Module):
    def forward(self, L_batch_0_: "i64[256, 1][1, 1]cuda:0", L_self_modules_user_id_embedder_modules_emb_parameters_weight_: "f32[2000, 128][128, 1]cuda:0", L_batch_4_: "i64[256, 80][80, 1]cuda:0", L_batch_1_: "i64[256, 80][80, 1]cuda:0", L_batch_3_: "b8[256, 80][80, 1]cuda:0", L_batch_2_: "i64[256, 4][4, 1]cuda:0", L_batch_5_: "i64[256, 4][4, 1]cuda:0", L_self_modules_sem_id_embedder_modules_emb_parameters_weight_: "f32[1025, 128][128, 1]cuda:0", L_self_modules_wpe_parameters_weight_: "f32[80, 128][128, 1]cuda:0", L_self_parameters_bos_emb_: "f32[128][1]cuda:0", L_self_modules_tte_parameters_weight_: "f32[4, 128][128, 1]cuda:0"):
        l_batch_0_ = L_batch_0_
        l_self_modules_user_id_embedder_modules_emb_parameters_weight_ = L_self_modules_user_id_embedder_modules_emb_parameters_weight_
        l_batch_4_ = L_batch_4_
        l_batch_1_ = L_batch_1_
        l_batch_3_ = L_batch_3_
        l_batch_2_ = L_batch_2_
        l_batch_5_ = L_batch_5_
        l_self_modules_sem_id_embedder_modules_emb_parameters_weight_ = L_self_modules_sem_id_embedder_modules_emb_parameters_weight_
        l_self_modules_wpe_parameters_weight_ = L_self_modules_wpe_parameters_weight_
        l_self_parameters_bos_emb_ = L_self_parameters_bos_emb_
        l_self_modules_tte_parameters_weight_ = L_self_modules_tte_parameters_weight_
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:51 in forward, code: hashed_indices = x % self.num_buckets
        hashed_indices: "i64[256, 1][1, 1]cuda:0" = l_batch_0_ % 2000;  l_batch_0_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:53 in forward, code: return self.emb(hashed_indices)
        user_emb: "f32[256, 1, 128][128, 128, 1]cuda:0" = torch.nn.functional.embedding(hashed_indices, l_self_modules_user_id_embedder_modules_emb_parameters_weight_, None, None, 2.0, False, False);  hashed_indices = l_self_modules_user_id_embedder_modules_emb_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:29 in forward, code: sem_ids = batch.token_type_ids*self.num_embeddings + batch.sem_ids
        mul: "i64[256, 80][80, 1]cuda:0" = l_batch_4_ * 256;  l_batch_4_ = None
        sem_ids: "i64[256, 80][80, 1]cuda:0" = mul + l_batch_1_;  mul = l_batch_1_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:30 in forward, code: sem_ids[~batch.seq_mask] = self.padding_idx
        invert: "b8[256, 80][80, 1]cuda:0" = ~l_batch_3_
        sem_ids[invert] = 1024;  setitem = sem_ids;  invert = setitem = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:33 in forward, code: sem_ids_fut = batch.token_type_ids_fut*self.num_embeddings + batch.sem_ids_fut
        mul_1: "i64[256, 4][4, 1]cuda:0" = l_batch_5_ * 256
        sem_ids_fut: "i64[256, 4][4, 1]cuda:0" = mul_1 + l_batch_2_;  mul_1 = l_batch_2_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:34 in forward, code: sem_ids_fut = self.emb(sem_ids_fut)
        sem_ids_fut_1: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.nn.functional.embedding(sem_ids_fut, l_self_modules_sem_id_embedder_modules_emb_parameters_weight_, 1024, None, 2.0, False, False);  sem_ids_fut = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/embedding/id_embedder.py:38 in forward, code: seq=self.emb(sem_ids),
        sem_ids_emb: "f32[256, 80, 128][10240, 128, 1]cuda:0" = torch.nn.functional.embedding(sem_ids, l_self_modules_sem_id_embedder_modules_emb_parameters_weight_, 1024, None, 2.0, False, False);  sem_ids = l_self_modules_sem_id_embedder_modules_emb_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:292 in _predict, code: seq_lengths = batch.seq_mask.sum(axis=1)
        seq_lengths: "i64[256][1]cuda:0" = l_batch_3_.sum(axis = 1);  l_batch_3_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:299 in _predict, code: pos = torch.arange(N, device=sem_ids_emb.device).unsqueeze(0)
        arange: "i64[80][1]cuda:0" = torch.arange(80, device = device(type='cuda', index=0))
        pos: "i64[1, 80][80, 1]cuda:0" = arange.unsqueeze(0);  arange = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:300 in _predict, code: wpe = self.wpe(pos)
        wpe: "f32[1, 80, 128][10240, 128, 1]cuda:0" = torch.nn.functional.embedding(pos, l_self_modules_wpe_parameters_weight_, None, None, 2.0, False, False);  pos = l_self_modules_wpe_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:302 in _predict, code: input_embedding = torch.cat([user_emb, wpe + sem_ids_emb], axis=1)
        add_2: "f32[256, 80, 128][10240, 128, 1]cuda:0" = wpe + sem_ids_emb;  wpe = sem_ids_emb = None
        input_embedding: "f32[256, 81, 128][10368, 128, 1]cuda:0" = torch.cat([user_emb, add_2], axis = 1);  user_emb = add_2 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:303 in _predict, code: input_embedding_fut = self.bos_emb.repeat(B, 1, 1)
        input_embedding_fut: "f32[256, 1, 128][128, 128, 1]cuda:0" = l_self_parameters_bos_emb_.repeat(256, 1, 1);  l_self_parameters_bos_emb_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:305 in _predict, code: tte_fut = self.tte(batch.token_type_ids_fut)
        tte_fut: "f32[256, 4, 128][512, 128, 1]cuda:0" = torch.nn.functional.embedding(l_batch_5_, l_self_modules_tte_parameters_weight_, None, None, 2.0, False, False);  l_batch_5_ = l_self_modules_tte_parameters_weight_ = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:308 in _predict, code: sem_ids_emb_fut + tte_fut
        add_3: "f32[256, 4, 128][512, 128, 1]cuda:0" = sem_ids_fut_1 + tte_fut;  sem_ids_fut_1 = tte_fut = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:306 in _predict, code: input_embedding_fut = torch.cat([
        input_embedding_fut_1: "f32[256, 5, 128][640, 128, 1]cuda:0" = torch.cat([input_embedding_fut, add_3], axis = 1);  input_embedding_fut = add_3 = None
        
         # File: /home/ec2-user/code/RQ-VAE-Recommender/modules/model.py:313 in _predict, code: input_embedding = padded_to_jagged_tensor(input_embedding, lengths=seq_lengths+1)
        add_4: "i64[256][1]cuda:0" = seq_lengths + 1;  seq_lengths = None
        return (input_embedding, add_4, input_embedding_fut_1)
        